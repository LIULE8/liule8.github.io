<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark实战之高级编程-如何处理结构化数据 | Leo's notes</title><meta name="keywords" content="Spark"><meta name="author" content="Leo Liu"><meta name="copyright" content="Leo Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="如何处理结构化数据：DataFrame 、Dataset和Spark SQL 我们学习了 Spark 核心数据结构 RDD 和算子，以及 Spark 相关的一些底层原理。可以看到 RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark实战之高级编程-如何处理结构化数据">
<meta property="og:url" content="https://liule8.github.io/post/ff3b864f.html">
<meta property="og:site_name" content="Leo&#39;s notes">
<meta property="og:description" content="如何处理结构化数据：DataFrame 、Dataset和Spark SQL 我们学习了 Spark 核心数据结构 RDD 和算子，以及 Spark 相关的一些底层原理。可以看到 RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://liule8.github.io/images/posts/cover/spark.jpg">
<meta property="article:published_time" content="2021-11-13T09:51:32.000Z">
<meta property="article:modified_time" content="2021-11-19T11:41:37.219Z">
<meta property="article:author" content="Leo Liu">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liule8.github.io/images/posts/cover/spark.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liule8.github.io/post/ff3b864f"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-11-19 11:41:37'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">400</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">84</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/posts/cover/spark.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Leo's notes</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark实战之高级编程-如何处理结构化数据</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-13T09:51:32.000Z" title="发表于 2021-11-13 09:51:32">2021-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-11-19T11:41:37.219Z" title="更新于 2021-11-19 11:41:37">2021-11-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>24分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark实战之高级编程-如何处理结构化数据"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>如何处理结构化数据：DataFrame 、Dataset和Spark SQL</h1>
<p>我们学习了 Spark 核心数据结构 RDD 和算子，以及 Spark 相关的一些底层原理。可以看到 RDD 将大数据集抽象为集合，这掩盖了分布式数据集的复杂性，而函数式编程风格的算子也能满足不同的数据处理逻辑。但是，RDD + 算子的组合，对于普通分析师来说还是不太友好，他们习惯于“表”的概念而非“集合”，而使用基于集合完成数据处理的逻辑更像是程序员们的思维方式。对于数据处理逻辑，分析师们更习惯用 SQL 而非算子来表达。所以，Spark 借鉴了 Python 数据分析库 pandas 中 DataFrame 的概念，推出了 DataFrame、Dataset 与 Spark SQL。</p>
<p>在数据科学领域中，DataFrame 抽象了矩阵，如 R、pandas 中的 DataFrame；在数据工程领域，如 Spark SQL 中，DataFrame 更多地代表了关系型数据库中的表，这样就可以利用简单易学的 SQL 来进行数据分析；在 Spark 中，我们既可以用 Spark SQL + DataFrame 的组合实现海量数据分析，也可用 DataFrame + MLlib（Spark 机器学习库）的组合实现海量数据挖掘。</p>
<p>在计算机领域中，高级往往意味着简单、封装程度高，而与之对应的通常是复杂、底层。对于 Spark 编程来说，RDD + 算子的组合无疑是比较底层的，而 <strong>DataFrame + Spark SQL 的组合无论从学习成本，还是从性能开销上来说，都显著优于前者组合，所以无论是分析师还是程序员，这种方式才是使用 Spark 的首选</strong>。 此外，对于分析师来说，DataFrame 对他们来说并不陌生，熟悉的概念也能让他们快速上手。</p>
<p>这里特别说明的是，由于 DataFrame API 的 Scala 版本与 Python 版本大同小异，差异极小，所以代码以 Scala 版本为主。</p>
<h2 id="DataFrame、Dataset-的起源与演变">DataFrame、Dataset 的起源与演变</h2>
<p>DataFrame 在 Spark 1.3 被引入，它的出现取代了 SchemaRDD，Dataset 最开始在 Spark 1.6 被引入，当时还属于实验性质，在 2.0 版本时正式成为 Spark 的一部分，并且在 Spark 2.0 中，DataFrame API 与 Dataset API 在形式上得到了统一。</p>
<p>Dataset API 提供了类型安全的面向对象编程接口。Dataset 可以通过将表达式和数据字段暴露给查询计划程序和 Tungsten 的快速内存编码，从而利用 Catalyst 优化器。但是，现在 DataFrame 和 Dataset 都作为 Apache Spark 2.0 的一部分，<strong>其实 DataFrame 现在是 Dataset Untyped API 的特殊情况</strong>。更具体地说：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> = <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure>
<p>下面这张图比较清楚地表示了 DataFrame 与 Dataset 的变迁与关系。</p>
<p><img src="http://image.leonote.cn/20211113095907.png" alt="image-20211113095907412"></p>
<p><strong>由于 Python 不是类型安全的语言，所以 Spark Python API 没有 Dataset API，而只提供了 DataFrame API。当然，Java 和 Scala 就没有这种问题。</strong></p>
<h2 id="DataFrame-API">DataFrame API</h2>
<p>DataFrame 与 Dataset API 提供了简单的、统一的并且更富表达力的 API ，简言之，与 RDD 与算子的组合相比，DataFrame 与 Dataset API 更加高级。</p>
<p>DataFrame 不仅可以使用 SQL 进行查询，其自身也具有灵活的 API 可以对数据进行查询，与 RDD API 相比，DataFrame API 包含了更多的应用语义，<strong>所谓应用语义，就是能让计算框架知道你的目标的信息</strong>，这样计算框架就能更有针对性地对作业进行优化，本课时主要介绍如何创建DataFrame 以及如何利用 DataFrame 进行查询。</p>
<h3 id="创建DataFrame">创建DataFrame</h3>
<p>DataFrame 目前支持多种数据源、文件格式，如 Json、CSV 等，也支持由外部数据库直接读取数据生成，此外还支持由 RDD 通过类型反射生成，甚至还可以通过流式数据源生成。DataFrame API 非常标准，创建 DataFrame 都通过 read 读取器进行读取。下面列举了如何读取几种常见格式的文件。</p>
<p>读取 Json 文件。Json 文件如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="string">&quot;age&quot;</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="string">&quot;age&quot;</span>:<span class="number">19</span>&#125;</span><br><span class="line">......</span><br><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以利用初始化好的 SparkSession（spark）读取 Json 格式文件。</p>
<ul>
<li>读取 CSV 文件：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.csv(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>从 Parquet 格式文件中生成：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.parquet(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>从 ORC 格式文件中生成：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.orc(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span><br><span class="line">关于 <span class="type">ORC</span> 与 <span class="type">Parquet</span> 文件格式会在后面详细介绍。</span><br></pre></td></tr></table></figure>
<ul>
<li>从文本中生成：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.text(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>通过 JDBC 连接外部数据库读取数据生成</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read</span><br><span class="line">.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:postgresql:dbserver&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;schema.tablename&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;username&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;password&quot;</span>)</span><br><span class="line">.load()</span><br></pre></td></tr></table></figure>
<p>上面的代码表示通过 JDBC 相关配置，读取数据。</p>
<ul>
<li>通过 RDD 反射生成。此种方法是字符串反射为 DataFrame 的 Schema，再和已经存在的 RDD 一起生成 DataFrame，代码如下所示：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> schemaString = <span class="string">&quot;id f1 f2 f3 f4&quot;</span></span><br><span class="line"><span class="comment">// 通过字符串转换和类型反射生成schema</span></span><br><span class="line"><span class="keyword">val</span> fields = schemaString.split(<span class="string">&quot; &quot;</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, nullable = <span class="literal">true</span>))</span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(fields)</span><br><span class="line"><span class="comment">// 需要将RDD转化为RDD[Row]类型</span></span><br><span class="line"><span class="keyword">val</span> rowRDD = spark.sparkContext.textFile(textFilePath).map(_.split(<span class="string">&quot;,&quot;</span>)).map(attributes =&gt; </span><br><span class="line"><span class="type">Row</span>(attributes(<span class="number">0</span>), </span><br><span class="line">attributes(<span class="number">1</span>),</span><br><span class="line">attributes(<span class="number">2</span>),</span><br><span class="line">attributes(<span class="number">3</span>),</span><br><span class="line">attributes(<span class="number">4</span>).trim)</span><br><span class="line">)</span><br><span class="line"><span class="comment">// 生成DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(rowRDD, schema)</span><br></pre></td></tr></table></figure>
<p>注意这种方式需要隐式转换，需在转换前写上第一行：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>
<p>DataFrame 初始化完成后，可以通过 show 方法来查看数据，Json、Parquet、ORC 等数据源是自带 Schema 的，而那些无 Schema 的数据源，DataFrame 会自己生成 Schema。</p>
<p>Json 文件生成的 DataFrame 如下：</p>
<p><img src="http://image.leonote.cn/20211113100658.png" alt="image-20211113100658348"></p>
<p>CSV 文件生成的 DataFrame 如下：</p>
<p><img src="http://image.leonote.cn/20211113100718.png" alt="image-20211113100718907"></p>
<h3 id="查询">查询</h3>
<p>完成初始化的工作之后就可以使用 DataFrame API 进行查询了，DataFrame API 主要分为两种风格，一种依然是 RDD 算子风格，如 reduce、groupByKey、map、flatMap 等，另外一种则是 SQL 风格，如 select、where 等。</p>
<h4 id="算子风格">算子风格</h4>
<p>我们简单选取几个有代表性的 RDD 算子风格的 API，具体如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>[<span class="type">K</span>: <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">K</span>): <span class="type">KeyValueGroupedDataset</span>[<span class="type">K</span>, <span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>与 RDD 算子版作用相同，返回类型为 Dataset。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span> : <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>
<p>与 RDD 算子版作用相同，返回类型为 Dataset。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span> : <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">Dataset</span>[<span class="type">U</span>]</span><br></pre></td></tr></table></figure>
<p>与 RDD 算子版作用相同，返回类型为 Dataset。<br>
这些算子用法大同小异，但都需要传入 Encoder 参数，这可以通过隐式转换解决，在调用算子前，需加上：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._</span><br></pre></td></tr></table></figure>
<h4 id="SQL风格">SQL风格</h4>
<p>这类 API 的共同之处就是支持将部分 SQL 语法的字符串作为参数直接传入。</p>
<ul>
<li>select 和 where</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select</span></span>(cols: <span class="type">Column</span>*): <span class="type">DataFrame</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">where</span></span>(conditionExpr: <span class="type">String</span>): <span class="type">Dataset</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>
<p>条件查询，例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;age&quot;</span>).where(<span class="string">&quot;name is not null and age &gt; 10&quot;</span>).foreach(println(_))</span><br></pre></td></tr></table></figure>
<ul>
<li>groupBy</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span></span><br></pre></td></tr></table></figure>
<p>分组统计。例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>).groupBy(<span class="string">&quot;age&quot;</span>).count().foreach(println(_))</span><br></pre></td></tr></table></figure>
<p>此外，某些 RDD 算子风格的 API 也可以传入部分 SQL 语法的字符串，如 filter。例如：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).filter(<span class="string">&quot;age &gt; 10&quot;</span>).foreach(println(_))</span><br></pre></td></tr></table></figure>
<ul>
<li>join</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>(right: <span class="type">Dataset</span>[_], usingColumns: <span class="type">Seq</span>[<span class="type">String</span>], joinType: <span class="type">String</span>): <span class="type">DataFrame</span></span><br></pre></td></tr></table></figure>
<p>DataFrame API 还支持最普遍的连接操作，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> leftDF = ...</span><br><span class="line"><span class="keyword">val</span> rightDF = ...</span><br><span class="line">leftDF.join(rightDF, leftDF(<span class="string">&quot;pid&quot;</span>) === rightDF(<span class="string">&quot;fid&quot;</span>), <span class="string">&quot;left_outer&quot;</span>).foreach(println(_))</span><br></pre></td></tr></table></figure>
<p>其中 joinType 参数支持常用的连接类型，选项有 inner、cross、outer、full、full_outer、left、left_outer、right、right_outer、left_semi 和 left_anti，其中 cross 表示笛卡儿积，这在实际使用中比较少见；left_semi 是左半连接，是 Spark 对标准 SQL 中的 in 关键字的变通实现；left_anti 是 Spark 对标准 SQL 中的 not in 关键字的变通实现。<br>
除了 groupBy 这种分组方式，DataFrame 还支持一些特别的分组方式如 pivot、rollup、cube 等，以及常用的分析函数，先来看一个数据集：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">92</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">87</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">75</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">62</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">96</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">98</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">78</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">87</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2017&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">87</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">90</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">76</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">74</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">68</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">95</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">87</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">81</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2016&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">95</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">91</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">85</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">77</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">63</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">99</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">79</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">85</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>, <span class="attr">&quot;year&quot;</span>:<span class="string">&quot;2015&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>以上是某班学生 3 年的成绩单，一共有 3 个维度，即 name、subject 和 year，度量为 grade，也就是成绩，因此，这个 DataFrame 可以看成三维数据立方体，如下图所示。</p>
<p><img src="http://image.leonote.cn/20211113101208.png" alt="image-20211113101208384"></p>
<p>现在需要统计每个学生各科目 3 年的平均成绩，该操作可以通过下面的方式实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.groupBy(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;subject&quot;</span>).avg(<span class="string">&quot;grade&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>但是，这种形式使结果数据集只有两列——name 和 subject，不利于进一步分析，而利用 DataFrame 的数据透视 pivot 功能无疑更加方便。 pivot 功能在 pandas、Excel 等分析工具已得到了广泛应用，用户想使用透视功能，需要指定分组规则、需要透视的列以及聚合的维度列。所谓“透视”比较形象，即在分组结果上，对每一组进行“透视”，透视的结果会导致每一组基于透视列展开，最后再根据聚合操作进行聚合，统计每个学生每科 3 年平均成绩实现如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.groupBy(<span class="string">&quot;name&quot;</span>).pivot(<span class="string">&quot;subject&quot;</span>).avg(<span class="string">&quot;grade&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="http://image.leonote.cn/20211113101357.png" alt="image-20211113101357318"></p>
<p>除了 groupBy 之外，DataFrame 还提供 rollup 和 cube 的方式进行分组聚合，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rollup</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span></span><br></pre></td></tr></table></figure>
<p>rollup 也是用来进行分组统计，只不过分组逻辑有所不同，假设 rollup(A,B,C)，其中 A、B、C 分别为 3 列，那么会先对 A、B、C 进行分组，然后依次对 A、B 进行分组、对 A 进行分组、对全表进行分组，执行：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.rollup(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;subject&quot;</span>).avg(<span class="string">&quot;grade&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="http://image.leonote.cn/20211113101531.png" alt="image-20211113101531729"></p>
<p>可以看到，除了按照 name + subject 的组合键进行分组，还分别对每个人进行了分组，如 Michael,null，此外还将全表分为了一组，如 null,null。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cube</span></span>(col1: <span class="type">String</span>, cols: <span class="type">String</span>*): <span class="type">RelationalGroupedDataset</span></span><br></pre></td></tr></table></figure>
<p>cube 与 rollup 类似，分组依据有所不同，仍以 cube(A,B,C) 为例，分组依据分别是 (A,B,C)、(A,B)、(A,C)、(B,C)、(A)、(B)、©、全表，执行：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfSG.cube(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;subject&quot;</span>).avg(<span class="string">&quot;grade&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<p><img src="http://image.leonote.cn/20211113101702.png" alt="image-20211113101702791"></p>
<p>可以看到与 rollup 不同，这里还分别对每个科目进行分组，如 null、math。</p>
<p>在实际使用中，你应该尽量选用并习惯于用 SQL 风格的算子完成开发任务，SQL 风格的查询 API 不光表现力强，另外也非常易读。</p>
<h3 id="写出">写出</h3>
<p>与创建 DataFrame 的 read 读取器相对应，写出为 write 输出器 API。下面列举了如何输出几种常见格式的文件。</p>
<ul>
<li>写出为 Json 文件：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).filter(<span class="string">&quot;age &gt; 10&quot;</span>).write.json(<span class="string">&quot;/your/output/path&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>写出为 Parquet 文件：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).filter(<span class="string">&quot;age &gt; 10&quot;</span>).write.parquet(<span class="string">&quot;/your/output/path&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>写出为 ORC 文件：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).filter(<span class="string">&quot;age &gt; 10&quot;</span>).write.orc(<span class="string">&quot;/your/output/path&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>写出为文本文件：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).filter(<span class="string">&quot;age &gt; 10&quot;</span>).write.text(<span class="string">&quot;/your/output/path&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>写出为 CSV 文件：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> saveOptions = <span class="type">Map</span>(<span class="string">&quot;header&quot;</span> -&gt; <span class="string">&quot;true&quot;</span>, <span class="string">&quot;path&quot;</span> -&gt; <span class="string">&quot;csvout&quot;</span>)</span><br><span class="line">df.select(<span class="string">&quot;age&quot;</span>, <span class="string">&quot;name&quot;</span>).filter(<span class="string">&quot;age &gt; 10&quot;</span>)</span><br><span class="line">.write</span><br><span class="line">.format(<span class="string">&quot;com.databricks.spark.csv&quot;</span>)</span><br><span class="line">.mode(<span class="type">SaveMode</span>.<span class="type">Overwrite</span>)</span><br><span class="line">.options(saveOptions)</span><br><span class="line">.save()</span><br></pre></td></tr></table></figure>
<p>我们还可以在保存时对格式已经输出的方式进行设定，例如本例中是保留表头，并且输出方式是 Overwrite，输出方式有 Append、ErrorIfExist、Ignore、Overwrite，分别代表追加到已有输出路径中、如果输出路径存在则报错、存在则停止、存在则覆盖。</p>
<ul>
<li>写出到关系型数据库：</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> prop = <span class="keyword">new</span> java.util.<span class="type">Properties</span></span><br><span class="line">prop.setProperty(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;spark&quot;</span>)</span><br><span class="line">prop.setProperty(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;123&quot;</span>)</span><br><span class="line">df.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(<span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span>,<span class="string">&quot;tablename&quot;</span>,prop)</span><br></pre></td></tr></table></figure>
<p>写出到关系型数据库同样基于 JDBC ，用此种方式写入关系型数据库，表名可以不存在。</p>
<h2 id="Dataset-API">Dataset API</h2>
<p>从本质上来说，DataFrame 只是 Dataset 的一种特殊情况，在 Spark 2.x 中已经得到了统一：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DataFrame</span> = <span class="type">Dataset</span>[<span class="type">Row</span>]</span><br></pre></td></tr></table></figure>
<p>因此，在使用 DataFrame API 的过程中，很容易就会自动转换为 Dataset[String]、Dataset[Int] 等类型。除此之外，用户还可以自定义类型。下面来看看 DataFrame 转成 Dataset 的例子，下面是一个 Json 文件，记录了学生的单科成绩：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">92</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">87</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">75</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">62</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;Chinese&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">96</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">98</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">78</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Berta&quot;</span>, <span class="attr">&quot;grade&quot;</span>:<span class="number">87</span>, <span class="attr">&quot;subject&quot;</span>:<span class="string">&quot;math&quot;</span>&#125;</span><br></pre></td></tr></table></figure>
<p>代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 首先定义StudentGrade类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">StudentGrade</span>(<span class="params">name: <span class="type">String</span>, subject: <span class="type">String</span>, grade: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="comment">// 生成DataFrame</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">dfSG</span> </span>= spark.read.json(<span class="string">&quot;data/examples/target/scala-2.11/classes/student_grade.json&quot;</span>)</span><br><span class="line"><span class="comment">// 方法1:通过map函数手动转换为Dataset[StudentGrade]类型</span></span><br><span class="line"><span class="keyword">val</span> dsSG: <span class="type">Dataset</span>[<span class="type">StudentGrade</span>] = dfSG.map(a =&gt; <span class="type">StudentGrade</span>(a.getAs[<span class="type">String</span>](<span class="number">0</span>),a.getAs[<span class="type">String</span>](<span class="number">1</span>),a.getAs[<span class="type">Long</span>](<span class="number">2</span>)))</span><br><span class="line"><span class="comment">// 方法2:使用DataFrame的as函数进行转换</span></span><br><span class="line"><span class="keyword">val</span> dsSG2: <span class="type">Dataset</span>[<span class="type">StudentGrade</span>] = dfSG.as[<span class="type">StudentGrade</span>]</span><br><span class="line"><span class="comment">// 方法3：通过RDD转换而成（基于同样内容的CSV文件）</span></span><br><span class="line"><span class="keyword">val</span> dsSG3 = spark.sparkContext.</span><br><span class="line">textFile(<span class="string">&quot;data/examples/target/scala-2.11/classes/student_grade.csv&quot;</span>).</span><br><span class="line">map[<span class="type">StudentGrade</span>](row =&gt; &#123;</span><br><span class="line">     <span class="keyword">val</span> fields = row.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">	 <span class="type">StudentGrade</span>(</span><br><span class="line">	      fields(<span class="number">0</span>).toString(),</span><br><span class="line">	      fields(<span class="number">1</span>).toString(),</span><br><span class="line">	      fields(<span class="number">2</span>).toLong</span><br><span class="line">	    )</span><br><span class="line">&#125;).toDS</span><br><span class="line">	 </span><br><span class="line"><span class="comment">// 求每科的平均分</span></span><br><span class="line">dsSG3.groupBy(<span class="string">&quot;subject&quot;</span>).mean(<span class="string">&quot;grade&quot;</span>).foreach(println(_))</span><br></pre></td></tr></table></figure>
<p>以上 3 种方法都可以将 DataFrame 转换为 Dataset 。转换完成后，就可以使用其 API 对数据进行分析，使用方式与 DataFrame 并无不同。</p>
<h2 id="Spark-SQL">Spark SQL</h2>
<p>在实际工作中，使用频率最高的当属 Spark SQL，通常一个大数据处理项目中，70% 的数据处理任务都是由 Spark SQL 完成，它贯穿于数据预处理、数据转换和最后的数据分析。由于 SQL 的学习成本低、用户基数大、函数丰富，Spark SQL 也通常是使用 Spark 最方便的方式。此外，由于 SQL 包含了丰富的应用语义，所以 Catalyst 优化器带来的性能巨大提升也使 Spark SQL 成为编写 Spark 作业的最佳方式。接下来我将为你介绍 Spark SQL 的使用。</p>
<p>从使用层面上来讲，要想用好 Spark SQL，只需要编写 SQL 就行了，最后简单介绍了下 SQL 的常用语法，方便没有接触过 SQL 的同学快速入门。</p>
<h3 id="创建临时视图">创建临时视图</h3>
<p>想使用 Spark SQL，可以先创建临时视图，相当于数据库中的表，这可以通过已经存在的 DataFrame、Dataset 直接生成；也可以直接从 Hive 元数据库中获取元数据信息直接进行查询。先来看看创建临时视图：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">StudentGrade</span>(<span class="params">name: <span class="type">String</span>, subject: <span class="type">String</span>, grade: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="comment">// 生成DataFrame</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">dfSG</span> </span>= spark.read.json(<span class="string">&quot;data/examples/target/scala-2.11/classes/student_grade.json&quot;</span>)</span><br><span class="line"><span class="comment">// 生成Dataset</span></span><br><span class="line"><span class="keyword">val</span> dsSG = dfSG.map(</span><br><span class="line">	  a =&gt; <span class="type">StudentGrade</span>( </span><br><span class="line">	      a.getAs[<span class="type">String</span>](<span class="string">&quot;name&quot;</span>),</span><br><span class="line">	      a.getAs[<span class="type">String</span>](<span class="string">&quot;subject&quot;</span>),</span><br><span class="line">	      a.getAs[<span class="type">Long</span>](<span class="string">&quot;grade&quot;</span>)</span><br><span class="line">	  )</span><br><span class="line">)</span><br><span class="line">	 </span><br><span class="line"><span class="comment">// 创建临时视图</span></span><br><span class="line">dfSG.createOrReplaceTempView(<span class="string">&quot;student_grade_df&quot;</span>)</span><br><span class="line">dsSG.createOrReplaceTempView(<span class="string">&quot;student_grade_ds&quot;</span>)</span><br><span class="line"><span class="comment">// 计算每科的平均分</span></span><br><span class="line">spark.sql(<span class="string">&quot;SELECT subject, AVG(grade) FROM student_grade_df GROUP BY subject&quot;</span>).show()</span><br><span class="line">spark.sql(<span class="string">&quot;SELECT subject, AVG(grade) FROM student_grade_ds GROUP BY subject&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<p>对于 Dataset 来说，对象类型的数据结构会作为临时视图的元数据，在 SQL 中可以直接使用。</p>
<h3 id="使用Hive元数据">使用Hive元数据</h3>
<p>随着 Spark 越来越流行，有很多情况，需要将 Hive 作业改写成 Spark SQL 作业，Spark SQL 可以通过 hive-site.xml 文件的配置，直接读取 Hive 元数据。这样，改写的工作量就小了很多，代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder()</span><br><span class="line">.master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">.appName(<span class="string">&quot;Hive on Spark&quot;</span>)</span><br><span class="line">.enableHiveSupport()</span><br><span class="line">.getOrCreate()</span><br><span class="line"><span class="comment">// 直接查询</span></span><br><span class="line">spark.sql(…………)</span><br></pre></td></tr></table></figure>
<p>代码中通过 enableHiveSupport 方法开启对 Hive 的支持，但需要将 Hive 配置文件 hive-site.xml 复制到 Spark 的配置文件夹下。</p>
<h3 id="查询语句">查询语句</h3>
<p>Spark 的 SQL 语法源于 Presto （一种支持 SQL 的大规模并行处理技术，适合 OLAP），在源码中我们可以看见，Spark 的 SQL 解析引擎直接采用了 Presto 的 SQL 语法文件。查询是 Spark SQL 的核心功能，Spark SQL 的查询语句模式如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[ WITH with_query [, ...] ]</span><br><span class="line">SELECT [ ALL | DISTINCT ] select_expr [, ...]</span><br><span class="line">[ FROM from_item [, ...] ]</span><br><span class="line">[ WHERE condition ]</span><br><span class="line">[ GROUP BY expression [, ...] ]</span><br><span class="line">[ HAVING condition]</span><br><span class="line">[ UNION [ ALL | DISTINCT ] select ]</span><br><span class="line">[ ORDER BY expression [ ASC | DESC ] [, ...] ]</span><br><span class="line">[ LIMIT count ]</span><br></pre></td></tr></table></figure>
<p>其中 from_item 为以下之一：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">table_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ]</span><br><span class="line">from_item join_type from_item [ ON join_condition | USING ( join_column [, ...] ) ]</span><br></pre></td></tr></table></figure>
<p>该模式基本涵盖了 Spark SQL 中查询语句的各种写法。</p>
<h4 id="SELECT-与-FROM-子句">SELECT 与 FROM 子句</h4>
<p>SELECT 与 FROM 是构成查询语句的最小单元，SELECT 后面跟列名表示要查询的列，或者用 * 表示所有列，FROM 后面跟表名，示例如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> name, grade <span class="keyword">FROM</span> student_grade t;</span><br></pre></td></tr></table></figure>
<p>在使用过程中，对列名和表名都可以赋予别名，这里对 student_grade 赋予别名 t，此外我们还可以对某一列用关键字 DISTINCT 进行去重，默认为 ALL，表示不去重：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">COUNT</span>( <span class="keyword">DISTINCT</span> name) <span class="keyword">FROM</span> student_grade;</span><br></pre></td></tr></table></figure>
<p>上面这条 SQL 代表统计有多少学生参加了考试。</p>
<h4 id="WHERE-子句">WHERE 子句</h4>
<p>WHERE 子句经常和 SELECT 配合使用，用来过滤参与查询的数据集，WHERE 后面一般会由运算符组合成谓词表达式（返回值为 True 或者 False ），例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade <span class="keyword">WHERE</span> grade <span class="operator">&gt;</span> <span class="number">90</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade <span class="keyword">WHERE</span> name <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade <span class="keyword">WHERE</span> name <span class="keyword">LIKE</span> &quot;*ndy&quot;;</span><br></pre></td></tr></table></figure>
<p>常见的运算符还有 !=、&lt;&gt; 等，此外还可以用逻辑运算符：AND、OR 组合谓词表达式进行查询，例如：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade <span class="keyword">WHERE</span> grade <span class="operator">&gt;</span> <span class="number">90</span> <span class="keyword">AND</span> name <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br></pre></td></tr></table></figure>
<h4 id="GROUP-BY-子句">GROUP BY 子句</h4>
<p>GROUP BY 子句用于对 SELECT 语句的输出进行分组，分组中是匹配值的数据行。GROUP BY 子句支持指定列名或列序号（从 1 开始）表达式。以下查询是等价的，都会对 subject 列进行分组，第一个查询使用列序号，第二个查询使用列名：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">avg</span>(grade), subject <span class="keyword">FROM</span> student_grade <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">SELECT</span> <span class="built_in">avg</span>(grade), subject <span class="keyword">FROM</span> student_grade <span class="keyword">GROUP</span> <span class="keyword">BY</span> subject;</span><br></pre></td></tr></table></figure>
<p>使用 GROUP BY 子句时需注意，出现在 SELECT 后面的列，要么同时出现在 GROUP BY 后面，要么就在聚合函数中。</p>
<h4 id="HAVING-子句">HAVING 子句</h4>
<p>HAVING 子句与聚合函数以及 GROUP BY 子句配合使用，用来过滤分组统计的结果。HAVING 子句去掉不满足条件的分组。在分组和聚合计算完成后，HAVING 对分组进行过滤。例如以下查询会过滤掉平均分大于 90 分的科目：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> subject,<span class="built_in">AVG</span>(grade) </span><br><span class="line"><span class="keyword">FROM</span> student_grade </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> subject </span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">AVG</span>(grade) <span class="operator">&lt;</span> <span class="number">90</span>;</span><br></pre></td></tr></table></figure>
<h4 id="UNION-子句">UNION 子句</h4>
<p>UNION 子句用于将多个查询语句的结果合并为一个结果集：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">query UNION [ALL | DISTINCT] query</span><br></pre></td></tr></table></figure>
<p>参数 ALL 或 DISTINCT 可以控制最终结果集包含哪些行。如果指定参数 ALL，则包含全部行，即使行完全相同；如果指定参数 DISTINCT ，则合并结果集，结果集只有唯一不重复的行；如果不指定参数，执行时默认使用 DISTINCT。下面这句 SQL 是将两个班级的成绩进行合并：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade_class1</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade_class2;</span><br></pre></td></tr></table></figure>
<p>多个 UNION 子句会从左向右执行，除非用括号明确指定顺序。</p>
<h4 id="ORDER-BY-子句">ORDER BY 子句</h4>
<p>ORDER BY 子句按照一个或多个输出表达式对结果集排序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ORDER BY expression [ ASC | DESC ] [ NULLS &#123; FIRST | LAST &#125; ] [, ...]</span><br></pre></td></tr></table></figure>
<p>每个表达式由列名或列序号（从 1 开始）组成。ORDER BY 子句作为查询的最后一步，在 GROUP BY 和 HAVING 子句之后。ASC 为默认升序，DESC 为降序。下面这句 SQL 会对结果进行过滤，并按照平均分进行排序，注意这里使用了列别名：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> subject,<span class="built_in">AVG</span>(grade) avg </span><br><span class="line"><span class="keyword">FROM</span> student_grade </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> subject </span><br><span class="line"><span class="keyword">HAVING</span> <span class="built_in">AVG</span>(grade) <span class="operator">&lt;</span> <span class="number">90</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> avg <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure>
<h4 id="LIMIT-子句">LIMIT 子句</h4>
<p>LIMIT 子句限制结果集的行数，这在查询大表时很有用。以下示例为对单科成绩进行排序并只返回前 3 名的记录：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student_grade </span><br><span class="line"><span class="keyword">WHERE</span> subject <span class="operator">=</span> <span class="string">&#x27;math&#x27;</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> grade <span class="keyword">DESC</span> </span><br><span class="line">LIMIT <span class="number">3</span>;</span><br></pre></td></tr></table></figure>
<h4 id="JOIN-子句">JOIN 子句</h4>
<p>JOIN 操作可以将多个有关联的表进行关联查询，下面这句 SQL 是查询数学成绩在 90 分以上的学生的院系，其中院系信息可以从学生基础信息表内，通过姓名连接得到：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> g.<span class="operator">*</span>, a.department </span><br><span class="line"><span class="keyword">FROM</span> student_grade g </span><br><span class="line"><span class="keyword">JOIN</span> student_basic b </span><br><span class="line"><span class="keyword">ON</span> g.name <span class="operator">=</span> b.name </span><br><span class="line"><span class="keyword">WHERE</span> g.subject <span class="operator">=</span> <span class="string">&#x27;math&#x27;</span> <span class="keyword">and</span> grade <span class="operator">&gt;</span> <span class="number">90</span></span><br></pre></td></tr></table></figure>
<p>在这句 SQL 中，表 g 被称为驱动表或是左表，表 b 被称为右表。如前所述，Spark 支持多种连接类型。</p>
<h2 id="小结">小结</h2>
<p>主要介绍了 DataFrame、Dataset 与 Spark SQL，相对于 RDD 与算子，这种数据处理方式无疑对分析师来说更为友好，而且这也是 Spark 官方推荐的 Spark API，无论是从性能还是从开发效率来说都是全方位领先于 RDD 与算子的组合，这也是很好理解的，举个例子，学习了 Python 后，想用 Python 直接进行数据分析无疑是很不方便的，Python 的 pandas 库则很好地解决了这个问题。</p>
<p>由于 Spark 对于 SQL 支持得非常好，而 pandas 在这方面没那么强大，所以，在某些场景，你可以选择 Spark SQL 来代替 pandas，这有时对于分析师来说非常好用。</p>
<h2 id="思考题">思考题</h2>
<p>表中存有某一年某只股票的历史价格，表结构如下：</p>
<p>股票 id，时间戳，成交价格</p>
<p>问题是：请用一句 SQL 计算这只股票最长连续价格上涨天数，这里解释一个概念，如果某天的开盘价小于当天的收盘价，就认为这只股票当天是上涨的。这个需求看起来简单，但是用 SQL 写出来还是比较复杂的，需要用到子查询等内容，如果你能够完成这个思考题，我相信你能很好地使用 Spark SQL。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Leo Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://liule8.github.io/post/ff3b864f.html">https://liule8.github.io/post/ff3b864f.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://liule8.github.io" target="_blank">Leo's notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/images/posts/cover/spark.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/5220052f.html"><img class="prev-cover" src="/images/posts/cover/spark.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark实战之高级编程-如何使用用户自定义函数</div></div></a></div><div class="next-post pull-right"><a href="/post/284c1536.html"><img class="next-cover" src="/images/posts/cover/spark.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark实战之编程-计算框架的分布式实现</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/69b6c9e6.html" title="Spark实战之基础-Hadoop"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-Hadoop</div></div></a></div><div><a href="/post/bf0f3957.html" title="Spark实战之基础-MapReduce"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-MapReduce</div></div></a></div><div><a href="/post/abfe544b.html" title="Spark实战之基础-YARN"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-YARN</div></div></a></div><div><a href="/post/2a7649c3.html" title="Spark实战之基础-如何选择 Spark 编程语言以及部署 Spark"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-07</div><div class="title">Spark实战之基础-如何选择 Spark 编程语言以及部署 Spark</div></div></a></div><div><a href="/post/99c2f5e4.html" title="Spark实战之基础-解析 Spark 数据处理与分析场景"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-07</div><div class="title">Spark实战之基础-解析 Spark 数据处理与分析场景</div></div></a></div><div><a href="/post/27968528.html" title="Spark实战之流处理-如何在生产环境中使用 Spark Streaming"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-20</div><div class="title">Spark实战之流处理-如何在生产环境中使用 Spark Streaming</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Leo Liu</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">400</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">84</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/liule8"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/LIULE8" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:leo.liu.scau@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">这里是公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">如何处理结构化数据：DataFrame 、Dataset和Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame%E3%80%81Dataset-%E7%9A%84%E8%B5%B7%E6%BA%90%E4%B8%8E%E6%BC%94%E5%8F%98"><span class="toc-number">1.1.</span> <span class="toc-text">DataFrame、Dataset 的起源与演变</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataFrame-API"><span class="toc-number">1.2.</span> <span class="toc-text">DataFrame API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BADataFrame"><span class="toc-number">1.2.1.</span> <span class="toc-text">创建DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2"><span class="toc-number">1.2.2.</span> <span class="toc-text">查询</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E5%AD%90%E9%A3%8E%E6%A0%BC"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">算子风格</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#SQL%E9%A3%8E%E6%A0%BC"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">SQL风格</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E5%87%BA"><span class="toc-number">1.2.3.</span> <span class="toc-text">写出</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dataset-API"><span class="toc-number">1.3.</span> <span class="toc-text">Dataset API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-SQL"><span class="toc-number">1.4.</span> <span class="toc-text">Spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%B4%E6%97%B6%E8%A7%86%E5%9B%BE"><span class="toc-number">1.4.1.</span> <span class="toc-text">创建临时视图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Hive%E5%85%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.4.2.</span> <span class="toc-text">使用Hive元数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5"><span class="toc-number">1.4.3.</span> <span class="toc-text">查询语句</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SELECT-%E4%B8%8E-FROM-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">SELECT 与 FROM 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#WHERE-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.2.</span> <span class="toc-text">WHERE 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GROUP-BY-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.3.</span> <span class="toc-text">GROUP BY 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#HAVING-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.4.</span> <span class="toc-text">HAVING 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#UNION-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.5.</span> <span class="toc-text">UNION 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ORDER-BY-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.6.</span> <span class="toc-text">ORDER BY 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LIMIT-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.7.</span> <span class="toc-text">LIMIT 子句</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#JOIN-%E5%AD%90%E5%8F%A5"><span class="toc-number">1.4.3.8.</span> <span class="toc-text">JOIN 子句</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.5.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">1.6.</span> <span class="toc-text">思考题</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/a79999eb.html" title="数据结构与算法之基础篇-递归树"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-递归树"/></a><div class="content"><a class="title" href="/post/a79999eb.html" title="数据结构与算法之基础篇-递归树">数据结构与算法之基础篇-递归树</a><time datetime="2021-11-20T19:27:51.000Z" title="发表于 2021-11-20 19:27:51">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/6fd3f48a.html" title="数据结构与算法之基础篇-红黑树(下)"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-红黑树(下)"/></a><div class="content"><a class="title" href="/post/6fd3f48a.html" title="数据结构与算法之基础篇-红黑树(下)">数据结构与算法之基础篇-红黑树(下)</a><time datetime="2021-11-20T17:23:16.000Z" title="发表于 2021-11-20 17:23:16">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/76c8c5cb.html" title="数据结构与算法之基础篇-红黑树(上)"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-红黑树(上)"/></a><div class="content"><a class="title" href="/post/76c8c5cb.html" title="数据结构与算法之基础篇-红黑树(上)">数据结构与算法之基础篇-红黑树(上)</a><time datetime="2021-11-20T14:13:02.000Z" title="发表于 2021-11-20 14:13:02">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/8564d9ad.html" title="数据结构与算法之基础篇-二叉树基础(下)"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-二叉树基础(下)"/></a><div class="content"><a class="title" href="/post/8564d9ad.html" title="数据结构与算法之基础篇-二叉树基础(下)">数据结构与算法之基础篇-二叉树基础(下)</a><time datetime="2021-11-20T10:42:08.000Z" title="发表于 2021-11-20 10:42:08">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/533482d9.html" title="Spark实战之流处理-统一批处理与流处理：Dataflow"><img src="/images/posts/cover/spark.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark实战之流处理-统一批处理与流处理：Dataflow"/></a><div class="content"><a class="title" href="/post/533482d9.html" title="Spark实战之流处理-统一批处理与流处理：Dataflow">Spark实战之流处理-统一批处理与流处理：Dataflow</a><time datetime="2021-11-20T09:55:00.000Z" title="发表于 2021-11-20 09:55:00">2021-11-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Leo Liu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>