<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark实战之流处理-如何在生产环境中使用 Spark Streaming | Leo's notes</title><meta name="keywords" content="Spark"><meta name="author" content="Leo Liu"><meta name="copyright" content="Leo Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="如何在生产环境中使用 Spark Streaming 我们学习了 Spark Streaming 的抽象、架构以及数据处理方式，但是流处理的输入是动态的数据源，假设在出错是常态的情况下，如何在动态的数据流中仍然兼顾恰好一次的消息送达保证（结果正确性），是生产环境中必须考虑的问题。 输入和输出 Spark Streaming 作为一个流处理系统，对接了很多输入源，除了一些实验性质的输入源，如Cons">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark实战之流处理-如何在生产环境中使用 Spark Streaming">
<meta property="og:url" content="https://liule8.github.io/post/27968528.html">
<meta property="og:site_name" content="Leo&#39;s notes">
<meta property="og:description" content="如何在生产环境中使用 Spark Streaming 我们学习了 Spark Streaming 的抽象、架构以及数据处理方式，但是流处理的输入是动态的数据源，假设在出错是常态的情况下，如何在动态的数据流中仍然兼顾恰好一次的消息送达保证（结果正确性），是生产环境中必须考虑的问题。 输入和输出 Spark Streaming 作为一个流处理系统，对接了很多输入源，除了一些实验性质的输入源，如Cons">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://liule8.github.io/images/posts/cover/spark.jpg">
<meta property="article:published_time" content="2021-11-20T08:43:00.000Z">
<meta property="article:modified_time" content="2021-11-20T01:55:38.192Z">
<meta property="article:author" content="Leo Liu">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liule8.github.io/images/posts/cover/spark.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liule8.github.io/post/27968528"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-11-20 01:55:38'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">400</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">84</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/posts/cover/spark.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Leo's notes</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark实战之流处理-如何在生产环境中使用 Spark Streaming</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-20T08:43:00.000Z" title="发表于 2021-11-20 08:43:00">2021-11-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-11-20T01:55:38.192Z" title="更新于 2021-11-20 01:55:38">2021-11-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>11分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark实战之流处理-如何在生产环境中使用 Spark Streaming"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>如何在生产环境中使用 Spark Streaming</h1>
<p>我们学习了 Spark Streaming 的抽象、架构以及数据处理方式，但是流处理的输入是动态的数据源，假设在出错是常态的情况下，如何在动态的数据流中仍然兼顾恰好一次的消息送达保证（结果正确性），是生产环境中必须考虑的问题。</p>
<h2 id="输入和输出">输入和输出</h2>
<p>Spark Streaming 作为一个流处理系统，对接了很多输入源，除了一些实验性质的输入源，如ConstantInputDStream（每批次都为常数集合）、socketTextStream（监听套接字作为输入）、textFileStream（本地文件作为输入，常常用来监控文件夹），在生产环境中用得最多的还是 Kafka 这类消息队列。我们选用 Kafka 0.8 版本，介绍 Spark Streaming 与 Kafka 集成，这是在生产环境中最常见的一种情况。</p>
<p>在 Spark 中，为连接 Kafka 提供了两种方式，即基于 Receiver 的方式和 Kafka Direct API。这两种方式在使用上大同小异，但原理却截然不同，先来看看基于 Receiver 的方式：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">  <span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;localhost:9092,anotherhost:9092&quot;</span>,</span><br><span class="line">  <span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">  <span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">  <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;groupId&quot;</span>,</span><br><span class="line">  <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span>,</span><br><span class="line">  <span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">true</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">&quot;topicA&quot;</span>, <span class="string">&quot;topicB&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  ssc,</span><br><span class="line">  <span class="type">PreferConsistent</span>,</span><br><span class="line">  <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">messages.map(record =&gt; (record.key, record.value))</span><br></pre></td></tr></table></figure>
<p>用这种方式来与 Kafka 集成，配置中设置了 enable.auto.commit 为 true，表明自己不需要维护 offset，而是由 Kafka 自己来维护（在 Kafka 0.10 后，默认的 offset 存储位置改为了 Kafka，实际上就是 Kafka 的一个 topic），Kafka 消费者会周期性地（默认为 5s）去修改偏移量。这种方式接收的数据都保存在 Receiver 中，一旦出现意外，数据就有可能丢失，要想避免丢失的情况，就必须采用 WAL（Write Ahead Log，预写日志）机制，在数据写入内存前先进行持久化。</p>
<p>现在我们来试想一种情况，数据从 Kafka 取出后，进行了 WAL，在这个时候，Driver 与 Executor 因为某种原因宕机，这时最新偏移量还没来得及提交，那么在 Driver 恢复后，会从记录的偏移量继续消费数据并处理 WAL 的数据，这样一来，被 WAL 持久化的数据就会被重复计算一次。因此，开启了 WAL 后，这样的容错机制最多只能实现“至少一次”的消息送达语义。而且开启 WAL 后，增加了 I/O 开销，降低了 Spark Streaming 的吞吐量，还会产生冗余存储。这个过程如下图所示。</p>
<p><img src="http://image.leonote.cn/20211120093213.png" alt="image-20211120093213280"></p>
<p>如果业务场景对“恰好一次”的消息送达语义有着强烈的需求，那么基于 Receiver 的方式是无法满足的，基于 Spark Streaming 提供的 Direct API 形式，克服了这一缺点。Direct API 是 Spark 1.3 后增加的新特性，相比基于 Receiver 方法的“间接”，这种方式更加“直接”。</p>
<p>在这种方式中，消费者会定期轮询 Kafka，得到在每个 topic 中每个分区的最新偏移量，根据这个偏移量来确定每个批次的范围，这个信息会记录在 Checkpoint 中，当作业启动时，会根据这个范围用消费者 API 直接获取数据。这样的话，就相当于把 Kafka 变成了一个文件系统，而 offset 的范围就是文件地址，Direct API 用这种方式将流式数据源变成了静态数据源，再利用 Spark 本身的 DAG 容错机制，使所有计算失败的数据均可溯源，从而实现了“恰好一次”的消息送达语义。**请注意，Direct API 不需要采用WAL预写日志机制，因为所有数据都相当于在 Kafka 中被持久化了，作业恢复后直接从 Kafka 读取即可，**如下图所示：</p>
<p><img src="http://image.leonote.cn/20211120094830.png" alt="image-20211120094830835"></p>
<p>这种方式带来的优点显而易见，不仅克服了 WAL 带来的效率缺陷，还简化了并行性，使用 Direct API，Spark Streaming 会为每个 Kafka 的分区创建对应的 RDD 分区，这样就不需要使用 ssc.union() 方法来进行合并了，这也便于理解和调优。另外，这样的架构还保证了输入-处理阶段的“恰好一次”的消息送达语义，这就类似于消息的“回放”，虽然目前 Kafka 本身不支持消息回放，但用这种方式间接地实现了消息回放的功能。下面我们来看一个使用 Direct API 的完整例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkStreamingKafkaDirexct</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder</span><br><span class="line">      .master(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">      .appName(<span class="string">&quot;SparkStreamingKafkaDirexct&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, batchDuration = <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Kafka的topic</span></span><br><span class="line">    <span class="keyword">val</span> topics = args(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> topicsSet: <span class="type">Set</span>[<span class="type">String</span>] = topics.split(<span class="string">&quot;,&quot;</span>).toSet</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Kafka配置参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      <span class="string">&quot;metadata.broker.list&quot;</span> -&gt; <span class="string">&quot;kafka01:9092,kafka02:9092,kafka03:9092&quot;</span>,</span><br><span class="line">      <span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;apple_sample&quot;</span>,</span><br><span class="line">      <span class="string">&quot;serializer.class&quot;</span> -&gt; <span class="string">&quot;kafka.serializer.StringEncoder&quot;</span>,</span><br><span class="line">      <span class="comment">// 自动将偏移重置为最新的偏移，如果是第一次启动程序，应该为smallest，从头开始读</span></span><br><span class="line">      <span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;latest&quot;</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用Kafka Direct API直接读数据</span></span><br><span class="line">    <span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topicsSet, kafkaParams)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在该批次数据处理完之后，将该offset提交给Kafka，“...”代表用户自己定义的处理逻辑</span></span><br><span class="line">    messages.map(</span><br><span class="line">    ...).foreachRDD(mess =&gt; &#123;</span><br><span class="line">      <span class="comment">// 获取offset集合</span></span><br><span class="line">      <span class="keyword">val</span> offsetsList = mess.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">      asInstanceOf[<span class="type">CanCommitOffsets</span>].commitAsync(offsetsList)</span><br><span class="line">    &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从上面这段代码中我们可以发现，首先关闭了自动提交偏移量，改由手动维护。然后再从最新的偏移量开始生成 RDD，经过各种转换算子处理后输出结果，最后用 commitAsync 异步向 Kafka 提交最新的偏移量。一旦使用了 Direct API，用户需要追踪到结果数据输出完成后，再提交偏移量的改动，否则会造成不确定的影响。使用这种方式，无法在事务层面保证处理-输出这个阶段做到“恰好一次”，因此只能采用输出幂等的方式来达到同样的效果。</p>
<p>如果想要在事务的层面，让处理-输出这个阶段做到“恰好一次”，那么可以将 Kafka 的偏移量与最终结果存储在同一个数据库实例上，这就需要修改代码，一开始，需要从外部数据库上获取最新的偏移量：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">//从外部数据库获取偏移量</span></span><br><span class="line"><span class="keyword">val</span> fromOffsets: <span class="type">Map</span>[<span class="type">TopicPartition</span>, <span class="type">Long</span>] = setFromOffsets(offsetList)</span><br><span class="line">...</span><br><span class="line"><span class="comment">// 用最新的offset得到初始化RDD</span></span><br><span class="line"><span class="keyword">val</span> messages = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](ssc,</span><br><span class="line"><span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>,</span><br><span class="line"><span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topicsSet, kafkaParams, fromOffsets))</span><br></pre></td></tr></table></figure>
<p>在最后输出的操作里，由于偏移量与最终数据处理结果要保存到同一个数据库，因此可以利用外部数据库的事务特性，完成最后的工作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">messages.map(…).foreachRDD(mess =&gt; &#123;</span><br><span class="line">   <span class="comment">// 获取offset集合</span></span><br><span class="line">   <span class="keyword">val</span> offsetsList = mess.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">   <span class="comment">// 将修改offset与输出最后结果作为一个事务提交</span></span><br><span class="line">   <span class="comment">//    transaction&#123;</span></span><br><span class="line">   <span class="comment">//       yourUpdateTheOffset(offsetsList)</span></span><br><span class="line">   <span class="comment">//       yourOutputToDatabase(mess)</span></span><br><span class="line">   <span class="comment">//    &#125;</span></span><br><span class="line">&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这样一来，Spark Streaming 才算是真正实现了端到端的消息送达保证。</p>
<p><strong>在实际开发中，将偏移量和输出结果存储到同一个外部数据库的方式用得并不多，因为这会使业务数据与消息数据耦合在一起，结构不够优雅，反而幂等输出更加流行。</strong></p>
<p>最后，来看看 Spark Streaming 的输出操作。Spark Streaming 也是懒加载模式，同样需要类似于 RDD 的行动算子才能真正开始运行，在 Spark Streaming 中，我们称其为输出算子，一共有下面这几种。</p>
<ul>
<li>
<p>print()：打印 DStream 中每个批次的前十个元素。</p>
</li>
<li>
<p>saveAsTextFiles(prefix, [suffix])：将 DStream 中的内容保存为文本文件。</p>
</li>
<li>
<p>saveAsObjectFiles(prefix, [suffix])：将 DStream 中的内容保存为 Java 序列化对象的 SequenceFile。</p>
</li>
<li>
<p>saveAsHadoopFiles(prefix, [suffix])：将 DStream 中的内容保存为 Hadoop 序列化格式（Writable）的文件，可以指定 K、V 类型。</p>
</li>
<li>
<p>foreachRDD(func)：该算子是 Spark Streaming 独有的，与 transform 算子类似，都是直接可以操作 RDD，我们可以利用该算子来做一些处理工作，例如生成 Parquet 文件写入 HDFS、将数据插入到外部数据库中，如 HBase、Elasticsearch。</p>
</li>
</ul>
<h2 id="容错与结果正确性">容错与结果正确性</h2>
<p>介绍了 Spark Streaming 的架构、用法之后，将会讨论 Spark Streaming 的容错机制，以及结果的正确性保证。要想 Spark Streaming 应用能够全天候无间断地运行，需要利用 Spark 自带的 Checkpoint 容错机制。Checkpoint 会在 Spark Streaming 运行过程中，周期性地保存一些作业相关信息，这样才能让 Spark Streaming 作业从故障（例如系统故障、JVM 崩溃等）中恢复。<strong>值得注意的是，作为 Checkpoint 的存储系统，是必须保证高可用的，常见的如 HDFS 就很可靠，更优的选择则是 Alluxio</strong>。</p>
<p>Checkpoint 主要保存了以下两类信息，其中元数据检查点主要用来恢复 Driver 程序，数据检查点主要用来恢复 Executor 程序。下面我们来分别介绍一下这两类信息：</p>
<h3 id="元数据检查点">元数据检查点</h3>
<p>元数据主要包括。</p>
<ul>
<li>配置：创建该 Spark Streaming 应用的配置。</li>
<li>DStream 算子：Spark Streaming 作业中定义的算子。</li>
<li>未完成的批次：那些还在作业队列里未完成的批次。</li>
</ul>
<p>Checkpoint 会周期性地将这些信息保存至外部可靠存储（如 HDFS、Alluxio）。</p>
<h3 id="数据检查点">数据检查点</h3>
<p>将中间生成的 RDD 保存到可靠的外部存储中。我们讨论过，如果要使用状态管理的算子，如 updateStateByKey、mapWithState 等，就必须开启 Checkpoint 机制，因为这类算子必须保存中间结果，以供下次计算使用。另外，我们知道 Spark 本身的容错机制是依靠 RDD DAG 的依赖关系通过计算恢复的，但是这也会造成依赖链过长、恢复时间过长的问题，因此我们必须周期性地存储中间结果（状态）至可靠的外部存储来缩短依赖链。我们也可以手动调用 DStream 的 checkpoint 算子进行缓存。如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds: <span class="type">DStream</span>[<span class="type">Int</span>] = ...</span><br><span class="line"><span class="keyword">val</span> cds: <span class="type">DStream</span>[<span class="type">Int</span>] = ds.checkpoint(<span class="type">Seconds</span>(<span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<p>Checkpoint 机制会按照设置的间隔对 DStream 进行持久化。如果需要启用 Checkpoint 机制，需要对代码做如下改动：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> checkpointDirectory = <span class="string">&quot;/your_cp_path&quot;</span> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">functionToCreateContext</span></span>(): <span class="type">StreamingContext</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Checkpoint&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf,<span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line">      <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>,<span class="number">9999</span>) </span><br><span class="line">      ssc.checkpoint(checkpointDirectory)</span><br><span class="line">      ssc</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> context = <span class="type">StreamingContext</span>.getOrCreate(checkpointDirectory, </span><br><span class="line"> functionToCreateContext _)</span><br></pre></td></tr></table></figure>
<p>StreamingContext 需要以 getOrCreate 的方式初始化。这样就能保证，如果从故障中恢复，会获取到上一个检查点的信息。</p>
<p>如果数据源是文件，那么上面的方法可以保证完全不丢数据，因为所有的状态都可以根据持久化的数据源复现出来，但如果是流式数据源，想要保证不丢数据是很困难的。因为当 Driver 出故障的时候，有可能接收的数据会丢失，并且不能找回。为了解决这个问题，Spark 1.2 之后引入了预写日志（WAL），Spark Streaming WAL 指的是接收到数据后，在数据处理之前，先对数据进行持久化，完成这个工作的是 BlockManagerBasedBlockHandler 类的实现类WriteAheadLog BasedBlockHandler 。如果开启了 WAL，那么数据会先进行持久化再写到 Executor 的内存中。这样即使内存数据丢失了，在 Driver 恢复后，丢失的数据还是会被处理，这就实现了“至少一次”的消息送达语义。打开 WAL 的方式为：设置spark.streaming.receiver.writeAheadLog.enable 为 true。这种方法其实是对 Receiver 的容错。</p>
<p>那么 Checkpoint 就以这种形式完成了 Driver、Executor 和 Receiver 的容错。下面我们来讨论一下 Spark Streaming 计算结果的正确性。</p>
<p>先来看看消息送达保证，Spark Streaming 框架本身实现“恰好一次”的消息送达语义比较容易，因为 Spark Streaming 本质上还是进行的批处理，所以它只需在批的层面通过 BatchId 追踪数据处理情况，这和 Spark 是完全一致的，因此它完全能够保证一个批只被处理一次，当一个批没有被成功处理时，肯定就是发生了故障，这时 Checkpoint 机制能够保证从最近持久化的中间结果与待执行的计算任务（DStreamGraph）开始重新计算，保证数据只被处理一次，从而得到正确的结果，<strong>该阶段可以认为是处理阶段的消息送达保证</strong>。</p>
<p><strong>在流处理场景下，容错问题与结果正确性问题不能孤立地来看待，而是需要考虑在出现故障的情况下如何能够保证结果的正确性</strong>。</p>
<h2 id="小结">小结</h2>
<p>将流式处理流程抽象为输入-处理-输出，而基于这个流程，又将流程拆分为三个部分：</p>
<ul>
<li>
<p>输入-处理</p>
</li>
<li>
<p>处理</p>
</li>
<li>
<p>处理-输出</p>
</li>
</ul>
<p><strong>而这每个部分，都需要假设错误会经常发生的情况下，还要保证“恰好一次”的消息送达保证，才是真正的端到端的消息送达保证，这也是生产环境中必须考虑的问题</strong>。Spark Streaming 给出了答案，其中处理-输出这个过程，通常会以幂等的方式解决，这也是在生产环境中非常常用的做法。</p>
<h2 id="思考题">思考题</h2>
<p>用偏移量与最终数据处理结果保存到同一个数据库，这么做的缺点是什么？</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Leo Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://liule8.github.io/post/27968528.html">https://liule8.github.io/post/27968528.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://liule8.github.io" target="_blank">Leo's notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/images/posts/cover/spark.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/533482d9.html"><img class="prev-cover" src="/images/posts/cover/spark.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark实战之流处理-统一批处理与流处理：Dataflow</div></div></a></div><div class="next-post pull-right"><a href="/post/9c7fe8ec.html"><img class="next-cover" src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据结构与算法之基础篇-二叉树基础(上)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/69b6c9e6.html" title="Spark实战之基础-Hadoop"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-Hadoop</div></div></a></div><div><a href="/post/bf0f3957.html" title="Spark实战之基础-MapReduce"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-MapReduce</div></div></a></div><div><a href="/post/abfe544b.html" title="Spark实战之基础-YARN"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-YARN</div></div></a></div><div><a href="/post/2a7649c3.html" title="Spark实战之基础-如何选择 Spark 编程语言以及部署 Spark"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-07</div><div class="title">Spark实战之基础-如何选择 Spark 编程语言以及部署 Spark</div></div></a></div><div><a href="/post/99c2f5e4.html" title="Spark实战之基础-解析 Spark 数据处理与分析场景"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-07</div><div class="title">Spark实战之基础-解析 Spark 数据处理与分析场景</div></div></a></div><div><a href="/post/d10a7c4d.html" title="Spark实战之流处理-流处理"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-15</div><div class="title">Spark实战之流处理-流处理</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Leo Liu</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">400</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">84</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/liule8"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/LIULE8" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:leo.liu.scau@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">这里是公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">如何在生产环境中使用 Spark Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA"><span class="toc-number">1.1.</span> <span class="toc-text">输入和输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E4%B8%8E%E7%BB%93%E6%9E%9C%E6%AD%A3%E7%A1%AE%E6%80%A7"><span class="toc-number">1.2.</span> <span class="toc-text">容错与结果正确性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%83%E6%95%B0%E6%8D%AE%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-number">1.2.1.</span> <span class="toc-text">元数据检查点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="toc-number">1.2.2.</span> <span class="toc-text">数据检查点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.3.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">1.4.</span> <span class="toc-text">思考题</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/a79999eb.html" title="数据结构与算法之基础篇-递归树"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-递归树"/></a><div class="content"><a class="title" href="/post/a79999eb.html" title="数据结构与算法之基础篇-递归树">数据结构与算法之基础篇-递归树</a><time datetime="2021-11-20T19:27:51.000Z" title="发表于 2021-11-20 19:27:51">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/6fd3f48a.html" title="数据结构与算法之基础篇-红黑树(下)"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-红黑树(下)"/></a><div class="content"><a class="title" href="/post/6fd3f48a.html" title="数据结构与算法之基础篇-红黑树(下)">数据结构与算法之基础篇-红黑树(下)</a><time datetime="2021-11-20T17:23:16.000Z" title="发表于 2021-11-20 17:23:16">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/76c8c5cb.html" title="数据结构与算法之基础篇-红黑树(上)"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-红黑树(上)"/></a><div class="content"><a class="title" href="/post/76c8c5cb.html" title="数据结构与算法之基础篇-红黑树(上)">数据结构与算法之基础篇-红黑树(上)</a><time datetime="2021-11-20T14:13:02.000Z" title="发表于 2021-11-20 14:13:02">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/8564d9ad.html" title="数据结构与算法之基础篇-二叉树基础(下)"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-二叉树基础(下)"/></a><div class="content"><a class="title" href="/post/8564d9ad.html" title="数据结构与算法之基础篇-二叉树基础(下)">数据结构与算法之基础篇-二叉树基础(下)</a><time datetime="2021-11-20T10:42:08.000Z" title="发表于 2021-11-20 10:42:08">2021-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/533482d9.html" title="Spark实战之流处理-统一批处理与流处理：Dataflow"><img src="/images/posts/cover/spark.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Spark实战之流处理-统一批处理与流处理：Dataflow"/></a><div class="content"><a class="title" href="/post/533482d9.html" title="Spark实战之流处理-统一批处理与流处理：Dataflow">Spark实战之流处理-统一批处理与流处理：Dataflow</a><time datetime="2021-11-20T09:55:00.000Z" title="发表于 2021-11-20 09:55:00">2021-11-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Leo Liu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>