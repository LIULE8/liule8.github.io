<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark实战之流处理-新一代流式计算框架：Structured Streaming | Leo's notes</title><meta name="keywords" content="Spark"><meta name="author" content="Leo Liu"><meta name="copyright" content="Leo Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="新一代流式计算框架：Structured Streaming 作为 Spark Streaming 的技术继任者，Structured Streaming 出现得其实有点晚，但是得益于 Spark 庞大的用户群体和社区，Structured Streaming 仍然是实时处理领域一种非常有竞争力和前景的技术。 从前面的 Spark Streaming 可以看到，Spark Streaming 仍然">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark实战之流处理-新一代流式计算框架：Structured Streaming">
<meta property="og:url" content="https://liule8.github.io/post/6dabd64e.html">
<meta property="og:site_name" content="Leo&#39;s notes">
<meta property="og:description" content="新一代流式计算框架：Structured Streaming 作为 Spark Streaming 的技术继任者，Structured Streaming 出现得其实有点晚，但是得益于 Spark 庞大的用户群体和社区，Structured Streaming 仍然是实时处理领域一种非常有竞争力和前景的技术。 从前面的 Spark Streaming 可以看到，Spark Streaming 仍然">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://liule8.github.io/images/posts/cover/spark.jpg">
<meta property="article:published_time" content="2021-11-28T08:23:47.000Z">
<meta property="article:modified_time" content="2021-12-03T12:34:17.419Z">
<meta property="article:author" content="Leo Liu">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liule8.github.io/images/posts/cover/spark.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://liule8.github.io/post/6dabd64e"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-03 12:34:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/images/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">426</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">84</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/images/posts/cover/spark.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Leo's notes</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark实战之流处理-新一代流式计算框架：Structured Streaming</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-11-28T08:23:47.000Z" title="发表于 2021-11-28 08:23:47">2021-11-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-03T12:34:17.419Z" title="更新于 2021-12-03 12:34:17">2021-12-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Spark/">Spark</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark实战之流处理-新一代流式计算框架：Structured Streaming"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>新一代流式计算框架：Structured Streaming</h1>
<p>作为 Spark Streaming 的技术继任者，Structured Streaming 出现得其实有点晚，但是得益于 Spark 庞大的用户群体和社区，Structured Streaming 仍然是实时处理领域一种非常有竞争力和前景的技术。</p>
<p>从前面的 Spark Streaming 可以看到，Spark Streaming 仍然采用 RDD 与算子的组合进行编程。这其实是 Spark 官方不推荐的，它的缺点也显而易见，而 Structured Streaming 的改进是由内而外的，<strong>它不仅参考了 MillWheel 系统与 Dataflow 模型，并且还引入 DataFrame 与 SQL 作为自己的编程接口</strong>。</p>
<h2 id="Structured-Streaming-抽象与架构">Structured Streaming 抽象与架构</h2>
<p>前面提过，Spark Streaming 的本质是将数据流抽象成流，以微批的方式进行处理，而 Structured Streaming 则不同，它采用的是 Google Dataflow 的思想，将数据流抽象成<strong>无边界表（unbounded table）</strong>，如下图所示：</p>
<p><img src="http://image.leonote.cn/20211128082633.png" alt="image-20211128082632955"></p>
<p><strong>这其实是将流抽象成了批</strong>，在这种抽象中，实时的数据流会不停追加到下图这张表中。我们通过对输入表查询的方式得到流处理的结果，并生成结果表。在每个触发间隔，新的数据将追加到输入表中，最终触发计算将会更新结果表，如下图所示：</p>
<p><img src="http://image.leonote.cn/20211128090220.png" alt="image-20211128090220478"></p>
<p>第二行的方块表示<strong>结果表（Result Table）</strong>，第三行的方块表示输出的外部存储，结果表更新到外部存储的方式，也就是输出模式，有 3 种，分别是完全模式、追加模式与更新模式。</p>
<ul>
<li><strong>完全模式</strong>：整个更新的结果表将被写入外部存储，由存储连接器来决定如何写入整张表。上图即为完全模式下的输出结果，一般用来调试。</li>
<li><strong>追加模式</strong>：只将上次触发后追加到结果表的数据写到外部存储。这种模式适用于不希望修改已经存在于结果表的数据场景。这是默认的输出模式。在这种模式下，窗口内定时触发生成的中间结果会保存到中间状态，最后一次触发生成的结果才会写到结果表。</li>
<li><strong>更新模式</strong>：只将上次触发后更新到结果表的数据写到外部存储。这与完全模式不同，它只输出上次触发后变更的结果记录。如果窗口中不包含聚合逻辑，则意味着不需要修改以前结果表中的数据，其实与追加模式无异。</li>
</ul>
<p><strong>这些其实就是对应的Dataflow的触发模式。</strong></p>
<p>作为 MillWheel 和 Dataflow 的实现者，实现端到端恰好一次的消息送达保证是必不可少的，为了实现这一目标，Structured Streaming 引入了 Source、Sink 和 StreamExecution 等组件，可以准确地追踪处理进度，以便从重启或重新处理中处理任何类型的故障。每个流式数据源（Source）都具有偏移量（类似于 Kafka 的偏移量）追踪读取进度，以便在故障发生后进行数据回放。在每次触发计算时，执行引擎（StreamExecution）使用检查点和预写日志机制记录来记录数据源偏移量。数据输出（Sink）允许自定义，如设计成幂等效果或者采取两段式提交。结合可重放的数据源与可靠的数据输出，Structured Streaming 可以在任何时候实现端到端的恰好一次消息送达保证。</p>
<p>Structured Streaming 不会保存整个无边界表的数据。它会从数据源读取数据，以增量处理的方式来更新结果，然后丢弃这些源数据，只会保留更新结果所需的最小化中间状态数据（如中间计数）。Structured Streaming 支持基于事件时间处理数据，对于晚到和乱序数据的处理，也同样引入了水位机制，后面会详细介绍。</p>
<p>Structured Streaming 也遵循 Driver 和 Executor 的主从架构，其功能与前面介绍的无异：Driver 负责调度，Executor 负责执行，如下图所示。其中 Driver 和前面介绍的 Driver 差别不大，SQLExecution 变成了 StreamExecution，Executor 多了 StateStore，但是取消了和 Receiver 相关的组件，这是一个比较大的变化，代表接收数据的方式变了。下面我将就这两个组件展开讲解。</p>
<p><img src="http://image.leonote.cn/20211128090343.png" alt="image-20211128090343469"></p>
<h3 id="StreamExecution">StreamExecution</h3>
<p>StreamExecution 是 Structured Streaming 的执行引擎，是 StreamingQuery 接口的实现，在用户的代码中被初始化，start() 方法是这个过程的入口，如下面的代码所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> lines = spark</span><br><span class="line">.readStream</span><br><span class="line">.format(<span class="string">&quot;socket&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;host&quot;</span>, <span class="string">&quot;localhost&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;port&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">.load()</span><br><span class="line">...</span><br><span class="line"><span class="keyword">val</span> query:  <span class="type">QueryExecution</span> = wordCounts.map(…).writeStream</span><br><span class="line">.trigger(<span class="type">ProcessingTime</span>(<span class="number">2.</span>seconds))</span><br><span class="line">.outputMode(<span class="string">&quot;complete&quot;</span>)</span><br><span class="line">.format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">.start()</span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>
<p>StreamExecution 有几个比较重要的成员变量：Trigger、LogicalPlan、Sink 和 Source。其中 Trigger 表示用户设定的触发间隔，它决定了一次处理的数据大小，LogicalPlan 代表执行计划，从用户代码的 start 方法开始，就进入了 StreamExecution。首先，可以看见这里设置触发器为每 2 分钟（处理时间）处理一次，用户从 Source 中取得相应的数据；接着根据用户定义的运算逻辑和 Trigger 得到优化过后的 IncrementalExecution，最后再交给 Sink 的 addBatch 方法触发整个过程执行；执行完成后，会通知 Source 修改相应的偏移量。<strong>其中 IncrementalExecution 是 Structured Streaming 独有的，它体现了 Structured Streaming 的执行方式是增量微批，主要针对前面 3 种输出模式进行优化。</strong> Source、Sink 和 StreamExecution 抽象了整个流处理过程。从上面的这个过程可以看出，StreamExecution 存在的目的还是要将流式数据转化为 DataFrame 的执行计划并执行，其实最后还是由 SQLExecution 负责执行，也就是与批处理同样的执行引擎。这样的好处就在于无缝对接了 DataFrame 与 Tungsten 巨大的性能优化，并且统一了流处理和批处理的计算引擎。</p>
<p>再来看看 query 初始化的代码，与前面的 Dataflow 的样例代码非常相似，在这段代码中我们定义了 Trigger、输出模式、处理逻辑。在处理逻辑中，还可以定义窗口和水位，完全是一种 Dataflow 思想的体现。其中，Trigger 组件的选项有以下几个。</p>
<ul>
<li>
<p>未指定（默认）。如果没有指定触发器，默认的触发逻辑是微批处理，一旦前一个微批完成处理，将立即生成下一个微批进行处理。</p>
</li>
<li>
<p>固定间隔的微批触发器。将以固定间隔进行触发，固定间隔长度由用户设置（默认的触发器实际上固定间隔长度为 0，即不固定）。如果先前的微批在当前间隔完成，则等到该间隔结束再开始下一个微批处理；如果前一个微批处理需要的时间超过了间隔长度，那么下一个微批将在前一个微批完成后立即执行，而不是等待下一个间隔边界；如果没有数据，则不会触发新的微批处理。固定间隔的微批触发器设定如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="number">2000</span>))</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>一次性微批触发器。查询只会触发一次针对所有可用数据的微批处理，然后自行停止。**这在希望定期启动集群来处理上一个时间段里累积的所有数据，然后停止集群的场景里非常有用。**在某些情况下，可以显著减少性能开销。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.trigger(<span class="type">Trigger</span>.<span class="type">Once</span>())</span><br></pre></td></tr></table></figure>
</li>
<li>
<p>固定检查点间隔的连续触发器。固定检查点间隔的连续触发器是 Spark 新加入的流执行模式——连续处理的体现。连续处理是 Spark 2.3 中引入的实验性质的流执行模式，可以实现约 1ms 的端到端延迟，并且实现了“至少一次”消息送达保证，而默认的微批处理引擎虽然实现了“恰好一次”的消息送达保证，但是延迟最少也要 100ms 左右。对于某些类型的查询，我们可以通过只修改触发器而不修改应用逻辑来启用该特性。如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.trigger(<span class="type">Trigger</span>.<span class="type">Continuous</span>(<span class="string">&quot;1 second&quot;</span>)) <span class="comment">// 只需修改这一行</span></span><br></pre></td></tr></table></figure>
<p>参数 1 秒指的是检查点间隔，意味着执行引擎每秒会记录一次执行进度，得到的检查点文件将采用和微批执行引擎兼容的格式，所以我们可以在不同触发器之间任意切换，例如以微批触发模式启动的查询，可以以连续触发模式重新启动。无论何时切换，用户都可以获得“至少一次”的消息传递保证。目前连续触发器只支持特定的操作、数据源和输出。</p>
</li>
</ul>
<p>从触发器的多样性来看，Dataflow 中的触发器功能无疑更加强大，既提供了固定周期触发器（AtPeriod），也可以基于水位（AtWatermark）等，但按照 Structured Streaming 的发展速度，相信很快能够实现更多类型的触发器。</p>
<h3 id="StateStore">StateStore</h3>
<p>StateStore 的作用是作为数据流转的状态存储（持久化）。它本质是一个分布式、高可用、分版本的键值存储，提供 get、put、remove 等增删改查操作。它的分片逻辑是算子编号（operatorId）加分区编号（partitionId）。</p>
<h2 id="操作">操作</h2>
<p>既然使用了 DataFrame 与 SQL，因此从使用上来说，无论是普通的转换 API 还是 Spark SQL，都是完全一样的，甚至一些封装过的批处理的代码都可以直接复用。那么按照 Dataflow的理论，这就是 what 部分，我们还要解决：</p>
<ul>
<li>根据事件时间，哪些数据会参与计算（where）；</li>
<li>什么时候触发计算（when）；</li>
<li>早期的计算结果如何被修正（how）。</li>
</ul>
<p>前面其实已经介绍过相应的内容，<strong>where 由窗口来指定，when 由 Trigger 组件来指定，how 由 watermark 机制来解决。</strong></p>
<p>下面这个例子包含了 what、where、when、how 的声明：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">val</span> windowDuration = <span class="string">&quot;3000 seconds&quot;</span></span><br><span class="line"><span class="keyword">val</span> slideDuration = <span class="string">&quot;1000 seconds&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> lines = spark.readStream</span><br><span class="line">.format(<span class="string">&quot;socket&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;host&quot;</span>, <span class="string">&quot;localhost&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;port&quot;</span>, <span class="number">9999</span>)</span><br><span class="line">.option(<span class="string">&quot;includeTimestamp&quot;</span>, <span class="literal">true</span>)</span><br><span class="line">.load()</span><br><span class="line"></span><br><span class="line"> <span class="keyword">val</span> words = lines.as[(<span class="type">String</span>, <span class="type">TimestampType</span>)].flatMap(line =&gt;</span><br><span class="line">   line._1.split(<span class="string">&quot; &quot;</span>).map(word =&gt; (word, line._2))</span><br><span class="line"> ).toDF(<span class="string">&quot;word&quot;</span>, <span class="string">&quot;timestamp&quot;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> windowedCounts = words.groupBy(</span><br><span class="line">window($<span class="string">&quot;timestamp&quot;</span>, windowDuration, slideDuration), </span><br><span class="line">$<span class="string">&quot;word&quot;</span>)</span><br><span class="line">.count().orderBy(<span class="string">&quot;window&quot;</span>)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>上面这段代码的计算逻辑如下图所示：</p>
<p><img src="http://image.leonote.cn/20211128090512.png" alt="image-20211128090512220"></p>
<p>上图中，下方的结果表显示了 5 分钟滑动步长以及 10 分钟大小的窗口聚合结果。窗口跨度也是结果表的一个字段，该字段名为 window，从图中虚线箭头可以看出，触发器（Trigger）采用的是固定间隔触发，每 5 分钟触发一次。聚合的时间窗口是以数据的事件时间为准的，那么这样一定会存在晚到和乱序数据的问题。与 Google Dataflow 模型类似，Structured Streaming 也基于水位机制对晚到和乱序数据提供了相应处理机制，如下图所示：</p>
<p><img src="http://image.leonote.cn/20211128090542.png" alt="image-20211128090541936"></p>
<p>当 12:04 的晚到数据在 12:11 到达时，它实际应该落在 12:00-12:10 的窗口里。在下一次触发时间（12:15）到来时，会更新结果表中的结果。Structured Streaming 会将这种类似聚合需求长时间维护在一个中间状态以便处理晚到数据，但是，<strong>如果对于长期运行的应用，对晚到数据无限期等待的代价或许会很大，这时系统应该有一种方法知道什么时候应该丢弃那些过时的中间结果，这样应用也不用再去处理这种晚到数据了。</strong> 和 Google Dataflow 一样，Structured Streaming 也采用了水位机制，旨在让计算引擎追踪数据中当前的事件时间，并尝试清除过时的中间状态。在 Structured Streaming 中，水位定义为当前最大的事件时间减去晚到时间最大容忍值。对一个 T 时刻的特定窗口，计算单元会维护其结果状态，当计算单元接收到的晚到数据，满足观察到的最大事件时间减去晚到时间最大容忍值，大于 T 的条件时，都允许晚到数据修改结果状态。换句话说，在水位线前面的数据会被处理，后面的数据则会被丢弃，如下图所示。</p>
<p><img src="http://image.leonote.cn/20211128090612.png" alt="image-20211128090612825"></p>
<p>坐标轴中的虚线表示了至今为止计算单元观察到的最大事件时间；浅色实心圆点代表正常处理的数据；深色实心圆点代表了晚到数据，但是在可以容忍的晚到范围之内，而空心圆点数据表示晚到且在不能容忍的时间范围之内。判断是否可以容忍的依据是阶梯形实线（当前最大事件时间减去最大容忍晚到时间），也就是上面说的水位，在每个触发间隔前，会重新计算水位。晚到数据以这种机制被计算、被丢弃，最后更新结果表。</p>
<p>上图是更新输出模式，所以在每个触发点都会输出结果表，晚到数据会在后面的触发点再更新到结果表。</p>
<p>开启 Structured Streaming 的水位机制很简单，只需在代码中加上 withWatermark：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> windowedCounts = words</span><br><span class="line"><span class="comment">// 最大容忍时间是5分钟，指定水位基于的列名timestamp</span></span><br><span class="line">.withWatermark(<span class="string">&quot;timestamp&quot;</span>, <span class="string">&quot;5 minmutes&quot;</span>)</span><br><span class="line">.groupBy(</span><br><span class="line">window(</span><br><span class="line">$<span class="string">&quot;timestamp&quot;</span>, </span><br><span class="line">windowDuration, </span><br><span class="line">slideDuration), </span><br><span class="line">$<span class="string">&quot;word&quot;</span>)</span><br><span class="line">.count()</span><br></pre></td></tr></table></figure>
<p>想要使用水位机制需要满足一定条件，具体如下。</p>
<ul>
<li>输出模式必须是更新模式和追加模式。在这两种模式中，会导致结果输出有所差别，在更新模式中，晚到的数据会被处理后再修改结果表，如图f所示，但是在追加模式中，由于不能修改结果表的内容，因此只有在水位超过了窗口结束时间后的下一个触发点才会触发聚合操作，也就是说，在追加模式中，<strong>是等所有晚到时间小于最大容忍时间的数据都接收到后，再进行聚合操作，并一次性输出到结果表</strong>，如下图所示，这两种模式的差别主要考虑的是接收器的差别，有些接收器不能进行修改，如文件系统。</li>
<li>聚合操作必须有事件时间列，或者有一个基于事件时间列的窗口。</li>
<li>水位机制与聚合操作中使用的时间列必须相同。</li>
<li>水位机制必须在聚合操作之前被调用。</li>
</ul>
<p><img src="http://image.leonote.cn/20211128090646.png" alt="image-20211128090646065"></p>
<h2 id="输入与输出">输入与输出</h2>
<p>在 Structured Streaming 中，输入被统一抽象为 Source，与 Spark Streaming 中各种不同的输入源不同，输出也被统一抽象为 Sink，这在 Spark Streaming 并没有进行抽象，这么做的好处不言而喻，有了统一的输出抽象，要实现端到端级别的“恰好一次”消息送达语义的实现就不再那么困难。也就是说，Structured Streaming 将整个流处理过程抽象为“Source + StreamExecution + Sink”，对应我们前面讲的“输入-处理-输出”过程，这样 Structured Streaming 就有能力为用户提供端到端级别“恰好一次”消息送达语义的实现。</p>
<p>下面分别来看看 Source 和 Sink 组件。</p>
<h3 id="Source">Source</h3>
<p>Source 抽象了流式数据从接收到处理之间的过程，它是一个接口，其定义非常简单，如下面的代码所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Source</span> </span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">schema</span></span>: <span class="type">StructType</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getOffset</span></span>: <span class="type">Option</span>[<span class="type">Offset</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getBatch</span></span>(start: <span class="type">Option</span>[<span class="type">Offset</span>], end: <span class="type">Offset</span>): <span class="type">DataFrame</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">commit</span></span>(end: <span class="type">Offset</span>) : <span class="type">Unit</span> = &#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从这个接口可以看出，无论是哪种数据源，都必须实现 getOffset 方法，这意味着在 Structured Streaming 中，所有数据源都可以向 Spark Streaming 中 Kafka Direct API 那样通过维护偏移量的方式进行容错（回放）。所有的数据源皆是如此，也就意味着，Source 完全抛弃了基于 Receiver 的数据接收方式，省事省心。<br>
目前 Source 的实现类有 KafkaSource 和 FileStreamSource，分别代表了 Kafka 数据源和 HDFS 数据源，还提供了用于测试的控制台数据源、Socket 数据源和 Rate 数据源（Rate 数据源能以指定速率生成数据，生成的每条数据都带有时间戳和消息 ID）的实现类。KafkaSource 很好理解，它本身的数据结构中就自带 offset，可以很好地和接口中 getOffset 相匹配，而 HDFS 的 offset 概念稍微复杂一点，每当调用 getOffset 方法时，HDFS 都会扫描文件夹下的文件，将最新的一些文件进行聚合成批，再赋予一个递增数字，FileStreamSource 对象中会维护一个名为 seenFiles 的 HashMap，用来保存已经扫描过的文件，判断依据是文件年龄和文件名。不管是哪种偏移量（除了 TextSocketSource），最后都会被 StreamExecution 预写日志 WAL 以便进行回放。这与 Spark Streaming 中介绍的思路是完全一样的，只不过 Structured Streaming 已经在这个环节中做到了对用户完全透明并泛化到所有数据源。</p>
<p>下面来看一个从 KafkaSource 中读取数据的例子，在这个例子中，会根据Kafka地址读取消息并将其转换为结构化数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">.builder</span><br><span class="line">.master(<span class="string">&quot;local[2]&quot;</span>)</span><br><span class="line">.appName(<span class="string">&quot;StructuredNetworkWordCount&quot;</span>)</span><br><span class="line">.getOrCreate()</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"> </span><br><span class="line"><span class="keyword">val</span> df = spark</span><br><span class="line">.readStream</span><br><span class="line">.format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">.option(<span class="string">&quot;subscribe&quot;</span>, <span class="string">&quot;topicA&quot;</span>)</span><br><span class="line">.load()</span><br><span class="line"> </span><br><span class="line">df.selectExpr(<span class="string">&quot;CAST(key AS STRING)&quot;</span>, <span class="string">&quot;CAST(value AS STRING)&quot;</span>).as[(<span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>
<h3 id="Sink">Sink</h3>
<p>Sink 是 Structured Streaming 对输出过程的抽象，目的也是实现对用户透明的容错处理。与Source 对应，Sink 也有 FileStreamSink、KafkaSink、ForeachSink 等，分别对应的输出为 HDFS、Kafka 和自定义 Sink，此外还提供了用于测试的控制台输出和内存输出的实现类。Sink 接口非常简单，如下面的代码所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Sink</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">addBatch</span></span>(batchId: <span class="type">Long</span>, data: <span class="type">DataFrame</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中只定义了一个方法 addBatch，整个计算过程都是由这个方法触发执行的，现在来深入了解下这个方法，以 KafkaSink 为例：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">addBatch</span></span>(batchId: <span class="type">Long</span>, data: <span class="type">DataFrame</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="keyword">if</span> (batchId &lt;= latestBatchId) &#123;</span><br><span class="line">     logInfo(<span class="string">s&quot;Skipping already committed batch <span class="subst">$batchId</span>&quot;</span>)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="type">KafkaWriter</span>.write(sqlContext.sparkSession,</span><br><span class="line">       data.queryExecution, executorKafkaParams, topic)</span><br><span class="line">     latestBatchId = batchId</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法中，先判断了是否是重复提交的数据，然后由 KafkaWriter 的 write() 方法进行写入，写入完成后，更新最新的 batchId。接下来我们来看看 write() 方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(</span><br><span class="line">     sparkSession: <span class="type">SparkSession</span>,</span><br><span class="line">     queryExecution: <span class="type">QueryExecution</span>,</span><br><span class="line">     kafkaParameters: ju.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>],</span><br><span class="line">     topic: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">       <span class="keyword">val</span> schema = queryExecution.analyzed.output</span><br><span class="line">       validateQuery(queryExecution, kafkaParameters, topic)</span><br><span class="line">       queryExecution.toRdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">        <span class="keyword">val</span> writeTask = <span class="keyword">new</span> <span class="type">KafkaWriteTask</span>(kafkaParameters, schema, topic)</span><br><span class="line">        <span class="type">Utils</span>.tryWithSafeFinally(block = writeTask.execute(iter))(</span><br><span class="line">          finallyBlock = writeTask.close())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这个方法中，由 queryExecution 的 toRDD 方法触发计算开始，在最后 foreachPartition 操作中完成写入 Kafka 的操作。最后的写入操作是在 KafkaWriteTask 的 execute 方法完成的，如下面的代码所示：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(iterator: <span class="type">Iterator</span>[<span class="type">InternalRow</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  producer = <span class="type">CachedKafkaProducer</span>.getOrCreate(producerConfiguration)</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext &amp;&amp; failedWrite == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> currentRow = iterator.next()</span><br><span class="line">    <span class="keyword">val</span> projectedRow = projection(currentRow)</span><br><span class="line">    <span class="keyword">val</span> topic = projectedRow.getUTF8String(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">val</span> key = projectedRow.getBinary(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> value = projectedRow.getBinary(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">if</span> (topic == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NullPointerException</span>(<span class="string">s&quot;null topic present in the data. Use the &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$&#123;KafkaSourceProvider.TOPIC_OPTION_KEY&#125;</span> option for setting a default topic.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 由上面构建好的topic、key和value生成最后待插入的消息</span></span><br><span class="line">    <span class="keyword">val</span> record = <span class="keyword">new</span> <span class="type">ProducerRecord</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>]](topic.toString, key, value)</span><br><span class="line">    <span class="keyword">val</span> callback = <span class="keyword">new</span> <span class="type">Callback</span>() &#123;</span><br><span class="line">     <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onCompletion</span></span>(recordMetadata: <span class="type">RecordMetadata</span>, e: <span class="type">Exception</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">       <span class="keyword">if</span> (failedWrite == <span class="literal">null</span> &amp;&amp; e != <span class="literal">null</span>) &#123;</span><br><span class="line">        failedWrite = e</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   producer.send(record, callback)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>插入完成后，按照前面的思路，还要更新 Source 的偏移量，这由 StreamExecution 中的 batchCommitLog.add(currentBatchId) 完成。整个过程与 Spark Streaming 中实现的过程大同小异，不同的是这个过程更加透明，用户不用直接维护偏移量，比起 Spark Streaming 的各种 API 调用，更加优雅。但是目前的版本，Spark Streaming 中的问题仍然存在，还是<strong>无法在事务层面保证处理-输出的这个环节做到完美的“恰好一次”，只能做到“至少一次”</strong>，需要通过使输出幂等来实现，或者使用 Structured Streaming 提供的去重操作：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 参数为去重列名</span></span><br><span class="line">streamingDf.dropDuplicates(<span class="string">&quot;uniquecolumn&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>目前 FileStreamSink 提供了幂等保证，用户不用自己实现幂等逻辑，而 ForeachSink 则提供了自定义的 Writer，因此是否幂等取决于用户自己是否实现，目前 ForeachWriter 只有 Scala 和 Java 接口。ForeachWriter 代码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ForeachWriter</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(partitionId: <span class="type">Long</span>,version: <span class="type">Long</span>): <span class="type">Boolean</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(value: <span class="type">T</span>): <span class="type">Unit</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(errorOrNull: <span class="type">Throwable</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ForeachWriter 是一个抽象类，任何继承它的类必须重写 open、process 和 close 这 3 个方法，这 3 个方法会在 Executor 上依次被调用：</p>
<ul>
<li>open 方法的参数 partitionId 代表了输出分区ID；</li>
<li>version 是一个单调递增的 ID，随着每次触发而增加，我们可以通过两个参数判断这一批数据是否继续输出。如果返回值为 true，就调用 process 方法；如果返回值为 false，则不会调用 process 方法。</li>
</ul>
<p>每当调用 open 方法时，close 方法也会被调用（除非 JVM 因为某些错误而退出），因此我们可以在 open 方法中打开外部存储连接，并在 close 方法中关闭外部存储连接。</p>
<h2 id="小结">小结</h2>
<p>介绍了 Structured Streaming 的相关内容，可以看到 Structured Streaming 忠实地复刻了 Dataflow 思路与实现，但是在声明计算逻辑环节复用了 Spark的DataFrame + Spark SQL，使得流处理编程变得十分简单，相信你也有相同的体会。另外，这样一来的话，<strong>Spark编程的批处理与流处理在形式上就得到了完美的统一</strong>。</p>
<h2 id="思考题">思考题</h2>
<p>在流处理中，连接操作涉及两个数据源的操作，会相对比较复杂，其中每个数据源都有可能是静态的数据，或者是动态的数据流，那么 Structured Streaming 是如何处理连接操作的呢，它支持哪些类型的连接，又有什么限制呢？</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Leo Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://liule8.github.io/post/6dabd64e.html">https://liule8.github.io/post/6dabd64e.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://liule8.github.io" target="_blank">Leo's notes</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/images/posts/cover/spark.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/a13191ee.html"><img class="prev-cover" src="/images/posts/cover/spark.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark实战之流处理-如何对 Spark 流处理进行性能调优</div></div></a></div><div class="next-post pull-right"><a href="/post/d0237848.html"><img class="next-cover" src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据结构与算法之基础篇-AC自动机</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/ce43d250.html" title="Spark实战之图挖掘-什么是图"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-30</div><div class="title">Spark实战之图挖掘-什么是图</div></div></a></div><div><a href="/post/2453efad.html" title="Spark实战之图挖掘-大规模并行图挖掘引擎 GraphX"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-01</div><div class="title">Spark实战之图挖掘-大规模并行图挖掘引擎 GraphX</div></div></a></div><div><a href="/post/afff6324.html" title="Spark实战之图挖掘-数据并行"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-30</div><div class="title">Spark实战之图挖掘-数据并行</div></div></a></div><div><a href="/post/69b6c9e6.html" title="Spark实战之基础-Hadoop"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-Hadoop</div></div></a></div><div><a href="/post/bf0f3957.html" title="Spark实战之基础-MapReduce"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-MapReduce</div></div></a></div><div><a href="/post/abfe544b.html" title="Spark实战之基础-YARN"><img class="cover" src="/images/posts/cover/spark.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-06</div><div class="title">Spark实战之基础-YARN</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/images/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Leo Liu</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">426</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">84</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">51</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/liule8"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/LIULE8" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:leo.liu.scau@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">这里是公告</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">新一代流式计算框架：Structured Streaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Structured-Streaming-%E6%8A%BD%E8%B1%A1%E4%B8%8E%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">Structured Streaming 抽象与架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#StreamExecution"><span class="toc-number">1.1.1.</span> <span class="toc-text">StreamExecution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#StateStore"><span class="toc-number">1.1.2.</span> <span class="toc-text">StateStore</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C"><span class="toc-number">1.2.</span> <span class="toc-text">操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E4%B8%8E%E8%BE%93%E5%87%BA"><span class="toc-number">1.3.</span> <span class="toc-text">输入与输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Source"><span class="toc-number">1.3.1.</span> <span class="toc-text">Source</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sink"><span class="toc-number">1.3.2.</span> <span class="toc-text">Sink</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-number">1.4.</span> <span class="toc-text">小结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-number">1.5.</span> <span class="toc-text">思考题</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/accf4e02.html" title="数据结构与算法之高级篇-最短路径"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之高级篇-最短路径"/></a><div class="content"><a class="title" href="/post/accf4e02.html" title="数据结构与算法之高级篇-最短路径">数据结构与算法之高级篇-最短路径</a><time datetime="2021-12-04T13:03:30.000Z" title="发表于 2021-12-04 13:03:30">2021-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/4233c11f.html" title="数据结构与算法之高级篇-拓扑排序"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之高级篇-拓扑排序"/></a><div class="content"><a class="title" href="/post/4233c11f.html" title="数据结构与算法之高级篇-拓扑排序">数据结构与算法之高级篇-拓扑排序</a><time datetime="2021-12-04T12:25:30.000Z" title="发表于 2021-12-04 12:25:30">2021-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/c24b527c.html" title="数据结构与算法之基础篇-动态规划实战"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-动态规划实战"/></a><div class="content"><a class="title" href="/post/c24b527c.html" title="数据结构与算法之基础篇-动态规划实战">数据结构与算法之基础篇-动态规划实战</a><time datetime="2021-12-02T19:39:46.000Z" title="发表于 2021-12-02 19:39:46">2021-12-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/181f652a.html" title="数据结构与算法之基础篇-动态规划理论"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-动态规划理论"/></a><div class="content"><a class="title" href="/post/181f652a.html" title="数据结构与算法之基础篇-动态规划理论">数据结构与算法之基础篇-动态规划理论</a><time datetime="2021-12-01T19:59:25.000Z" title="发表于 2021-12-01 19:59:25">2021-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/c0e7981d.html" title="数据结构与算法之基础篇-初识动态规划"><img src="/images/posts/cover/dataStructuresAndAlgorithms.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据结构与算法之基础篇-初识动态规划"/></a><div class="content"><a class="title" href="/post/c0e7981d.html" title="数据结构与算法之基础篇-初识动态规划">数据结构与算法之基础篇-初识动态规划</a><time datetime="2021-12-01T19:19:00.000Z" title="发表于 2021-12-01 19:19:00">2021-12-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Leo Liu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', '', 'katex-wrap')
  })
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>