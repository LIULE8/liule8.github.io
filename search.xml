<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring Data ElasticSearch]]></title>
    <url>%2F2021%2F01%2F31%2FSpringDataElasticSearch%2F</url>
    <content type="text"><![CDATA[Spring Data ElasticSearchSpring Data ElasticSearch 入门案例Spring Data 和 Elasticsearch 结合的时候，唯一需要注意的是版本之间的兼容性问题，Elasticsearch 和 Spring Boot 是同时向前发展的，而 Elasticsearch 的大版本之间还存在一定的 API 兼容性问题，所以必须要知道这些版本之间的关系，表格如下： Spring Data Release Train Spring Data Elasticsearch Elasticsearch Spring Boot 2020.0.0[1] 4.1.x[1] 7.9.3 2.4.x[1] Neumann 4.0.x 7.6.2 2.3.x Moore 3.2.x 6.8.12 2.2.x Lovelace 3.1.x 6.2.2 2.1.x Kay[2] 3.0.x[2] 5.5.0 2.0.x[2] Ingalls[2] 2.1.x[2] 2.4.0 1.5.x[2] 由于版本越新越便利，所以一般情况下直接采用最新的版本。 第一步：利用 Helm Chart 安装一个 Elasticsearch 集群 7.9.3 版本，执行命令如下。 121. helm2 repo add elastic https://helm.elastic.co2. helm2 install --name myelasticsearch elastic/elasticsearch --set imageTag=7.9.3 安装完之后，就可以看到如下信息。 这代表安装成功。 由于 ElasticSearch 是发展变化的，所以它的安装方式可以参考官方文档 然后利用 k8s 集群端口映射到本地，就可以开始测试了。 123~ ❯❯❯ kubectl port-forward svc/elasticsearch-master 9200:9200 -n my-namespaceForwarding from 127.0.0.1:9200 -&gt; 9200Forwarding from [::1]:9200 -&gt; 9200 第二步：在 gradle.build 里面配置 Spring Data ElasticSearch 依赖的 Jar 包。 依赖 Spring Boot 2.4.1 版本，完整的 gradle.build 文件如下所示。 1234567891011121314151617181920212223242526272829plugins &#123; id 'org.springframework.boot' version '2.4.1' id 'io.spring.dependency-management' version '1.0.10.RELEASE' id 'java'&#125;group = 'com.example.data.es'version = '0.0.1-SNAPSHOT'sourceCompatibility = '1.8'configurations &#123; compileOnly &#123; extendsFrom annotationProcessor &#125;&#125;repositories &#123; mavenCentral()&#125;dependencies &#123; implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'org.springframework.boot:spring-boot-starter-data-elasticsearch' implementation 'org.springframework.boot:spring-boot-starter-web' compileOnly 'org.projectlombok:lombok' developmentOnly 'org.springframework.boot:spring-boot-devtools' runtimeOnly 'io.micrometer:micrometer-registry-prometheus' annotationProcessor 'org.projectlombok:lombok' testImplementation 'org.springframework.boot:spring-boot-starter-test'&#125;test &#123; useJUnitPlatform()&#125; 第三步：新建一个目录，结构如下图所示，方便我们测试。 第四步：在 application.properties 里面新增 es 的连接地址，连接本地的 Elasticsearch。 1spring.data.elasticsearch.client.reactive.endpoints=127.0.0.1:9200 第五步：新增一个 ElasticSearchConfiguration 的配置文件，主要是为了开启扫描的包。 12345678package com.example.data.es.demo.es;import org.springframework.context.annotation.Configuration;import org.springframework.data.elasticsearch.repository.config.EnableElasticsearchRepositories;//利用@EnableElasticsearchRepositories注解指定Elasticsearch相关的Repository的包路径在哪里@EnableElasticsearchRepositories(basePackages = "com.example.data.es.demo.es")@Configurationpublic class ElasticSearchConfiguration &#123;&#125; 第六步：新增一个 Topic 的 Document，它类似 JPA 里面的实体，用来保存和读取 Topic 的数据，代码如下所示。 12345678910111213141516171819202122232425262728293031package com.example.data.es.demo.es;import lombok.Builder;import lombok.Data;import lombok.ToString;import org.springframework.data.annotation.Id;import org.springframework.data.elasticsearch.annotations.Document;import org.springframework.data.elasticsearch.annotations.Field;import org.springframework.data.elasticsearch.annotations.FieldType;import java.util.List;@Data@Builder@Document(indexName = "topic")@ToString(callSuper = true)//论坛主题信息public class Topic &#123; @Id private Long id; private String title; @Field(type = FieldType.Nested, includeInParent = true) private List&lt;Author&gt; authors;&#125;package com.example.data.es.demo.es;import lombok.Builder;import lombok.Data;@Data@Builder//作者信息public class Author &#123; private String name;&#125; 第七步：新建一个 Elasticsearch 的 Repository，用来对 Elasticsearch 索引的增删改查，代码如下所示。 1234567package com.example.data.es.demo.es;import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;import java.util.List;//类似JPA一样直接操作Topic类型的索引public interface TopicRepository extends ElasticsearchRepository&lt;Topic,Long&gt; &#123; List&lt;Topic&gt; findByTitle(String title);&#125; 第八步: 新建一个 Controller，对 Topic 索引进行查询和添加。 1234567891011121314151617@RestControllerpublic class TopicController &#123; @Autowired private TopicRepository topicRepository; //查询topic的所有索引 @GetMapping("topics") public List&lt;Topic&gt; query(@Param("title") String title) &#123; return topicRepository.findByTitle(title); &#125; //保存 topic索引 @PostMapping("topics") public Topic create(@RequestBody Topic topic) &#123; return topicRepository.save(topic); &#125;&#125; 第九步：发送一个添加和查询的请求测试一下。 发送三个 POST 请求，添加三条索引，代码如下所示。 123456789101112131415POST /topics HTTP/1.1Host: 127.0.0.1:8080Content-Type: application/jsonCache-Control: no-cachePostman-Token: d9cc1f6c-24dd-17ff-f2e8-3063fa6b86fc&#123; "title":"jack", "id":2, "authors":[&#123; "name":"jk1" &#125;,&#123; "name":"jk2" &#125;]&#125; 然后发送一个 get 请求，获得标题是 jack 的索引，如下面这行代码所示。 1GET http://127.0.0.1:8080/topics?title=jack 得到如下结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950GET http://127.0.0.1:8080/topics?title=jackHTTP/1.1 200 Content-Type: application/jsonTransfer-Encoding: chunkedDate: Wed, 30 Dec 2020 15:12:16 GMTKeep-Alive: timeout=60Connection: keep-alive[ &#123; "id": 1, "title": "jack", "authors": [ &#123; "name": "jk1" &#125;, &#123; "name": "jk2" &#125; ] &#125;, &#123; "id": 3, "title": "jack", "authors": [ &#123; "name": "jk1" &#125;, &#123; "name": "jk2" &#125; ] &#125;, &#123; "id": 2, "title": "jack", "authors": [ &#123; "name": "jk1" &#125;, &#123; "name": "jk2" &#125; ] &#125;]Response code: 200; Time: 348ms; Content length: 199 bytesCannot preserve cookies, cookie storage file is included in ignored list:&gt; /Users/jack/Company/git_hub/spring-data-jpa-guide/2.3/elasticsearch-data/.idea/httpRequests/http-client.cookies 使用 Spring Data Elasticsearch 来操作 ES 相关的 API 的话，比直接写 Http 的 client 要简单很多，因为这里面封装了很多基础逻辑，省去了很多重复造轮子的过程。 第十步：Elasticsearch Repository 的测试用例写法，如下面的代码和注释所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package com.example.data.es.demo;import com.example.data.es.demo.es.Author;import com.example.data.es.demo.es.Topic;import com.example.data.es.demo.es.TopicRepository;import org.assertj.core.util.Lists;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.TestPropertySource;import java.util.List;@SpringBootTest@TestPropertySource(properties = &#123;"logging.level.org.springframework.data.elasticsearch.core=TRACE", "logging.level.org.springframework.data.elasticsearch.client=trace", "logging.level.org.elasticsearch.client=TRACE", "logging.level.org.apache.http=TRACE"&#125;)//新增一些配置， 开启spring data elastic search的http的调用过程，可以查看一下日志public class ElasticSearchRepositoryTest &#123; @Autowired private TopicRepository topicRepository; @BeforeEach public void init() &#123;// topicRepository.deleteAll(); //可以直接删除所有索引 Topic topic = Topic.builder().id(11L) .title("jacktest") .authors(Lists.newArrayList(Author.builder() .name("jk1") .build())) .build(); topicRepository.save(topic);//集成测试保存索引 Topic topic1 = Topic.builder().id(14L) .title("jacktest") .authors(Lists.newArrayList(Author.builder() .name("jk1") .build())) .build(); topicRepository.save(topic1); Topic topic2 = Topic.builder().id(15L) .title("jacktest") .authors(Lists.newArrayList(Author.builder() .name("jk1") .build())) .build(); topicRepository.save(topic2);//保存索引 &#125; @Test public void testTopic() &#123; Iterable&lt;Topic&gt; topics = topicRepository.findAll(); topics.forEach(topic1 -&gt; &#123; System.out.println(topic1); &#125;); List&lt;Topic&gt; topicList = topicRepository.findByTitle("jacktest"); topicList.forEach(t -&gt; &#123; System.out.println(t);//获得索引的查询结果 &#125;); List&lt;Topic&gt; topicList2 = topicRepository.findByTitle("xxx"); topicList2.forEach(t -&gt; &#123; System.out.println(t);//也可以用断言测试 &#125;); &#125;&#125; 接着看一下测试用例的调用日志，从日志可以看出，调用的时候发生的 Http 的 PUT 请求，是用来创建和修改一个索引的文档的。请看下面的图片。 从中也可以看得出来，转化成 es 的 api 查询语法之后，发送的 post 请求又变成下图显示的样子。 Spring Data ElasticSearch 关键的类通过上面的案例可以知道，Spring Data ElasticSearch 的用法其实非常简单，并且通过日志也可以看到，底层实现是基于 http 请求，来操作 Elasticsearch 的 server 中的 api 进行的。 那么简单看一下这一框架还提供了哪些 ElasticSearch 的操作方法。看一下 Repository 的所有子类，如下图所示： 从图中可以看得出来，ElasticsearchRepository 是默认的 Repository 的实现类，如果继续往下面看源码的话，就可以看到里面进行了很多 ES 的 Http Client 操作。 同时再看一下 Structure 视图，如下所示。 从这张图可以知道，ElasticsearchRepository 默认提供了 search 和 index 相关的一些操作方法，并且 Spring Data Common 里面的一些公共方法同样适用，这和刚才演示的 Defining Method Query 的 JPA 语法同样适用，可以大大减轻操作 ES 的难度，提高了开发的效率，甚至像分页、排序、limit 等同样适用。 和 Spring Data JPA 用相同的思路，就可以很快掌握 Spring Data Elasticsearch 的基本用法，及其大概的实现思路。 ESRepository 和 JPARepository 同时存在假设刚才测试的样例里面，同时有关于 User 信息的 DB 操作，那么项目应该怎么写。 第一步：将对 Elasticsearch 的实体、Repository 和对 JPA 操作的实体、Repository 放到不同的文件里面，如下图所示。 第二步：新增 JpaConfiguration，用来指定 Jpa 相关的 Repository 目录，完整代码如下。 1234567package com.example.data.es.demo.jpa;import org.springframework.context.annotation.Configuration;import org.springframework.data.jpa.repository.config.EnableJpaRepositories;//利用 @EnableJpaRepositories 指定JPA的目录是 "com.example.data.es.demo.jpa"@EnableJpaRepositories(basePackages = "com.example.data.es.demo.jpa")@Configurationpublic class JpaConfiguration &#123;&#125; 第三步：新增 User 实体，用来操作用户基本信息。 1234567891011121314@Data@Builder@Entity@Table@NoArgsConstructor@AllArgsConstructor@ToStringpublic class User &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email;&#125; 第四步：新增 UserRepository，用来进行 DB 操作。 12345package com.example.data.es.demo.jpa;import org.springframework.data.jpa.repository.JpaRepository;//对User的DB操作，直接继承JpaRepositorypublic interface UserRepository extends JpaRepository&lt;User,Long&gt; &#123;&#125; 第五步：写测试用例进行测试。 123456789101112131415161718192021222324252627package com.example.data.es.demo;import com.example.data.es.demo.jpa.User;import com.example.data.es.demo.jpa.UserRepository;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;import java.util.List;//利用@DataJpaTest完成集成测试@DataJpaTestpublic class UserRepositoryTest &#123; @Autowired private UserRepository userRepository; @Test public void testJpa() &#123; //往数据库里面保存一条数据，并且打印一下 userRepository.save(User.builder().id(1L) .name("jkdb") .email("jack@email.com") .build()); List&lt;User&gt; users = userRepository.findAll(); users.forEach(user -&gt; &#123; System.out.println(user); &#125;); &#125;&#125; 这个时候，测试用例就变成了如下图所示的结构。 JPA 和 Elasticsearch 同时存在，和启动项目是一样的效果，这里就不写 Controller 了。 再整体运行一下这三个测试用例，进行完整的测试，就可以看到如下结果。 ElasticSearchRepositoryTest 执行的时候，通过日志可以看到这是对 ES 进行的操作，如下图所示。 UserRepositoryTest 执行的时候，通过日志可以看出来这是对 DB 进行的操作，所以谁也不影响谁，如下图所示。 Spring Data 对 JPA 等 SQL 型的传统数据库的支持是非常好的，同时对 NoSQL 型的非关系类数据库的支持也比较友好，大大降低了操作不同数据源的难度，可以有效提升开发效率。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data ElasticSearch</category>
      </categories>
      <tags>
        <tag>Spring Data ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 的单元测试和集成测试]]></title>
    <url>%2F2021%2F01%2F31%2FSpringDataJpa%E7%9A%84%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%E5%92%8C%E9%9B%86%E6%88%90%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[如何利用单元测试和集成测试让你开发效率翻倍？Spring boot 2.1，里面默认集成了 junit5，先从数据库层开始 Spring Data JPA 单元测试的最佳实践Spring Data JPA Repository 的测试用例第一步：引入 test 的依赖，gradle 的语法如下所示。 12testImplementation 'com.h2database:h2'testImplementation 'org.springframework.boot:spring-boot-starter-test' 第二步：利用项目里面的实体和 Repository，假设项目里面有 Address 和 AddressRepository，代码如下所示。 1234567891011121314@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructorpublic class Address extends BaseEntity &#123; private String city; private String address;&#125;//Repository的DAO层public interface AddressRepository extends JpaRepository&lt;Address, Long&gt;&#123; &#125; 第三步：新建 RepsitoryTest，@DataJpaTest 即可，代码如下所示。 12345678910111213@DataJpaTestpublic class AddressRepositoryTest &#123; @Autowired private AddressRepository addressRepository; //测试一下保存和查询 @Test public void testSave() &#123; Address address = Address.builder().city("shanghai").build(); addressRepository.save(address); List&lt;Address&gt; address1 = addressRepository.findAll(); address1.stream().forEach(address2 -&gt; System.out.println(address2)); &#125;&#125; 通过上面的测试用例可以看到，直接添加了 @DataJpaTest 注解，然后利用 Spring 的注解 @Autowired，引入了 spring context 里面管理的 AddressRepository 实例。换句话说，在这里面使用了集成测试，即直接连接的数据库来完成操作。 第四步：直接运行上面的测试用例，可以得到如下图所示的结果。 通过测试结果可以发现： 测试方法默认都会开启一个事务，测试完了之后就会进行回滚； 里面执行了 insert 和 select 两种操作； 如果开启了 Session Metrics 的日志的话，也可以观察出来其发生了一次 connection。 通过这个案例，可以知道 Repository 的测试用例写起来还是比较简单的，其中主要利用了 @DataJpaTest 的注解。下面打开 @DataJpaTest 的源码，看一下。 12345678910111213141516171819@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Inherited@BootstrapWith(DataJpaTestContextBootstrapper.class) //测试环境的启动方式@ExtendWith(SpringExtension.class)//加载了Spring测试环境@OverrideAutoConfiguration(enabled = false)@TypeExcludeFilters(DataJpaTypeExcludeFilter.class)@Transactional@AutoConfigureCache@AutoConfigureDataJpa//加载了依赖Spring Data JPA的原有配置@AutoConfigureTestDatabase //加载默认的测试数据库，这里面采用默认的H2@AutoConfigureTestEntityManager//加载测试所需要的EntityManager，主要是事务处理机制不一样@ImportAutoConfigurationpublic @interface DataJpaTest &#123; //默认打开sql的控制台输出 @PropertyMapping("spring.jpa.show-sql") boolean showSql() default true;......&#125; Repository 的测试场景可能在工作中，有的同事会说没有必要写 Repository 的测试用例，因为好多方法都是框架里面提供的，况且这个东西没有什么逻辑，写的时候有点浪费时间。 其实不然，如果能把 Repository 的测试用写好的话，这对我们的开发效率绝对是有提高的。否则当给你一个项目，让你直接改里面的代码，你可能就会比较慌，不敢改。所有你就要知道都有哪些场景是必须要写 Repository 的测试用例。 场景一：当新增一个 Entity 和实体对应的 Repository 的时候，需要写个简单的 save 和查询测试用例，主要目的是检查实体配置是否正确，否则当你写了一大堆 Repository 和 Entity 的时候，启动报错，不知道哪里配置得有问题，这样反而会降低开发效率； 场景二：当实体里面有一些 POJO 的逻辑，或者某些字段必须要有的时候，就需要写一些测试用例，假设 Address 实体里面不需要有 address 属性字段，并且有一个 @Transient 的字段和计算逻辑，这时就需要写一些测试用例去验证一下了。。 12345678910public class Address extends BaseEntity &#123; @JsonProperty("myCity") private String city; private String address; //必要字段 @Transient //非数据库字段，有一些简单运算 private String addressAndCity; public String getAddressAndCity() &#123; return address+"一些简单逻辑"+city; &#125;&#125; 场景三：当有自定义的方法的时候，就可能需要测试一下，看看返回结果是否满足需求，代码如下所示。 123public interface AddressRepository extends JpaRepository&lt;Address, Long&gt;&#123; Page&lt;Address&gt; findByAddress(@Param("address") String address, Pageable pageable);&#125; 场景四：当利用 @Query 注解，写了一些 JPQL 或者 SQL 的时候，就需要写一次测试用例来验证一下，代码如下所示。 12345public interface AddressRepository extends JpaRepository&lt;Address, Long&gt;&#123; //通过@Query注解自定的JPQL或Navicat SQL @Query(value = "FROM Address where deleted=false ") Page&lt;Address&gt; findAll(Pageable pageable);&#125; 那么对应的复杂一点的测试用例就要变成如下面这段代码所示的样子。 1234567891011121314151617181920@TestInstance(TestInstance.Lifecycle.PER_CLASS)@DataJpaTestpublic class AddressRepositoryTest &#123; @Autowired private AddressRepository addressRepository; @BeforeAll //利用 @BeforeAll准备一些Repositroy需要的测数据 @Rollback(false)// 由于每个方法都是有事务回滚机制的，为了测试 Repository 可能需要模拟一些数据，所以改变回滚机制 @Transactional public void init() &#123; Address address = Address.builder().city("shanghaiDeleted").deleted(true).build(); addressRepository.save(address); &#125; //测试没有包含删除的记录 @Test public void testFindAllNoDeleted() &#123; List&lt;Address&gt; address1 = addressRepository.findAll(); int deleteSize = address1.stream().filter(d-&gt;d.equals("shanghaiDeleted")).collect(Collectors.toList()).size(); Assertions.assertTrue(deleteSize==0); //测试一下不包含删除的条数 &#125;&#125; 场景五：当测试一些 JPA 或者 Hibernate 的底层特性的时候，测试用例可以很好地帮助我们。因为如果依赖项目启动来做测试，效率太低了，例如之前讲的一些 @PersistenceContext 特性，那么就可以通过类似如下的测试用例完成测试。 123456789101112131415161718192021222324252627282930313233343536373839404142@TestInstance(TestInstance.Lifecycle.PER_CLASS)@DataJpaTest@Import(TestConfiguration.class)public class UserInfoRepositoryTest &#123; @Autowired private UserInfoRepository userInfoRepository; //测试一些手动flush的机制 @PersistenceContext (properties = &#123;@PersistenceProperty( name = "org.hibernate.flushMode", value = "MANUAL"//手动flush )&#125;) private EntityManager entityManager; @BeforeAll @Rollback(false) @Transactional public void init() &#123; //提前准备一些数据方便测试 UserInfo u1 = UserInfo.builder().id(1L).lastName("jack").version(1).build(); userInfoRepository.save(u1); &#125; @Test @Transactional public void testLife() &#123; UserInfo userInfo = UserInfo.builder().name("new name").build(); //新增一个对象userInfo交给PersistenceContext管理，即一级缓存 entityManager.persist(userInfo); //此时没有detach和clear之前，flush的时候还会产生更新SQL userInfo.setName("old name"); entityManager.flush(); entityManager.clear();// entityManager.detach(userInfo);// entityManager已经clear，此时已经不会对UserInfo进行更新了 userInfo.setName("new name 11"); entityManager.flush(); //由于有cache机制，相同的对象查询只会触发一次查询SQL UserInfo u1 = userInfoRepository.findById(1L).get(); //to do some thing UserInfo u2 = userInfoRepository.findById(1L).get(); &#125;&#125; 集成测试和单元测试的区别什么是单元测试通俗来讲，就是不依赖本类之外的任何方法完成本类里面的所有方法的测试，也就是常说的依赖本类之外的，都通过 Mock 的方式进行。。 Service 层单元测试 首先，模拟一个业务中的 Service 方法，代码如下所示。 12345678910111213@Componentpublic class UserInfoServiceImpl implements UserInfoService &#123; @Autowired private UserInfoRepository userInfoRepository; //假设有个 findByUserId 的方法经过一些业务逻辑计算返回了一个业务对象 UserInfoDto @Override public UserInfoDto findByUserId(Long userId) &#123; UserInfo userInfo = userInfoRepository.findById(userId).orElse(new UserInfo()); //模拟一些业务计算改变一下name的值返回 UserInfoDto userInfoDto = UserInfoDto.builder().name(userInfo.getName()+"_HELLO").id(userInfo.getId()).build(); return userInfoDto; &#125;&#125; 其次，service 通过 Spring 的 @Component 注解进行加载，UserInfoRepository 通过 spring 的 @Autowired 注入进来，来测试一下 findByUserId 这个业务 service 方法，单元测试写法如下。 123456789101112131415161718@ExtendWith(SpringExtension.class)//通过这个注解利用Spring的容器@Import(UserInfoServiceImpl.class)//导入要测试的UserInfoServiceImplpublic class UserInfoServiceTest &#123; @Autowired //利用spring的容器，导入要测试的UserInfoService private UserInfoService userInfoService; @MockBean //里面@MockBean模拟service中用到的userInfoRepository，这样避免真实请求数据库 private UserInfoRepository userInfoRepository; // 利用单元测试的思想，mock userInfoService 里面的 UserInfoRepository，这样Service层就不用连接数据库，就可以测试自己的业务逻辑了 @Test public void testGetUserInfoDto() &#123; //利用 Mockito 模拟当调用 findById(1) 的时候，返回模拟数据 Mockito.when(userInfoRepository.findById(1L)) .thenReturn(java.util.Optional.ofNullable(UserInfo.builder().name("jack").id(1L).build())); UserInfoDto userInfoDto = userInfoService.findByUserId(1L); //经过一些service里面的逻辑计算，验证一下返回结果是否正确 Assertions.assertEquals("jack",userInfoDto.getName()); &#125;&#125; 这样就可以完成 Service 层的测试了。 其中 @ExtendWith(SpringExtension.class) 是 spring boot 与 Junit 5 结合使用的时候，当利用 Spring 的 TestContext 进行 mock 测试时要使用的。有的时候如果做一些简单 Util 的测试，就不一定会用到 SpringExtension.class。 在 service 的单元测试中，主要用到的知识点有四个。 通过 @ExtendWith(SpringExtension.class) 加载 Spring 的测试框架及其 TestContext； 通过 @Import(UserInfoServiceImpl.class) 导入具体要测试的类，这样 SpringTestContext 就不用加载项目里面的所有类，只需要加载 UserInfoServiceImpl.class 就可以了，这样可以大大提高测试用例的执行速度； 通过 @MockBean 模拟 UserInfoServiceImpl 依赖的 userInfoRepository，并且自动注入 Spring test context 里面，这样 Service 里面就自动有依赖了； 利用 Mockito.when().thenReturn() 的机制，模拟测试方法。 这样就可以通过 Assertions 里面的断言来测试 service 方法里面的逻辑是否符合预期了。 Controller 层单元测试 新增一个 UserInfoController 跟进 Id 获得 UserInfoDto 的信息，代码如下所示。 12345678910@RestControllerpublic class UserInfoController &#123; @Autowired private UserInfoService userInfoService; //跟进UserId取用户的详细信息 @GetMapping("/user/&#123;userId&#125;") public UserInfoDto findByUserId(@PathVariable Long userId) &#123; return userInfoService.findByUserId(userId); &#125;&#125; Controller 里面完整的测试用例，代码如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142package com.example.jpa.demo;import com.example.jpa.demo.service.UserInfoService;import com.example.jpa.demo.service.dto.UserInfoDto;import com.example.jpa.demo.web.UserInfoController;import org.junit.jupiter.api.Test;import org.mockito.Mockito;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.web.servlet.WebMvcTest;import org.springframework.boot.test.mock.mockito.MockBean;import org.springframework.http.MediaType;import org.springframework.mock.web.MockHttpServletResponse;import org.springframework.test.web.servlet.MockMvc;import org.springframework.test.web.servlet.request.MockMvcRequestBuilders;import org.springframework.test.web.servlet.result.MockMvcResultMatchers;import static org.springframework.test.web.servlet.result.MockMvcResultHandlers.print;import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status;@WebMvcTest(UserInfoController.class)public class UserInfoControllerTest &#123; @Autowired private MockMvc mvc; @MockBean private UserInfoService userInfoService; //单元测试mvc的controller的方法 @Test public void testGetUserDto() throws Exception &#123; //利用 @MockBean，当调用 userInfoService的findByUserId(1)的时候返回一个模拟的UserInfoDto数据 Mockito.when(userInfoService.findByUserId(1L)) .thenReturn(UserInfoDto.builder().name("jack").id(1L).build()); //利用mvc验证一下Controller里面的解决是否OK MockHttpServletResponse response = mvc .perform(MockMvcRequestBuilders .get("/user/1/")//请求的path .accept(MediaType.APPLICATION_JSON)//请求的mediaType，这里面可以加上各种需要的Header ) .andDo(print())//打印一下 .andExpect(status().isOk()) .andExpect(MockMvcResultMatchers.jsonPath("$.name").value("jack")) .andReturn().getResponse(); System.out.println(response); &#125;&#125; 其中主要利用了 @WebMvcTest 注解，来引入要测试的 Controller。打开 @WebMvcTest 可以看到关键源码，如下图所示。 @WebMvcTest 加载了 @ExtendWith(SpringExtension.class)，所以不需要额外指定，就拥有了 Spring 的 test context，并且也自动加载了 mvc 所需要的上下文 WebMvctestContextbootstrapper。 有的时候可能有一些全局的 Filter，也可以通过此注解里面的 includeFilters 和 excluedeFilters 加载和排除需要的 WebMvcFilter 进行测试。 当通过 @WebMvcTest(UserInfoController.class) 导入需要测试的 Controller 之后，就可以再通过 MockMvc 请求到加载的 Controller 里面的 path 了，并且可以通过 MockMvc 提供的一些方法发送请求，验证 Controller 的响应结果。 下面概括一下 Controller 层单元测试主要用到的三个知识点。 利用 @WebMvcTest 注解，加载要测试的 Controller，同时生成 mvc 所需要的 Test Context； 利用 @MockBean 默认 Controller 里面的依赖，如 Service，并通过 Mockito.when().thenReturn()；的语法 mock 依赖的测试数据； 利用 MockMvc 中提供的方法，发送 Controller 的 Rest 风格的请求，并验证返回结果和状态码。 什么是集成测试顾名思义，就是指多个模块放在一起测试，和单元测试正好相反，并非采用 mock 的方式测试，而是通过直接调用的方式进行测试。也就是说依赖 spring 容器进行开发，所有的类之间直接调用，模拟应用真实启动时候的状态。 Service 层的基层测试用例写法 还用刚才的例子，看一下 UserInfoService 里面的 findByUserId 通过集成测试如何进行。测试用例的写法如下。 1234567891011121314151617@DataJpaTest@ComponentScan(basePackageClasses= UserInfoServiceImpl.class)public class UserInfoServiceIntegrationTest &#123; @Autowired private UserInfoService userInfoService; @Autowired private UserInfoRepository userInfoRepository; @Test @Rollback(false)//如果事务回滚设置成false的话，数据库可以真实看到这条数据 public void testIntegtation() &#123; UserInfo u1 = UserInfo.builder().name("jack-db").ages(20).id(1L).telephone("1233456").build(); //数据库真实加一条数据 userInfoRepository.save(u1);//数据库里面真实保存一条数据 UserInfoDto userInfoDto = userInfoService.findByUserId(1L); userInfoDto.getName(); Assertions.assertEquals(userInfoDto.getName(),u1.getName()+"_HELLO"); &#125;&#125; 执行一下测试用例，结果如下图所示。 这时会发现数据已经不再回滚，也会正常地执行 SQL，而不是通过 Mock 的方式测试。 Controller 层的集成测试用例的写法 用集成测试把刚才 UserInfoCotroller 写的 user/1/ 接口测试一下，将集成测试的代码做如下改动。 1234567891011121314@SpringBootTest(classes = DemoApplication.class, webEnvironment = SpringBootTest.WebEnvironment.RANDOM_PORT) //加载 DemoApplication，指定一个随机端口public class UserInfoControllerIntegrationTest &#123; @LocalServerPort //获得模拟的随机端口 private int port; @Autowired //利用 RestTemplate，发送一个请求 private TestRestTemplate restTemplate; @Test public void testAllUserDtoIntegration() &#123; UserInfoDto userInfoDto = this.restTemplate .getForObject("http://localhost:" + port + "/user/1", UserInfoDto.class);//真实请求有一个后台的API Assertions.assertNotNull(userInfoDto); &#125;&#125; 再看日志的话，会发现此次的测试用例会在内部启动一个 tomcat 容器，然后再利用 TestResTemplate 进行真实请求，返回测试结果进行测试。 而其中会涉及一个注解 @SpringBootTest，它用来指定 Spring 应用的类是哪个，也就是真实项目的 Application 启动类；然后会指定一个端口，此处必须使用随机端口，否则可能会有冲突（如果启动的集成测试有点多的情况）。日志如下图所示。 如果看 @SprintBootTest 源码的话，会发现这个注解也是加载了 Spring 的测试环境 SpringExtension.class，并且里面有很多属性可以设置，测试的时候的配置文件 properties 和一些启动的环境变量 WebEnv；然后又利用了 Spring Boot Test 提供的 @LocalServerPort 获得启动时候的端口。源码如下图所示。 集成测试的一些思考 所有的方法都需要集成测试吗？ 这是写集成测试用的时候需要思考的，因为集成测试用例需要内部启动 Tomcat 容器，所以可能会启动得慢一点。如果项目加载的配置文件越来越多，势必会导致测试也会变慢。假设测试一个简单的逻辑就需要启动整个 Application，那么显然是不妥的。 那么整个 Application 不需要集成测试吗？也显然不是的，因为有些时候只有集成在一起才会发生问题，最简单的一个集成测试是需要测试是否能正常启动，所以一个项目里面会有个 ApplicationTests 来测试项目是否能正常启动。代码如下所示。 1234567@SpringBootTestclass DemoApplicationTests &#123; // 测试项目是否能正常启动 @Test void contextLoads() &#123; &#125;&#125; 一定是非集成测试就是单元测试吗？ 实际工作中并没有划分那么清楚，有的时候集成了 N 个组件一起测试，可能就是不连数据库。比如可能会使用 Feign-Client 根据第三方的接口获取一些数据，那么正常的做法就是新建一个 Service，代码如下所示。 12345678/** * 测试普通JSON返回结果，根据第三方接口取一个数据 */@FeignClient(name = "aocFeignTest", url = "http://room-api.staging.jack.net")interface AppSettingService &#123; @GetMapping("/api/v1/app/globalSettings") HashMap&lt;String,Object&gt; getAppSettings();&#125; 那么这个时候如果要测试，显然不需要启动整个 Application 来完成，但是需要按需加载一些 Configuration 才能测试，那么测试用例会变成如下情况。 123456789101112131415@ExtendWith(SpringExtension.class)//利用Spring上下文@Import(&#123;FeignSimpleConfiguration.class, FeignAutoConfiguration.class, HttpMessageConvertersAutoConfiguration.class, JacksonAutoConfiguration.class&#125;)//导入此处Fegin-Client测试所需要的配置文件@EnableFeignClients(clients = AppSettingService.class)//通过FeignClient的注解加载AppSettingService客户端。/** * 依赖HTTPMessageConverter的使用方法(import FeignSimpleConfiguration junit) */public class FeignJsonTest &#123; @Autowired // 利用Spring的上下文注入appSettingService private AppSettingService appSettingService; @Test public void testJsonFeignClient() &#123; HashMap&lt;String,Object&gt; r = .getAppSettings(); Assert.assertNotNull(r.get("data"));//测试一下接口返回的结果 &#125;&#125; 这个时候其实并没有启动这个 Application，但是我们也集成了 Fegin-Client 所需要的上下文 Configuration，从而利用 SpringExtension 加载所需要依赖的类，完成一次测试。 所以一定要根据自己的实际需要选择性地加载一些类来完成测试用例，而不是每次测试的时候都需要把所有类都加载一遍，这样返回会使测试用例的时间变长，从而降低工作效率。 Junit 4 和 Junit 5 在 Spring Boot 中的区别第一，Spring Boot 2.2+ 以上的版本默认导入的是 Junit 5 的 jar 包依赖，以下的版本默认导入的是 Junit 4 的 jar 包依赖的版本，所以在使用不同版本的 Spring Boot 的时候需要注意一下依赖的 jar 包是否齐全。 第二，org.junit.junit.Test 变成了 org.junit.jupiter.api.Test。 第三，一些注解发生了变化： @Before 变成了 @BeforeEach @After 变成了 @AfterEach @BeforeClass 变成了 @BeforeAll @AfterClass 变成了 @AfterAll @Ignore 变成了 @Disabled @Category 变成了 @Tag @Rule 和 @ClassRule 没有了，用 @ExtendWith 和 @RegisterExtension 代替 第四，引用 Spring 的上下文 @RunWith(SpringRunner.class) 变成了 @ExtendWith(SpringExtension.class)。 第五，org.junit.Assert 下面的断言都移到了org.junit.jupiter.api.Assertions 下面，所以一些断言的写法会发生如下变化： 123456//junit4断言的写法Assert.assertEquals(200, result.getStatusCodeValue());Assert.assertEquals(true, result.getBody().contains("employeeList"));//junit5断言的写法Assertions.assertEquals(400, ex.getRawStatusCode());Assertions.assertEquals(true, ex.getResponseBodyAsString().contains("Missing request header")); 第六，Junit 5 提供 @DisplayName(&quot;Test MyClass&quot;) 用来标识此次单元测试的名字，代码如下所示。 12345678@DisplayName("Test MyClass")class MyClassTest &#123; @Test @DisplayName("Verify MyClass.myMethod returns true") void testMyMethod() throws Exception &#123; // ... &#125;&#125;]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 什么是 Spring Data Rest]]></title>
    <url>%2F2021%2F01%2F30%2FSpringDataJpa%E4%B9%8BSpringDataRest%2F</url>
    <content type="text"><![CDATA[Spring Data Rest 是什么 和 JPA 是什么关系?Spring Data Rest Demo通过以下四个步骤演示一下 Spring Data Rest 的效果。 第一步：通过 gradle 引入相关的 jar 依赖，代码如下所示。 1234567implementation 'org.springframework.boot:spring-boot-starter-data-jpa'// spring data rest的依赖，由于使用的是spring boot，所以只需要添加starter即可implementation("org.springframework.boot:spring-boot-starter-data-rest")//添加swagger方便看得出来，生成了哪些api接口implementation 'io.springfox:springfox-boot-starter:3.0.0'// swagger 对spring data rest支持需要添加 springfox-data-restimplementation 'io.springfox:springfox-data-rest:3.0.0' 添加完依赖之后，可以通过 gradle 的依赖视图看一下都用了哪些 jar 包。 通过上图可以很清晰地看到 spring-data-rest 的 jar 包引入情况，以及依赖的 spring-data-jpa 和 Swagger。 第二步：在项目里面添加 SpringFoxConfiguration 开启 Swagger，代码如下所示 123@Configuration@EnableSwagger2public class SpringFoxConfiguration &#123;&#125; 第三步：通过 application.properties 指定一个 base-path，以方便和自己的 api 进行区分，代码如下所示。 12# 可以通过spring data rest里面提供的配置项，指定bast-pathspring.data.rest.base-path=api/rest/v2 这时打开 Swagger 看一下：http://127.0.0.1:8087/swagger-ui/ 由于 Demo 的项目结构是下图所示这样的。 你会发现有几个 Repository 会生成几个对应的 Rest 协议的 API，除了基本的 CRUD，例如 UserInfoRespository 自定义的方法它们也会帮我们展示出来。而 Room 实体因为没有对应的 Repository，所以不会有对应的 Rest 风格 API 生成。 通过这个 Demo 可以想象一下，如果要做一个 Rest 风格的 Server API 项目，只需要把对应的 Entity 和 Repository 建好，就可以直接拥有了所有的 CRUD 的 API 了，这样可以大大提高我们的开发效率。 Spring Data Rest 基本用法通过 Demo 可以看得出来，Spring Data Rest 的核心功能就是把 Spring Data Resositories 里对外暴露的方法生成对应的 API，如上面的 AddressRepository，里面对应的实体是 Address，代码如下。 1public interface AddressRepository extends JpaRepository&lt;Address, Long&gt;&#123;&#125; 它帮我们生成的 API 有下图所示的这些。 从 swagger 可以看到 Spring Data Rest 的几点用法。 语义化的方法把实体转化成复数的形式，生成基本的 PATCH、GET、PUT、POST、DELETE 带有语义的 Rest 相应的方法，包括的子资源有如下几个。 GET：返回单个实体 PUT：更新资源 PATCH：与 PUT 类似，但部分是更新资源状态 DELETE：删除暴露的资源 POST：从给定的请求正文创建一个新的实体 默认的状态码的支持 200 OK：适用于纯粹的 GET 请求 201 Created：针对创建新资源的 POST 和 PUT 请求 204 No Content：对于 PUT、PATCH 和 DELETE 请求 401 没有认证 403 没有权限，拒绝访问 404 没有找到对应的资源 分页支持通过 Swagger，可以看到其完全对分页和排序进行支持，完全兼容 Spring Data JPA 的分页和排序的参数，如下图所示。 通过 @RepositoryRestResource 改变资源的 metaData代码如下所示。 12345678@RepositoryRestResource( exported = true, //资源是否暴露，默认true path = "users",//资源暴露的path访问路径，默认实体名字+s collectionResourceRel = "userInfo",//资源名字，默认实体名字 collectionResourceDescription = @Description("用户基本信息资源"),//资源描述 itemResourceRel = "userDetail",//取资源详情的Item名字 itemResourceDescription = @Description("用户详情")) 将其放置在 UserInfoRepository 上面测试一下，代码变更如下。 123456789@RepositoryRestResource( exported = true, path = "users", collectionResourceRel = "userInfo", collectionResourceDescription = @Description("用户资源"), itemResourceRel = "userDetail", itemResourceDescription = @Description("用户详情"))public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt; &#123;&#125; 这时通过 Swagger 可以看到，url 的 path 上面变成了 users，而 body 里面的资源名字变成了 userInfo，取 itemResource 的 URL 描述变成了 userDetail，如下图所示。 @RepositoryRestResource 是使用在 Repository 类上面的全局设置，也可以针对具体的 Repsitory 里面的每个方法进行单独设置，这就是另外一个注解：@RestResource。 12345@RestResource( exported = true,//是否暴露给Search path = "findCities",//Search后面的path路径 rel = "cities"//资源名字) 可以将其用于 ***Repository 的方法中和 @Entity 的实体关系上，那么在 address 的 findByAddress 方法上面做一个测试，看看会变成什么样，代码如下所示。 12345678public interface AddressRepository extends JpaRepository&lt;Address, Long&gt;&#123; @RestResource( exported = true,//是否暴露给Search path = "findCities",//Search后面的path路径 rel = "cities"//资源名字 ) Page&lt;Address&gt; findByAddress(@Param("address") String address, Pageable pageable);&#125; 打开 Swagger 看一下结果，会发现 search 后面的 path 路径被自定义了，如下图所示。 同时这个注解也可以配置在关联关系上，如 @OneToMany 等。如果不想某些方法暴露成 RestAPI，就直接添加 @RestResource(exported = false) 这一注解即可，例如一些删除方法等。 spring data rest 的配置项支持这个可以直接在 application.properties 里面配置，在 IDEA 里面输入前缀的时候，就会有如下提示。 对应的描述如下表所示。 返回结果对 Jackson 的支持通过 jackson 的注解，可以改变 rest api 的属性的名字，或者忽略具体的某个属性。在 address 的实体里面，改变一下属性 city 的名字，同时忽略 address 属性，代码会变成如下所示的样子。 12345678910111213@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "userInfo")public class Address extends BaseEntity &#123; @JsonProperty("myCity") //改变JSON响应的属性名字 private String city; @JsonIgnore //JSON解析的时候忽略某个属性 private String address;&#125; 通过 Swagger 里面的 Description 可以看到，当前的资源的描述发生了变化，字段名变成了 myCity，address 属性没有了，具体如下图所示。 Spring Data Rest 返回 ResponseBody 的原理和接收 RequestBody 的原理都是基于 JSON 格式的 Spring Data Rest 和 Spring Data JPA 的关系 Spring Data JPA 基于 JPA 协议提供了一套标准的 Repository 的操作统一接口，方法名和 @Query 都是有固定语法和约定的规则的。 Spring Data Rest 利用 JPA 的约定和语法，利用 Java 反射、动态代理等机制，很容易可以生成一套标准的 rest 风格的 API 协议操作。 也就是说 JPA 制定协议和标准，Spring Data Rest 基于这套协议生成 rest 风格的 Controller。 🎯JPA 的应用领域其实有很多，在写一些基于实体的框架时就可以参考 Spring Data Rest 的做法。例如 yahoo 团队设计的 JSONAPI 协议，以及 Elide 的实现，也是基于 JPA 的实体注解来实现的。 甚至 Spring 在研究的 graph QL，也可以基于约定的实体来做很多事情。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 二级缓存的思考]]></title>
    <url>%2F2021%2F01%2F30%2FSpringDataJpa%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98%E7%9A%84%E6%80%9D%E8%80%83-%E7%BB%93%E5%90%88Redis%2F</url>
    <content type="text"><![CDATA[二级缓存的思考：JPA结合Redis二级缓存的概念一级缓存的实体的生命周期和 PersistenceContext 是相同的，即载体为同一个 Session 才有效；而 Hibernate 提出了二级缓存的概念，也就是可以在不同的 Session 之间共享实体实例，说白了就是在单个应用内的整个 application 生命周期之内共享实体，减少数据库查询。 由于 JPA 协议本身并没有规定二级缓存的概念，所以这是 Hibernate 独有的特性。在 Hibernate 中，从数据库里面查询实体的过程就变成了：第一步先看看一级缓存里面有没有实体，如果没有再看看二级缓存里面有没有，如果还是没有再从数据库里面查询。 Hibernate 中二级缓存的配置方法Hibernate 中，默认情况下二级缓存是关闭的，如果想开启二级缓存需要通过如下三个步骤。 第一步：引入第三方二级缓存的实现的 jar。 因为 Hibernate 本身并没有实现缓存的功能，而是主要依赖第三方，如 Ehcache、jcache、redis 等第三方库。下面以 EhCache 为例，利用 gradle 引入 hibernate-ehcache 的依赖。代码如下所示。 1implementation 'org.hibernate:hibernate-ehcache:5.2.2.Final' 如果想用 jcache，可以通过如下方式。 1compile 'org.hibernate:hibernate-jcache:5.2.2.Final' 第二步：在配置文件里面开启二级缓存。 二级缓存默认是关闭的，所以需要用如下方式开启二级缓存，并且配置 cache.region.factory_class 为不同的缓存实现类。 12hibernate.cache.use_second_level_cache=truehibernate.cache.region.factory_class=org.hibernate.cache.ehcache.EhCacheRegionFactory 第三步：在用到二级缓存的地方配置 @Cacheable 和 @Cache 的策略。 123456import javax.persistence.Cacheable;import javax.persistence.Entity;@Entity@Cacheable@org.hibernate.annotations.Cache(usage = CacheConcurrencyStrategy.READ_WRITE)public class UserInfo extends BaseEntity &#123;......&#125; 二级缓存的思考二级缓存主要解决的是单应用场景下跨 Session 生命周期的实体共享问题，可是一定要通过 Hibernate 来做吗？答案并不是，其实可以通过各种 Cache 的手段来做，因为 Hibernate 里面一级缓存的复杂度相对较高，并且使用的话实体的生命周期会有变化，查询问题的过程较为麻烦。 同时，随着现在逐渐微服务化、分布式化，如今的应用都不是单机应用，那么缓存之间如何共享呢？分布式缓存又该如何解决？比如一个机器变了，另一个机器没变，应该如何处理？似乎 Hibernate 并没有考虑到这些问题。 此外，还有什么时间数据会变更、变化了之后如何清除缓存，等等，这些都是要思考的，所以 Hibernate 的二级缓存听起来“高大上”，但是使用起来绝对没有那么简单。 那么经过这一连串的疑问，如果不用 Hibernate 的二级缓存，还有没有更好的解决方案呢？ 利用 Redis 进行缓存在实际工作中经常需要 cache 的就是 Redis，那么通过一个例子，来看下 Spring Cache 结合 Redis 是怎么使用的。 Spring Cache 和 Redis 结合第一步：在 gradle 中引入 cache 和 redis 的依赖，代码如下所示。 12345//原来只用到了JPAimplementation 'org.springframework.boot:spring-boot-starter-data-jpa'//为了引入cache和redis机制需要引入如下两个jar包implementation 'org.springframework.boot:spring-boot-starter-data-redis' //redis的依赖implementation 'org.springframework.boot:spring-boot-starter-cache' //cache 的依赖 第二步：在 application.properties 里面增加 redis 的相关配置，代码如下。 12345678spring.redis.host=127.0.0.1spring.redis.port=6379spring.redis.password=sySj6vmYkespring.redis.timeout=6000spring.redis.pool.max-active=8spring.redis.pool.max-idle=8spring.redis.pool.max-wait=-1spring.redis.pool.min-idle=0 第三步：通过 @EnableCaching 开启缓存，增加 configuration 配置类，代码如下所示。 123@EnableCaching@Configurationpublic class CacheConfiguration &#123;&#125; 第四步：在需要缓存的地方添加 @Cacheable 注解即可。为了方便演示，把 @Cacheable 注解配置在了 controller 方法上，代码如下。 123456@GetMapping("/user/info/&#123;id&#125;")@Cacheable(value = "userInfo", key = "&#123;#root.methodName, #id&#125;", unless = "#result == null") //利用默认key值生成规则value加key生成一个redis的key值，result==null的时候不进行缓存public UserInfo getUserInfo(@PathVariable("id") Long id) &#123; //第二次就不会再执行这里了 return userInfoRepository.findById(id).get();&#125; 第五步：启动项目，请求一下这个 API 会发现，第一次请求过后，redis 里面就有一条记录了，如下图所示。 可以看到，第二次请求之后，取数据就不会再请求数据库了。那么 redis 已经熟悉了，那么来看一下 Spring Cache 都做了哪些事情。 Spring Cache 介绍Spring 3.1 之后引入了基于注释（annotation）的缓存（cache）技术，它本质上不是一个具体的缓存实现方案（例如 EHCache 或者 Redis），而是一个对缓存使用的抽象概念，通过在既有代码中添加少量它定义的各种 annotation，就能够达到缓存方法的返回对象的效果。 Spring 的缓存技术还具备相当的灵活性，不仅能够使用 SpEL（Spring Expression Language）来定义缓存的 key 和各种 condition，还提供开箱即用的缓存临时存储方案，也支持主流的专业缓存，例如 Redis，EHCache 集成。而 Spring Cache 属于 Spring framework 的一部分，在下面图片所示的这个包里面。 Spring cache 里面的主要的注解 @Cacheable 应用到读取数据的方法上，就是可以缓存的方法，如查找方法：先从缓存中读取，如果没有再调用方法获取数据，然后把数据添加到缓存中。 123456789101112131415161718192021public @interface Cacheable &#123; @AliasFor("cacheNames") String[] value() default &#123;&#125;;//cache的名字。可以根据名字设置不同cache处理类。redis里面可以根据cache名字设置不同的失效时间。 @AliasFor("value") String[] cacheNames() default &#123;&#125;;//缓存的key的名字，支持 SPEL String key() default "";//key的生成策略，不指定可以用全局的默认的。 String keyGenerator() default ""; //客户选择不同的CacheManager String cacheManager() default ""; //配置不同的cache resolver String cacheResolver() default ""; //满足什么样的条件才能被缓存，支持SpEL，可以去掉方法名、参数 String condition() default "";//排除哪些返回结果不加入缓存里面去，支持SpEL，实际工作中常见的是result ==null等 String unless() default ""; //是否同步读取缓存、更新缓存 boolean sync() default false;&#125; 下面是@Cacheable 相关的例子。 12@Cacheable(cacheNames="book", condition="#name.length() &lt; 32", unless="#result.notNeedCache")//利用SPEL表达式只有当name参数长度小于32的时候再进行缓存，排除notNeedCache的对象public Book findBook(String name) @CachePut 调用方法时会自动把相应的数据放入缓存，它与 @Cacheable 不同的是所有注解的方法每次都会执行，一般配置在 Update 和 insert 方法上。其源码里面的字段和用法基本与 @Cacheable 相同，只是使用场景不一样。 @CacheEvict 删除缓存，一般配置在删除方法上面。代码如下所示。 1234567891011121314public @interface CacheEvict &#123;//与@Cacheable相同的部分就不重复叙述了。...... //是否删除所有的实体对象 boolean allEntries() default false; //是否方法执行之前执行。默认在方法调用成功之后删除 boolean beforeInvocation() default false;&#125; @Caching 所有Cache注解的组合配置方法，源码如下： public @interface Caching &#123; Cacheable[] cacheable() default &#123;&#125;; CachePut[] put() default &#123;&#125;; CacheEvict[] evict() default &#123;&#125;;&#125; 此外，还有 @CacheConfig 表示全局 Cache 配置；@EnableCaching，表示是否开启 SpringCache 的配置。 Spring Cache Redis 里面主要的类 org.springframework.boot.autoconfigure.cache.CacheAutoConfiguration cache 的自动装配类，此类被加载的方式是在 spring boot的spring.factories 文件里面，其关键源码如下所示。 123456789101112131415161718192021222324@Configuration(proxyBeanMethods = false)@ConditionalOnClass(CacheManager.class)@ConditionalOnBean(CacheAspectSupport.class)@ConditionalOnMissingBean(value = CacheManager.class, name = "cacheResolver")@EnableConfigurationProperties(CacheProperties.class)@AutoConfigureAfter(&#123; CouchbaseDataAutoConfiguration.class, HazelcastAutoConfiguration.class, HibernateJpaAutoConfiguration.class, RedisAutoConfiguration.class &#125;)@Import(&#123; CacheConfigurationImportSelector.class, CacheManagerEntityManagerFactoryDependsOnPostProcessor.class &#125;)public class CacheAutoConfiguration &#123; /** * &#123;@link ImportSelector&#125; to add &#123;@link CacheType&#125; configuration classes. */ static class CacheConfigurationImportSelector implements ImportSelector &#123; @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; CacheType[] types = CacheType.values(); String[] imports = new String[types.length]; for (int i = 0; i &lt; types.length; i++) &#123; imports[i] = CacheConfigurations.getConfigurationClass(types[i]); &#125; return imports; &#125; &#125;&#125; 通过源码可以看到，此类的关键作用是加载 Cache 的依赖配置，以及加载所有 CacheType 的配置文件，而 CacheConfigurations 里面定义了不同的 Cache 实现方式的配置，里面包含了 Ehcache、Redis、Jcache 的各种实现方式，如下图所示。 org.springframework.cache.annotation.CachingConfigurerSupport 通过此类可以自定义 Cache 里面的 CacheManager、CacheResolver、KeyGenerator、CacheErrorHandler，代码如下所示。 1234567891011121314151617181920212223242526public class CachingConfigurerSupport implements CachingConfigurer &#123; // cache的manager，主要是管理不同的cache的实现方式，如redis还是ehcache等 @Override @Nullable public CacheManager cacheManager() &#123; return null; &#125; // cache的不同实现者的操作方法，CacheResolver解析器，用于根据实际情况来动态解析使用哪个Cache @Override @Nullable public CacheResolver cacheResolver() &#123; return null; &#125; //cache的key的生成规则 @Override @Nullable public KeyGenerator keyGenerator() &#123; return null; &#125; //cache发生异常的回调处理，一般情况下我会打印个warn日志，方便知道发生了什么事情 @Override @Nullable public CacheErrorHandler errorHandler() &#123; return null; &#125;&#125; 其中，所有 CacheManager 是 Spring 提供的各种缓存技术抽象接口，通过它来管理，Spring framework 里面默认实现的 CacheManager 有不同的实现类，redis 默认加载的是 RedisCacheManager，如下图所示。 org.springframework.boot.autoconfigure.cache.RedisCacheConfiguration 它是加载 Cache 的实现者，也是 redis 的实现类，关键源码如下图所示。 可以看得出来，它依赖本身的 Redis 的连接，并且加载了 RedisCacheManager；同时可以看到关于 Cache 和 Redis 的配置有哪些。 通过 CacheProperties 里面 redis 的配置，可以设置“key 的统一前缀、默认过期时间、是否缓存 null 值、是否使用前缀”这四个配置。 Spring Cache 结合 Redis 使用的最佳实践不同 cache 的 name 在 redis 里面配置不同的过期时间 默认情况下所有 redis 的 cache 过期时间是一样的，实际工作中一般需要自定义不同 cache 的 name 的过期时间，这里 cache 的 name 就是指 @Cacheable 里面 value 属性对应的值。主要步骤如下。 第一步：自定义一个配置文件，用来指定不同的 cacheName 对应的过期时间不一样。代码如下所示。 123456789@Getter@Setter@ConfigurationProperties(prefix = "spring.cache.redis")/** * 改善一下cacheName的最佳实践方法，目前主要用不同的cache name不同的过期时间，可以扩展 */public class MyCacheProperties &#123; private HashMap&lt;String, Duration&gt; cacheNameConfig;&#125; 第二步：通过自定义类 MyRedisCacheManagerBuilderCustomizer 实现 RedisCacheManagerBuilderCustomizer 里面的 customize 方法，用来指定不同的 name 采用不同的 RedisCacheConfiguration，从而达到设置不同的过期时间的效果。代码如下所示。 1234567891011121314151617181920212223242526272829303132/** * 这个依赖spring boot 2.2 以上版本才有效 */public class MyRedisCacheManagerBuilderCustomizer implements RedisCacheManagerBuilderCustomizer &#123; private MyCacheProperties myCacheProperties; private RedisCacheConfiguration redisCacheConfiguration; public MyRedisCacheManagerBuilderCustomizer(MyCacheProperties myCacheProperties, RedisCacheConfiguration redisCacheConfiguration) &#123; this.myCacheProperties = myCacheProperties; this.redisCacheConfiguration = redisCacheConfiguration; &#125; /** * 利用默认配置的只需要在这里加就可以了 * spring.cache.cache-names=abc,def,userlist2,user3 * 下面是不同的cache-name可以配置不同的过期时间，yaml也支持，如果以后还有其他属性扩展可以改这里 * spring.cache.redis.cache-name-config.user2=2h * spring.cache.redis.cache-name-config.def=2m * @param builder */ @Override public void customize(RedisCacheManager.RedisCacheManagerBuilder builder) &#123; if (ObjectUtils.isEmpty(myCacheProperties.getCacheNameConfig())) &#123; return; &#125; Map&lt;String, RedisCacheConfiguration&gt; cacheConfigurations = myCacheProperties.getCacheNameConfig().entrySet().stream() .collect(Collectors .toMap(e-&gt;e.getKey(),v-&gt;builder .getCacheConfigurationFor(v.getKey()) .orElse(RedisCacheConfiguration.defaultCacheConfig().serializeValuesWith(redisCacheConfiguration.getValueSerializationPair())) .entryTtl(v.getValue()))); builder.withInitialCacheConfigurations(cacheConfigurations); &#125;&#125; 第三步：在 CacheConfiguration 里面把自定义的 CacheManagerCustomize 加载进去即可，代码如下。 12345678910111213141516171819@EnableCaching@Configuration@EnableConfigurationProperties(value = &#123;MyCacheProperties.class,CacheProperties.class&#125;)@AutoConfigureAfter(&#123;CacheAutoConfiguration.class&#125;)public class CacheConfiguration &#123; /** * 支持不同的cache name有不同的缓存时间的配置 * * @param myCacheProperties * @param redisCacheConfiguration * @return */ @Bean @ConditionalOnMissingBean(name = "myRedisCacheManagerBuilderCustomizer") @ConditionalOnClass(RedisCacheManagerBuilderCustomizer.class) public MyRedisCacheManagerBuilderCustomizer myRedisCacheManagerBuilderCustomizer(MyCacheProperties myCacheProperties, RedisCacheConfiguration redisCacheConfiguration) &#123; return new MyRedisCacheManagerBuilderCustomizer(myCacheProperties,redisCacheConfiguration); &#125;&#125; 第四步：使用的时候非常简单，只需要在 application.properties 里面做如下配置即可。 123456# 设置默认的过期时间是20分钟spring.cache.redis.time-to-live=20m# 设置刚才的例子 @Cacheable(value=&quot;userInfo&quot;)5分钟过期spring.cache.redis.cache-name-config.userInfo=5m# 设置 room的cache1小时过期spring.cache.redis.cache-name-config.room=1h 自定义 KeyGenerator 实现，redis 的 key 自定义拼接规则 假如不喜欢默认的 cache 生成的 key 的 string 规则，那么可以自定义。创建 MyRedisCachingConfigurerSupport 集成 CachingConfigurerSupport 即可，代码如下。 12345678910111213141516171819202122@Component@Log4j2public class MyRedisCachingConfigurerSupport extends CachingConfigurerSupport &#123; @Override public KeyGenerator keyGenerator() &#123; return getKeyGenerator(); &#125; /** * 覆盖默认的redis key的生成规则，变成"方法名:参数:参数" * @return */ public static KeyGenerator getKeyGenerator() &#123; return (target, method, params) -&gt; &#123; StringBuilder key = new StringBuilder(); key.append(ClassUtils.getQualifiedMethodName(method)); for (Object obc : params) &#123; key.append(":").append(obc); &#125; return key.toString(); &#125;; &#125;&#125; 当发生 cache 和 redis 的操作异常时，不希望阻碍主流程，打印一个关键日志即可 只需要在 MyRedisCachingConfigurerSupport 里面再实现父类的 errorHandler 即可，代码变成了如下模样。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Log4j2public class MyRedisCachingConfigurerSupport extends CachingConfigurerSupport &#123; @Override public KeyGenerator keyGenerator() &#123; return getKeyGenerator(); &#125; /** * 覆盖默认的redis key的生成规则，变成"方法名:参数:参数" * @return */ public static KeyGenerator getKeyGenerator() &#123; return (target, method, params) -&gt; &#123; StringBuilder key = new StringBuilder(); key.append(ClassUtils.getQualifiedMethodName(method)); for (Object obc : params) &#123; key.append(":").append(obc); &#125; return key.toString(); &#125;; &#125; /** * 覆盖默认异常处理方法，不抛异常，改打印error日志 * * @return */ @Override public CacheErrorHandler errorHandler() &#123; return new CacheErrorHandler() &#123; @Override public void handleCacheGetError(RuntimeException exception, Cache cache, Object key) &#123; log.error(String.format("Spring cache GET error:cache=%s,key=%s", cache, key), exception); &#125; @Override public void handleCachePutError(RuntimeException exception, Cache cache, Object key, Object value) &#123; log.error(String.format("Spring cache PUT error:cache=%s,key=%s", cache, key), exception); &#125; @Override public void handleCacheEvictError(RuntimeException exception, Cache cache, Object key) &#123; log.error(String.format("Spring cache EVICT error:cache=%s,key=%s", cache, key), exception); &#125; @Override public void handleCacheClearError(RuntimeException exception, Cache cache) &#123; log.error(String.format("Spring cache CLEAR error:cache=%s", cache), exception); &#125; &#125;; &#125;&#125; 改变默认的 cache 里面 redis 的 value 序列化方式 默认有可能是 JDK 序列化方式，所以一般看不懂 redis 里面的值，那么就可以把序列化方式改成 JSON 格式，只需要在 CacheConfiguration 里面增加默认的 RedisCacheConfiguration 配置即可，完整的 CacheConfiguration 变成如下代码所示的样子。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@EnableCaching@Configuration@EnableConfigurationProperties(value = &#123;MyCacheProperties.class,CacheProperties.class&#125;)@AutoConfigureAfter(&#123;CacheAutoConfiguration.class&#125;)public class CacheConfiguration &#123; /** * 支持不同的cache name有不同的缓存时间的配置 * * @param myCacheProperties * @param redisCacheConfiguration * @return */ @Bean @ConditionalOnMissingBean(name = "myRedisCacheManagerBuilderCustomizer") @ConditionalOnClass(RedisCacheManagerBuilderCustomizer.class) public MyRedisCacheManagerBuilderCustomizer myRedisCacheManagerBuilderCustomizer(MyCacheProperties myCacheProperties, RedisCacheConfiguration redisCacheConfiguration) &#123; return new MyRedisCacheManagerBuilderCustomizer(myCacheProperties,redisCacheConfiguration); &#125; /** * cache异常不抛异常，只打印error日志 * * @return */ @Bean @ConditionalOnMissingBean(name = "myRedisCachingConfigurerSupport") public MyRedisCachingConfigurerSupport myRedisCachingConfigurerSupport() &#123; return new MyRedisCachingConfigurerSupport(); &#125; /** * 依赖默认的ObjectMapper，实现普通的json序列化 * @param defaultObjectMapper * @return */ @Bean(name = "genericJackson2JsonRedisSerializer") @ConditionalOnMissingBean(name = "genericJackson2JsonRedisSerializer") public GenericJackson2JsonRedisSerializer genericJackson2JsonRedisSerializer(ObjectMapper defaultObjectMapper) &#123; ObjectMapper objectMapper = defaultObjectMapper.copy(); objectMapper.registerModule(new Hibernate5Module().enable(REPLACE_PERSISTENT_COLLECTIONS)); //支持JPA的实体的json的序列化 objectMapper.configure(MapperFeature.SORT_PROPERTIES_ALPHABETICALLY, true); objectMapper.deactivateDefaultTyping(); //关闭 defaultType，不需要关心redis里面是否为对象的类型 return new GenericJackson2JsonRedisSerializer(objectMapper); &#125; /** * 覆盖 RedisCacheConfiguration，只是修改serializeValues with jackson * * @param cacheProperties * @return */ @Bean @ConditionalOnMissingBean(name = "jacksonRedisCacheConfiguration") public RedisCacheConfiguration jacksonRedisCacheConfiguration(CacheProperties cacheProperties, GenericJackson2JsonRedisSerializer genericJackson2JsonRedisSerializer) &#123; CacheProperties.Redis redisProperties = cacheProperties.getRedis(); RedisCacheConfiguration config = RedisCacheConfiguration .defaultCacheConfig(); config = config.serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(genericJackson2JsonRedisSerializer));//修改的关键所在，指定Jackson2JsonRedisSerializer的方式 if (redisProperties.getTimeToLive() != null) &#123; config = config.entryTtl(redisProperties.getTimeToLive()); &#125; if (redisProperties.getKeyPrefix() != null) &#123; config = config.prefixCacheNameWith(redisProperties.getKeyPrefix()); &#125; if (!redisProperties.isCacheNullValues()) &#123; config = config.disableCachingNullValues(); &#125; if (!redisProperties.isUseKeyPrefix()) &#123; config = config.disableKeyPrefix(); &#125; return config; &#125;&#125;]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa Hibernate 一级缓存]]></title>
    <url>%2F2021%2F01%2F26%2FSpringDataJpa%E7%9A%84Hibernate%E7%9A%84%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[Hibernate 一级缓存一级缓存什么是一级缓存按照 Hibernate 和 JPA 协议里面的解释，经常说的 First Level Cache（一级缓存）也就是 PersistenceContext，既然如此，那么就意味着一级缓存的载体是 Session 或者 EntityManager；而一级缓存的实体也就是数据库里面对应的实体。 在 SessionImpl 的实现过程中，会发现 PersistenceContext 的实现类 StatefulPersistenceContext 是通过 HashMap 来存储实体信息的。 其关键源码如下所示： 1234567891011121314151617public class StatefulPersistenceContext implements PersistenceContext &#123; //根据EntityUniqueKey作为key来储存Entity private HashMap&lt;EntityUniqueKey, Object&gt; entitiesByUniqueKey; //根据EntityUniqueKey作为key取当前实体 @Override public Object getEntity(EntityUniqueKey euk) &#123; return entitiesByUniqueKey == null ? null : entitiesByUniqueKey.get( euk ); &#125; //储存实体，如果是第一次，那么创建HashMap&lt;&gt; @Override public void addEntity(EntityUniqueKey euk, Object entity) &#123; if ( entitiesByUniqueKey == null ) &#123; entitiesByUniqueKey = new HashMap&lt;&gt;( INIT_COLL_SIZE ); &#125; entitiesByUniqueKey.put( euk, entity ); &#125;......&#125; 其中 EntityUniqueKey 的核心源码如下所示： 12345678910111213141516public class EntityUniqueKey implements Serializable &#123; private final String uniqueKeyName; private final String entityName; private final Object key; private final Type keyType; private final EntityMode entityMode; private final int hashCode; @Override public boolean equals(Object other) &#123; EntityUniqueKey that = (EntityUniqueKey) other; return that != null &amp;&amp; that.entityName.equals( entityName ) &amp;&amp; that.uniqueKeyName.equals( uniqueKeyName ) &amp;&amp; keyType.isEqual( that.key, key ); &#125;...&#125; 通过源码可以看到，用 PersistenceContext 来判断实体是不是同一个，可以直接根据实体里面的主键进行。 一级缓存的作用由于一级缓存就是 PersistenceContext，那么一级缓存的最大作用就是管理 Entity 的生命周期。 New（Transient）状态的，不在一级缓存管理之列，这是新创建的； Detached 游离状态的，不在一级缓存里面，和 New 的唯一区别是它带有主键和 Version 信息； Manager、Removed 状态的实体在一级缓存管理之列，所有对这两种状态的实体进行的更新操作，都不会立即更新到数据库里面，只有执行了 flush 之后才会同步到数据库里面。 用一张图(注：图片来源于网络)来表示，如下所示。 对于实体 1 来说，新增和更新操作都是先进行一级缓存，只有 flush 的时候才会同步到数据库里面。而当我们执行了 entityManager.clean() 或者是 entityManager.detach(entity1)，那么实体 1 就会变成游离状态，这时再对实体 1 进行修改，如果再执行 flush 的话，就不会同步到 DB 里面了。用代码来说明一下，如下所示。 12345678910111213141516171819202122232425262728public class UserInfoRepositoryTest &#123; @Autowired private UserInfoRepository userInfoRepository; @PersistenceContext(properties = &#123;@PersistenceProperty( name = "org.hibernate.flushMode", value = "MANUAL"//手动flush )&#125;) private EntityManager entityManager; @Test @Transactional public void testLife() &#123; UserInfo userInfo = UserInfo.builder().name("new name").build(); //新增一个对象userInfo交给PersistenceContext管理，即一级缓存 entityManager.persist(userInfo); //此时没有detach和clear之前，flush的时候还会产生更新SQL userInfo.setName("old name"); entityManager.flush(); entityManager.clear();// entityManager.detach(userInfo); // entityManager已经clear，此时已经不会对UserInfo进行更新了 userInfo.setName("new name 11"); entityManager.flush(); //由于有cache机制，相同的对象查询只会触发一次查询SQL UserInfo u1 = userInfoRepository.findById(1L).get(); //to do some thing UserInfo u2 = userInfoRepository.findById(1L).get(); &#125;&#125; 把 SQL 打印一下，输出到控制台的 SQL 如下所示。 123Hibernate: insert into user_info (create_time, create_user_id, last_modified_time, last_modified_user_id, version, ages, email_address, last_name, name, telephone, id) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)Hibernate: update user_info set create_time=?, create_user_id=?, last_modified_time=?, last_modified_user_id=?, version=?, ages=?, email_address=?, last_name=?, name=?, telephone=? where id=? and version=?Hibernate: select userinfo0_.id as id1_2_0_, userinfo0_.create_time as create_t2_2_0_, userinfo0_.create_user_id as create_u3_2_0_, userinfo0_.last_modified_time as last_mod4_2_0_, userinfo0_.last_modified_user_id as last_mod5_2_0_, userinfo0_.version as version6_2_0_, userinfo0_.ages as ages7_2_0_, userinfo0_.email_address as email_ad8_2_0_, userinfo0_.last_name as last_nam9_2_0_, userinfo0_.name as name10_2_0_, userinfo0_.telephone as telepho11_2_0_, rooms1_.user_info_id as user_inf1_3_1_, room2_.id as rooms_id2_3_1_, room2_.id as id1_1_2_, room2_.create_time as create_t2_1_2_, room2_.create_user_id as create_u3_1_2_, room2_.last_modified_time as last_mod4_1_2_, room2_.last_modified_user_id as last_mod5_1_2_, room2_.version as version6_1_2_, room2_.title as title7_1_2_ from user_info userinfo0_ left outer join user_info_rooms rooms1_ on userinfo0_.id=rooms1_.user_info_id left outer join room room2_ on rooms1_.rooms_id=room2_.id where userinfo0_.id=? 通过日志可以看到没有第二次更新。 不能设置一级缓存的大小 从底层原理可以分析出：一级缓存依赖 Java 内存堆的大小，所以受到最大堆和最小堆的限制，即清除一级缓存的机制就是利用 JVM 的 GC 机制，清理掉 GC 就会清理掉一级缓存。 所以当请求并发量大的时候，Session 的对象就会变得很多，此时就会需要更多内存。当请求结束之后，随着 GC 的回收，里面就会清除一级缓存留下来的对象。 不能关闭一级缓存 除非不用 Hibernate 或 JPA，改用 Mybatis，因为一级缓存是 JPA 的最大优势之一。 Query Plan CacheJPA 里面大部分的查询都是基于 JPQL 查询语法，从而会有一个过程把 JPQL 转化成真正的 SQL，而后到数据库里执行。而 JPQL 转化成原始的 SQL 时，就会消耗一定的性能，所以 Hibernate 设计了一个 Query Plan Cache 的机制，用来存储 JPQL 或者 Criteria Query 到 Native SQL 中转化的结果，也就是说 Query Plan Cache 里面存储了最终要执行的 SQL，以及参数和返回结果的类型。 QueryPlanCache 是什么在 Hibernate 中，QueryPlanCache 就是指具体的某一个类。核心源码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package org.hibernate.engine.query.spi;//存储query plan 和 query parameter metdatapublic class QueryPlanCache implements Serializable &#123; //queryPlanCache的存储结构为自定义的HashMap结构，用来存储JPQL到SQL的转化过程及其SQL的执行语句和参数，返回结果的metadata; private final BoundedConcurrentHashMap queryPlanCache; //这个用来存储@Query的nativeQuery = true的query plan，即原始SQL的meta,包含参数和return type的 meta; private final BoundedConcurrentHashMap&lt;ParameterMetadataKey,ParameterMetadataImpl&gt; parameterMetadataCache; //QueryPlanCache的构造方法 public QueryPlanCache(final SessionFactoryImplementor factory, QueryPlanCreator queryPlanCreator) &#123; this.factory = factory; this.queryPlanCreator = queryPlanCreator; //maxParameterMetadata的个数，计算逻辑，可以自定义配置，或者采用默认值 Integer maxParameterMetadataCount = ConfigurationHelper.getInteger( Environment.QUERY_PLAN_CACHE_PARAMETER_METADATA_MAX_SIZE, factory.getProperties() ); if ( maxParameterMetadataCount == null ) &#123; maxParameterMetadataCount = ConfigurationHelper.getInt( Environment.QUERY_PLAN_CACHE_MAX_STRONG_REFERENCES, factory.getProperties(), DEFAULT_PARAMETER_METADATA_MAX_COUNT ); &#125; //maxQueryPlan的个数，计算逻辑，可以自定义配置大小，或者采用默认值 Integer maxQueryPlanCount = ConfigurationHelper.getInteger( Environment.QUERY_PLAN_CACHE_MAX_SIZE, factory.getProperties() ); if ( maxQueryPlanCount == null ) &#123; maxQueryPlanCount = ConfigurationHelper.getInt( Environment.QUERY_PLAN_CACHE_MAX_SOFT_REFERENCES, factory.getProperties(), DEFAULT_QUERY_PLAN_MAX_COUNT ); &#125; //新建一个 BoundedConcurrentHashMap的queryPlanCache，用来存储JPQL和Criteria Query到SQL的转化过程 queryPlanCache = new BoundedConcurrentHashMap( maxQueryPlanCount, 20, BoundedConcurrentHashMap.Eviction.LIRS ); //新建一个 BoundedConcurrentHashMap的parameterMetadataCache，用来存储Native SQL的转化过程 parameterMetadataCache = new BoundedConcurrentHashMap&lt;&gt;( maxParameterMetadataCount, 20, BoundedConcurrentHashMap.Eviction.LIRS ); nativeQueryInterpreter = factory.getServiceRegistry().getService( NativeQueryInterpreter.class ); &#125; // 默认的parameterMetadataCache的HashMap的存储空间大小，默认128条 public static final int DEFAULT_PARAMETER_METADATA_MAX_COUNT = 128; //默认的queryPlanCache的HashMap存储空间大小，默认2048条 public static final int DEFAULT_QUERY_PLAN_MAX_COUNT = 2048;......不重要的代码先省略&#125; QueryPlanCache 存储的内容新建一个 UserInfoRepository，来测试一下。 123456789101112131415public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt; &#123; //没有用@Query，直接使用method name defining query List&lt;UserInfo&gt; findByNameAndCreateTimeBetween(String name, Instant begin, Instant endTime); //演示SpEL根据数组下标取参数，和根据普通的Parma的名字:name取参数 @Query("select u from UserInfo u where u.lastName like %:#&#123;[0]&#125; and u.name like %:name%") List&lt;UserInfo&gt; findContainingEscaped(@Param("name") String name); //SpEL取Parma的名字customer里面的属性 @Query("select u from UserInfo u where u.name = :#&#123;#customer.name&#125;") List&lt;UserInfo&gt; findUsersByCustomersFirstname(@Param("customer") UserInfo customer); //利用SpEL根据一个写死的'jack'字符串作为参数 @Query("select u from UserInfo u where u.name = ?#&#123;'jack'&#125;") List&lt;UserInfo&gt; findOliverBySpELExpressionWithoutArgumentsWithQuestionmark(); @Query(value = "select * from user_info where name=:name",nativeQuery = true) List&lt;UserInfo&gt; findByName(@Param(value = "name") String name);&#125; 通过 @Query 定义的 nativeQuery=false 的 JPQL，会在启动成功之后预先放在 QueryPlanCache 里面，设置一个断点就可以看到如下内容： 发现里面 parameterMetadataCache 是空的，也就是没有放置 nativeQuery=true 的 Query SQL，并且可以看到在方法里面定义的其他三个 @Query 的 JPQL 解析过程。打开第一个详细看一下，如下图所示。 QueryPlanCache 还是能存挺多东西的：navtive sql、参数、return 等各种 metadata。也可以看出一个简单的 JPQL 查询会有些占用堆内存，所以如果是复杂点的项目，各种查询的 JPQL 多一点的话，启动所需要的最小堆内存会占用 300M、400M 的空间，这是正常现象。 在 UserInfoRepository 的五个方法中，剩下的两个方法分别是 name defining query 和 nativeQuery=true。这两种情况是，当调用的时候发现 QueryPlanCache 里面没有它们，于是就会被增加进去，下次就可以直接从 QueryPlanCache 里面取了。那么在 Controller 里面执行这两个方法，如下所示。 12userInfoRepository.findByNameAndCreateTimeBetween("JK", Instant.now(),Instant.now());userInfoRepository.findByName("jack"); 然后通过断点就会发现 QueryPlanCache 里面多了两个 Cache，如下图所示。 同时，parameterMetadataCache 里面就会多一条 key/value 的 nativeQuery=true 的解析记录，如下图所示。 总结起来就是，QueryPlanCache 用来存储的 JQPL 或者 SQL 的 Metadata 信息，从而提升了 Hibernate 执行 JPQL 的性能，因为只有第一次需要把 JPQL 转化成 SQL，后面的每次操作就可以直接从 HashMap 中找到对应的 SQL，直接执行就可以了。 QueryPlanCache 和 Session 是什么关系在 SessionFactoryImpl 的构造方法里面会 new QueryPlanCache(...)，关键源码如下。 说明这个 application 只需要创建一次 QueryPlanCache，整个项目周期是单例的，也就是可以被不同的 Session 共享，那么可以查看 Session 的关键源码，如下图所示。 也就是说，每一个 SessionImpl 的实例在获得 query plan 之前，都会去同一个 QueryPlanCache 里面查询一下 JPQL 对应的执行计划。所以可以看得出来 QueryPlanCache 和 Session 的关系有如下几点。 QueryPlanCache 在整个 Spring Application 周期内就是一个实例 不同的 Session 作用域，可以代表不同的 SessionImpl 实例共享 QueryPlanCache QueryPlanCache 和一级缓存完全不是一个概念 QueryPlanCache 中 In 查询引发的内存泄漏问题在实际的工作中使用 JPA 的时候，会发现其内存越来越大，而不会被垃圾回收机制给回收掉，现象就是堆内存随着时间的推移使用量越来越大，如下图所示，很明显是内存泄漏的问题。 而把堆栈拿出来分析的话会发现，其实是 Hibernate 的 QueryPlanCache 占用了大量的内存，如下图所示。 In 查询条件引发内存泄漏的原因在 UserInfoRepository 里面新增一个 In 条件的查询方法，模拟一下实际工作中的 In 查询条件的场景，如下所示。 1234public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt; &#123;//测试In查询条件的情况List&lt;UserInfo&gt; findByNameAndUrlIn(String name, Collection&lt;String&gt; urls);&#125; 假设有个需求，查询拥有个人博客地址的用户有哪些？ Controller 里面有如下方法。 12345@GetMapping("/users")public List&lt;UserInfo&gt; getUserInfos(List&lt;String&gt; urls) &#123; //根据urls批量查询，模拟实际工作中的批量查询情况，实际工作中可能会有大量的根据不同的IDS批量查询的场景； return userInfoRepository.findByNameAndUrlIn("jack",urls);&#125; debug 看一下 QueryPlanCache 里面的情况，会发现随着 In 查询条件的个数增加，会生成不同的 QueryPlanCache，如下图所示，分别是 1 个参数、3 个参数、6个参数的情况。 从图中可以想象一下，如果业务代码中有各种 In 的查询操作，不同的查询条件的个数肯定在大部分场景中也是不一样的，甚至有些场景能一下查询到几百个 ID 对应的数据，可想而知，那得生成多少个 In 相关的 QueryPlanCache 呀。 而依据 QueryPlanCache 的原理，整个工程都是单例的，放进去之后肯定不会进行内存垃圾回收，那么程序运行时间久了之后就会发生内存泄漏，甚至一段时间之后还会导致内存溢出的现象发生。那么该如何解决此类问题呢？ 解决 In 查询条件内存泄漏的方法第一种方法：修改缓存的最大条数限制正如上面介绍的，默认 DEFAULT_QUERY_PLAN_MAX_COUNT = 2048，也就是 query plan 的最大条数限制是 2048。这样默认值可能有点大了，可以通过如下方式修改默认值，请看代码。 1234#修改 默认的plan_cache_max_size，太小会影响JPQL的执行性能，所以根据实际情况可以自由调整，不宜太小，也不宜太大，太大可能会引发内存溢出spring.jpa.properties.hibernate.query.plan_cache_max_size=512#修改 默认的native query的cache大小spring.jpa.properties.hibernate.query.plan_parameter_metadata_max_size=128 第二种方法：根据 max plan count 适当增加堆内存大小因为 QueryPlanMaxCount 是有限制的，那么肯定最大堆内存的使用也是有封顶限制的，找到临界值修改最小、最大堆内存即可。 第三种方法：减少 In 的查询 SQL 生成条数12### 默认情况下，不同的in查询条件的个数会生成不同的plan query cache，我们开启了in_clause_parameter_padding之后会减少in生成cache的个数，会根据参数的格式运用几何的算法生成QueryCache；spring.jpa.properties.hibernate.query.in_clause_parameter_padding=true 当 In 的时候，参数个数会对应归并 QueryPlanCache 变成 1、2、4、8、16、32、64、128 个参数的 QueryPlanCache。那么再看一下刚才参数个数分别在 1、3、4、5、6、7、8 个的时候生成 QueryPlanCache 的情况，如下图所示。 会发现，In 产生个数是 1 个的时候，它会共享参数为 1 个的 QueryPlanCache；而当参数是 3、4 个 In 参数的时候，它就会使用 4 个参数的 QueryPlanCache；以此类推，当参数是 5、6、7、8 个的时候，会使用 8 个参数的 QueryPlanCache……这种算法可以大大地减少 In 的不同查询参数生成的 QueryPlanCache 个数，占用的内存自然会减少很多。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 使用 SPEL]]></title>
    <url>%2F2021%2F01%2F25%2FSpringDataJpa%E4%BD%BF%E7%94%A8SPEL%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa 使用 SPELSpEL 基础语法SpEL 大纲SpEL 的全称为 Spring Expression Language，即 Spring 表达式语言，是 Spring framework 里面的核心项目。spring-expression 的 jar 包的引用关系，如下图所示。 从核心引用来看，SpEL 贯穿所有 Spring 的核心功能。当然了，SpEL 可以脱离 Spring 工程独立使用，其项目里有三个重要的接口：ExpressionParser、Expression、EvaluationContext，我从官方文档中找了一张图来说明它们之间的关系。 ExpressionParser它是 SpEL 的处理接口，默认实现类是 SpelExpressionParser，对外提供的只有两个方法，如下述代码所示。 123456public interface ExpressionParser &#123; // 根据传入的表达式生成Expression Expression parseExpression(String expressionString) throws ParseException; // 根据传入的表达式和ParserContext生成Expression对象 Expression parseExpression(String expressionString, ParserContext context) throws ParseException;&#125; 这两个方法的目的都是生成 Expression。 Expression它默认的实现是 SpELExpression，主要对外提供的接口就是根据表达式获得表达式响应的结果，如下图所示。 而它的这些方法中，最重的一个参数就是 EvaluationContext。 EvaluationContext表示解析 String 表达式所需要的上下文，例如寻找 ROOT 是谁，反射解析的 Method、Field、Constructor 的解析器和取值所需要的上下文。看一下其接口提供的方法，如下图所示。 SpEL 的基本用法123456789//ExpressionParser是操作SpEL的总入口，创建一个接口ExpressionParser对应的实例SpelExpressionParserExpressionParser parser = new SpelExpressionParser();//通过上面的parser.parseExpression方法获得一个Expression的实例，里面实现的就是new一个SpelExpression对象；而parseExpression的参数就是SpEL的使用重点，各种表达式的字符串//1.简单的string类型用'' 引用Expression exp = parser.parseExpression("'Hello World'");//2.SpEL支持很多功能特性，如调用方法、访问属性、调用构造函数，可以直接调用String对象里面的concat方法进行字符串拼接Expression exp = parser.parseExpression("'Hello World'.concat('!')");//通过getValue方法可以得到经过Expresion计算parseExpression方法的字符串参数(符合SpEL语法的表达式)的结果String message = (String) exp.getValue(); 而访问属性值如下所示。 123//3.invokes getBytes()方法Expression exp = parser.parseExpression("'Hello World'.bytes");byte[] bytes = (byte[]) exp.getValue(); //得到 byte[]类型的结果 SpEL 字符串表达式还支持使用“.”进行嵌套属性 prop1.prop2.prop3 访问，代码如下。 123// invokes getBytes().lengthExpression exp = parser.parseExpression(&quot;&apos;Hello World&apos;.bytes.length&quot;);int length = (Integer) exp.getValue(); 访问构造方法，例如字符串的构造方法，如下所示。 12Expression exp = parser.parseExpression("new String('hello world').toUpperCase()");String message = exp.getValue(String.class); 也可以通过 EvaluationContext 来配置一些根元素，代码如下。 1234567891011//通过一个Expression表达式想取name属性对应的值ExpressionParser parser = new SpelExpressionParser();Expression exp = parser.parseExpression("name");//通过EvaluationContext设置rootObject等于new的UserInfo对象UserInfo rootUserInfo = UserInfo.builder().name("jack").build();EvaluationContext context = new StandardEvaluationContext(rootUserInfo);//getValue根据设置的context取值，可以得到jack字符串String name = (String) exp.getValue(context);//也可以利用SpEL的表达式进行运算，判断名字是否等于字符串NikolaExpression exp2 = parser.parseExpression("name == 'Nikola'");boolean result2 = exp2.getValue(context, Boolean.class); // 根据UserInfo的rootObject得到false SpelExpressionParser 的构造方法还支持一些配置，例如经常遇到空指针异常和下标越界的问题，就可以通过 SpelParserConfiguration 配置：当 Null 的时候自动初始化，当 Collection 越界的时候自动扩容增加。例子如下所示： 12345678910111213//构造一个Class，方便测试class MyUser &#123; public List&lt;String&gt; address;&#125;//开启自动初始化null和自动扩容collectionSpelParserConfiguration config = new SpelParserConfiguration(true,true);//利用config生成ExpressionParser的实例ExpressionParser parser = new SpelExpressionParser(config);//通过表达式取这个用户的第三个地址Expression expression = parser.parseExpression("address[3]");MyUser demo = new MyUser(); //new一个对象，但是没有初始化MyUser里面的address，由于配置了自动初始化和扩容，所以通过下面的计算，没有得到异常，o可以得到一个空的字符串Object o = expression.getValue(demo);// 空字符串 SpEL 在 Spring 中常见的使用场景SpEL 在 @Value 里面的用法最常见 @Value 的应用场景新建一个 DemoProperties 对象，用 Spring 装载，测试一下两个语法点：运算符和 Map、List。 第一个语法：通过 @Value 展示 SpEL 里面支持的各种运算符的写法。如下面的表格所示。 类型 操作符 逻辑运算 +, -, *, /, %, ^, div, mod 逻辑比较符号 &lt;, &gt;, ==, !=, &lt;=, &gt;=, lt, gt, eq, ne, le, ge 逻辑关系 and, or, not, &amp;&amp;, ||, ! 三元表达式 ?: 正则表达式 matches 展示一下 SpEL 里面支持的各种运算符 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106@Data@ToString@Component //通过@Value使用SpEL的地方，一定要将此对象交由Spring进行管理public class DemoProperties &#123; // 第一部分：逻辑运算操作 @Value("#&#123;19 + 1&#125;") // 20 private double add; @Value("#&#123;'String1 ' + 'string2'&#125;") // "String1 string2" private String addString; @Value("#&#123;20 - 1&#125;") // 19 private double subtract; @Value("#&#123;10 * 2&#125;") // 20 private double multiply; @Value("#&#123;36 / 2&#125;") // 19 private double divide; @Value("#&#123;36 div 2&#125;") // 18, the same as for / operator private double divideAlphabetic; @Value("#&#123;37 % 10&#125;") // 7 private double modulo; @Value("#&#123;37 mod 10&#125;") // 7, the same as for % operator private double moduloAlphabetic; // 第二部分：逻辑比较符号 @Value("#&#123;1 == 1&#125;") // true private boolean equal; @Value("#&#123;1 eq 1&#125;") // true private boolean equalAlphabetic; @Value("#&#123;1 != 1&#125;") // false private boolean notEqual; @Value("#&#123;1 ne 1&#125;") // false private boolean notEqualAlphabetic; @Value("#&#123;1 &lt; 1&#125;") // false private boolean lessThan; @Value("#&#123;1 lt 1&#125;") // false private boolean lessThanAlphabetic; @Value("#&#123;1 &lt;= 1&#125;") // true private boolean lessThanOrEqual; @Value("#&#123;1 le 1&#125;") // true private boolean lessThanOrEqualAlphabetic; @Value("#&#123;1 &gt; 1&#125;") // false private boolean greaterThan; @Value("#&#123;1 gt 1&#125;") // false private boolean greaterThanAlphabetic; @Value("#&#123;1 &gt;= 1&#125;") // true private boolean greaterThanOrEqual; @Value("#&#123;1 ge 1&#125;") // true private boolean greaterThanOrEqualAlphabetic; // 第三部分：逻辑关系运算符 @Value("#&#123;250 &gt; 200 &amp;&amp; 200 &lt; 4000&#125;") // true private boolean and; @Value("#&#123;250 &gt; 200 and 200 &lt; 4000&#125;") // true private boolean andAlphabetic; @Value("#&#123;400 &gt; 300 || 150 &lt; 100&#125;") // true private boolean or; @Value("#&#123;400 &gt; 300 or 150 &lt; 100&#125;") // true private boolean orAlphabetic; @Value("#&#123;!true&#125;") // false private boolean not; @Value("#&#123;not true&#125;") // false private boolean notAlphabetic; // 第四部分：三元表达式 &amp; Elvis运算符 @Value("#&#123;2 &gt; 1 ? 'a' : 'b'&#125;") // "b" private String ternary; //demoProperties 就是通过spring加载的当前对象， //取spring容器里面的某个bean的属性， //这里取的是demoProperties对象里面的someProperty属性， //如果不为null就直接用，如果为null返回'default'字符串 @Value("#&#123;demoProperties.someProperty != null ? demoProperties.someProperty : 'default'&#125;") private String ternaryProperty; /** * Elvis运算符是三元表达式简写的方式，和上面一样的结果。如果someProperty为null则返回default值。 */ @Value("#&#123;demoProperties.someProperty ?: 'default'&#125;") private String elvis; /** * 取系统环境的属性，如果系统属性pop3.port已定义会直接注入，如果未定义，则返回默认值25。systemProperties是spring容器里面的systemProperties实体； */ @Value("#&#123;systemProperties['pop3.port'] ?: 25&#125;") private Integer port; /** * 还可以用于安全引用运算符主要为了避免空指针，源于Groovy语言。 * 很多时候你引用一个对象的方法或者属性时都需要做非空校验。 * 为了避免此类问题，使用安全引用运算符只会返回null而不是抛出一个异常。 */ //@Value("#&#123;demoPropertiesx?:someProperty&#125;") // 如果demoPropertiesx不为null，则返回someProperty值 private String someProperty; // 第五部分：正则表达式的支持 @Value("#&#123;'100' matches '\\d+' &#125;") // true private boolean validNumericStringResult; @Value("#&#123;'100fghdjf' matches '\\d+' &#125;") // false private boolean invalidNumericStringResult; // 利用matches匹配正则表达式，返回true @Value("#&#123;'valid alphabetic string' matches '[a-zA-Z\\s]+' &#125;") private boolean validAlphabeticStringResult; @Value("#&#123;'invalid alphabetic string #$1' matches '[a-zA-Z\\s]+' &#125;") // false private boolean invalidAlphabeticStringResult; //如果someValue只有数字 @Value("#&#123;demoProperties.someValue matches '\\d+'&#125;") // true private boolean validNumericValue; //新增一个空的someValue属性方便测试 private String someValue="";&#125; 可以通过 @Value 测试各种 SpEL 的表达式，这和放在 parser.parseExpression(&quot;SpEL 的表达式字符串&quot;); 里面的效果是一样的。 测试用例如下所示： 12345678910111213141516171819@ExtendWith(SpringExtension.class)@Import(TestConfiguration.class)@ComponentScan(value = "com.example.jpa.demo.config.DemoProperties")public class DemoPropertiesTest &#123; @Autowired(required = false) private DemoProperties demoProperties; @Test public void testSpel() &#123; //测试@Value里面不同表达式的值了 System.out.println(demoProperties.toString()); &#125; @TestConfiguration static class TestConfig &#123; @Bean public DemoProperties demoProperties () &#123; return new DemoProperties(); &#125; &#125;&#125; 或者可以启动项目，也能看到结果。 @Value 的解析原理Spring 项目启动的时候会根据 @Value 的注解，去加载 SpelExpressionResolver 及算出来需要的 StandardEvaluationContext，然后再调用 Expression 方法进行 getValue 操作，其中计算 StandardEvaluationContext 的关键源码如下面两张图所示。 第二个语法：@Value 展示了 SpEL 可以直接读取 Map 和 List 里面的值，代码如下所示。 123456789101112131415161718192021222324252627282930//通过@Component加载一个类，并且给其中的List和Map附上值@Component("workersHolder")public class WorkersHolder &#123; private List&lt;String&gt; workers = new LinkedList&lt;&gt;(); private Map&lt;String, Integer&gt; salaryByWorkers = new HashMap&lt;&gt;(); public WorkersHolder() &#123; workers.add("John"); workers.add("Susie"); workers.add("Alex"); workers.add("George"); salaryByWorkers.put("John", 35000); salaryByWorkers.put("Susie", 47000); salaryByWorkers.put("Alex", 12000); salaryByWorkers.put("George", 14000); &#125; //Getters and setters ...&#125;//SpEL直接读取Map和List里面的值@Value("#&#123;workersHolder.salaryByWorkers['John']&#125;") // 35000private Integer johnSalary;@Value("#&#123;workersHolder.salaryByWorkers['George']&#125;") // 14000private Integer georgeSalary;@Value("#&#123;workersHolder.salaryByWorkers['Susie']&#125;") // 47000private Integer susieSalary;@Value("#&#123;workersHolder.workers[0]&#125;") // Johnprivate String firstWorker;@Value("#&#123;workersHolder.workers[3]&#125;") // Georgeprivate String lastWorker;@Value("#&#123;workersHolder.workers.size()&#125;") // 4private Integer numberOfWorkers; @Value 使用的注意事项 # 与 $ 的区别SpEL 表达式默认以 # 开始，以大括号进行包住，如 #{expression}。默认规则在 ParserContext 里面设置，也可以自定义，但是一般建议不要动。 这里注意要与 Spring 中的 Properties 进行区别，Properties 相关的表达式是以 $ 开始的大括号进行包住的，如 ${property.name}。 也就是说 @Value 的值有两类： ${ property**:**default_value } #{ obj.property**? :**default_value } 第一个注入的是外部参数对应的 Property，第二个则是 SpEL 表达式对应的内容。 而 Property placeholders 不能包含 SpEL 表达式，但是 SpEL 表达式可以包含 Property 的引用。如 #{${someProperty} + 2}，如果 someProperty=1，那么效果将是 #{ 1 + 2}，最终的结果将是 3。 JPA 中 @Query 的应用场景SpEL 除了能在 @Value 里面使用外，也能在 @Query 里使用，而在 @Query 里还有一个特殊的地方，就是它可以用来取方法的参数。 通过 SpEL 取被 @Query 注解的方法参数在 @Query 注解中使用 SpEL 的主要目的是取方法的参数，主要有三种用法，如下所示。 123456789//用法一：根据下标取方法里面的参数@Query("select u from User u where u.age = ?#&#123;[0]&#125;") List&lt;User&gt; findUsersByAge(int age);//用法二：#customer取@Param("customer")里面的参数@Query("select u from User u where u.firstname = :#&#123;#customer.firstname&#125;")List&lt;User&gt; findUsersByCustomersFirstname(@Param("customer") Customer customer);//用法三：用JPA约定的变量entityName取得当前实体的实体名字@Query("from #&#123;#entityName&#125;")List&lt;UserInfo&gt; findAllByEntityName(); 其中， 方法一可以通过 [0] 的方式，根据下标取到方法的参数； 方法二通过 #customer 可以根据 @Param 注解的参数的名字取到参数，必须通过 ?#{} 和 :#{} 来触发 SpEL 的表达式语法； 方法三通过下述语法，取约定的实体的名字。 1#&#123;#entityName&#125; 再来看一个更复杂一点的例子，代码如下。 123456789101112131415161718192021222324252627282930313233343536373839public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt; &#123; // JPA约定的变量entityName取得当前实体的实体名字 @Query("from #&#123;#entityName&#125;") List&lt;UserInfo&gt; findAllByEntityName(); //一个查询中既可以支持SpEL也可以支持普通的:ParamName的方式 @Modifying @Query("update #&#123;#entityName&#125; u set u.name = :name where u.id =:id") void updateUserActiveState(@Param("name") String name, @Param("id") Long id); //演示SpEL根据数组下标取参数，和根据普通的Parma的名字:name取参数 @Query("select u from UserInfo u where u.lastName like %:#&#123;[0]&#125; and u.name like %:name%") List&lt;UserInfo&gt; findContainingEscaped(@Param("name") String name); //SpEL取Parma的名字customer里面的属性 @Query("select u from UserInfo u where u.name = :#&#123;#customer.name&#125;") List&lt;UserInfo&gt; findUsersByCustomersFirstname(@Param("customer") UserInfo customer); //利用SpEL根据一个写死的'jack'字符串作为参数 @Query("select u from UserInfo u where u.name = ?#&#123;'jack'&#125;") List&lt;UserInfo&gt; findOliverBySpELExpressionWithoutArgumentsWithQuestionmark(); //同时SpEL支持特殊函数escape和escapeCharacter @Query("select u from UserInfo u where u.lastName like %?#&#123;escape([0])&#125;% escape ?#&#123;escapeCharacter()&#125;") List&lt;UserInfo&gt; findByNameWithSpelExpression(String name); // #entityName和#[]同时使用 @Query("select u from #&#123;#entityName&#125; u where u.name = ?#&#123;[0]&#125; and u.lastName = ?#&#123;[1]&#125;") List&lt;UserInfo&gt; findUsersByFirstnameForSpELExpressionWithParameterIndexOnlyWithEntityExpression(String name, String lastName); //对于 native SQL同样适用，并且同样支持取pageable分页里面的属性值 @Query(value = "select * from (" // + "select u.*, rownum() as RN from (" // + "select * from user_info ORDER BY ucase(firstname)" // + ") u" // + ") where RN between ?#&#123; #pageable.offset +1 &#125; and ?#&#123;#pageable.offset + #pageable.pageSize&#125;", // countQuery = "select count(u.id) from user_info u", // nativeQuery = true) Page&lt;UserInfo&gt; findUsersInNativeQueryWithPagination(Pageable pageable);&#125; 个人比较推荐使用 @Param 的方式，这样语义清晰，参数换位置了也不影响执行结果。 关于源码的实现，可以到 ExpressionBasedStringQuery.class 里面继续研究，关键代码如下图所示。 spring-security-data 在 @Query 中的用法在实际工作中，有时会用 spring-security 做鉴权，详细可以参考官方文档。 当我们用 Spring Security 的时候，其实可以额外引入 jai 包 spring-security-data。如果使用了 JPA 和 Spring Security 的话，build.gradle 最终会变成如下形式，请看代码。 123456//引入spring data jpaimplementation &apos;org.springframework.boot:spring-boot-starter-data-jpa&apos;//集成spring securityimplementation &apos;org.springframework.boot:spring-boot-starter-security&apos;// 集成spring security data对JPA的支持implementation &apos;org.springframework.security:spring-security-data&apos; 假设继承 Spring Security 之后，SecurityContextHolder 里面放置的 Authentication 是 UserInfo，代码如下。 12//应用上下文中设置登录用户信息，此时Authentication类型为UserInfoSecurityContextHolder.getContext().setAuthentication(authentication); 这样 JPA 里面的 @Query 就可以取到当前的 SecurityContext 信息，其用法如下所示。 1234567// 根据当前用户email取当前用户的信息@Query("select u from UserInfo u where u.emailAddress = ?#&#123;principal.email&#125;")List&lt;UserInfo&gt; findCurrentUserWithCustomQuery();//如果当前用户是admin，就返回某业务的所有对象；如果不是admin角色，就只给当前用户的某业务数据@Query("select o from BusinessObject o where o.owner.emailAddress like "+ "?#&#123;hasRole('ROLE_ADMIN') ? '%' : principal.emailAddress&#125;")List&lt;BusinessObject&gt; findBusinessObjectsForCurrentUser(); 通过看源码会发现，spring-security-data 就帮我们做了一件事情：实现 EvaluationContextExtension，设置了 SpEL 所需要的 rootObject 为 SecurityExpressionRoot。关键代码如下图所示。 由于 SecurityExpressionRoot 是 rootObject，根据上面介绍的 SpEL 的基本用法，SecurityExpressionRoot 里面的各种属性和方法都可以在 SpEL 中使用，如下图所示。 这其实也给了一些启发：当需要自动 rootObject 给 @Query 使用的时候，也可以采用这种方式，这样 @Query 的灵活性会增强很多。 SpEL 在 @Cacheable 中的应用场景在实际工作中还有一个经常用到 SpEL 的场景，就是在 Cache 的时候，也就是 Spring Cache 的相关注解里面，如 @Cacheable、@CachePut、@CacheEvict 等。代码如下所示。 1234567891011121314//缓存key取当前方法名，判断一下只有返回结果不为null或者非empty才进行缓存@Cacheable(value = "APP", key = "#root.methodName", cacheManager = "redis.cache", unless = "#result == null || #result.isEmpty()")@Overridepublic Map&lt;String, Map&lt;String, String&gt;&gt; getAppGlobalSettings() &#123;&#125;//evict策略的key是当前参数customer里面的name属性@Caching(evict = &#123;@CacheEvict(value="directory", key="#customer.name") &#125;)public String getAddress(Customer customer) &#123;...&#125;//在condition里面使用，当参数里面customer的name属性的值等于字符串Tom才放到缓存里面@CachePut(value="addresses", condition="#customer.name=='Tom'")public String getAddress(Customer customer) &#123;...&#125;//用在unless里面，利用SpEL的条件表达式判断，排除返回的结果地址长度小于64的请求@CachePut(value="addresses", unless="#result.length()&lt;64")public String getAddress(Customer customer) &#123;...&#125; Spring Cache 中 SpEL 支持的上下文语法Spring Cache 提供了一些可使用的 SpEL 上下文数据，如下表所示（摘自 Spring 官方文档）。 支持的属性 作用域 功能描述 使用方法 methodName root 对象 当前被调用的方法名 #root.methodName method root 对象 当前被调用的方法 #root.method.name target root 对象 当前被调用的目标对象 #root.target targetClass root 对象 当前被调用的目标对象类 #root.targetClass args root 对象 当前被调用的方法的参数列表 #root.args[0] caches root 对象 当前方法调用使用的缓存列表（如@Cacheable(value={“cache1”, “cache2”})），则有两个 cache #root.caches[0].name argument name 执行上下文 当前被调用的方法的参数，如 findById(Long id)，我们可以通过 #id 拿到参数 #user.id 表示参数 user 里面的 id result 执行上下文 方法执行后的返回值（仅当方法执行之后的判断有效，如‘unless’，’cache evict’的 beforeInvocation=false） #result 也可以看一下 Spring Cache 中 SpEL 的 EvaluationContext 加载方式，关键源码如下图所示。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 如何解决 N+1 SQL 问题]]></title>
    <url>%2F2021%2F01%2F12%2FSpringDataJpa%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3N%2B1SQL%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa 如何解决 N+1 SQL 问题在 JPA 的使用过程中，N+1 SQL 是很常见的问题 什么是 N+1 SQL 问题？ 想要解决一个问题，必须要知道它是什么、如何产生的，这样才能有方法、有逻辑地去解决它。 下面通过一个例子来看一下什么是 N+1 的 SQL 问题。 假设一个 UserInfo 实体对象和 Address 是一对多的关系，代码如下： 123456789101112131415161718192021222324252627282930//UserInfo实体对象如下：@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@Table@ToString(exclude = "addressList")//exclude防止 toString打印日志的时候死循环public class UserInfo extends BaseEntity &#123; private String name; private String telephone; // UserInfo实体对象的关联关系由Address对象里面的userInfo字段维护，默认是lazy加载模式 @OneToMany(mappedBy = "userInfo",fetch = FetchType.EAGER) private List&lt;Address&gt; addressList;&#125;//Address对象如下：@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "userInfo")public class Address extends BaseEntity &#123; private String city; //维护UserInfo和Address的外键关系 @ManyToOne(fetch = FetchType.EAGER) @JsonBackReference //此注解防止JSON死循环 private UserInfo userInfo;&#125; 其次，假设数据库里面有三条 UserInfo 的数据，ID 分别为 3、6、9，如下图所示： 其中，每个 UserInfo 分别有两条 Address 数据，也就是一共 6 条 Address 的数据，如下图所示： 然后，请求通过 UserInfoRepository 查询所有的 UserInfo 信息，代码如下： 1userInfoRepository.findAll() 现在，控制台将会得到四个 SQL，如下所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849org.hibernate.SQL :select userinfo0_.id as id1_1_, userinfo0_.create_time as create_t2_1_, userinfo0_.create_user_id as create_u3_1_, userinfo0_.last_modified_time as last_mod4_1_, userinfo0_.last_modified_user_id as last_mod5_1_, userinfo0_.version as version6_1_, userinfo0_.ages as ages7_1_, userinfo0_.email_address as email_ad8_1_, userinfo0_.last_name as last_nam9_1_, userinfo0_.name as name10_1_, userinfo0_.telephone as telepho11_1_from user_info userinfo0_ org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_0_, addresslis0_.id as id1_0_0_, addresslis0_.id as id1_0_1_, addresslis0_.create_time as create_t2_0_1_, addresslis0_.create_user_id as create_u3_0_1_, addresslis0_.last_modified_time as last_mod4_0_1_, addresslis0_.last_modified_user_id as last_mod5_0_1_, addresslis0_.version as version6_0_1_, addresslis0_.city as city7_0_1_, addresslis0_.user_info_id as user_inf8_0_1_from address addresslis0_where addresslis0_.user_info_id = ? org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_0_, addresslis0_.id as id1_0_0_, addresslis0_.id as id1_0_1_, addresslis0_.create_time as create_t2_0_1_, addresslis0_.create_user_id as create_u3_0_1_, addresslis0_.last_modified_time as last_mod4_0_1_, addresslis0_.last_modified_user_id as last_mod5_0_1_, addresslis0_.version as version6_0_1_, addresslis0_.city as city7_0_1_, addresslis0_.user_info_id as user_inf8_0_1_from address addresslis0_where addresslis0_.user_info_id = ? org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_0_, addresslis0_.id as id1_0_0_, addresslis0_.id as id1_0_1_, addresslis0_.create_time as create_t2_0_1_, addresslis0_.create_user_id as create_u3_0_1_, addresslis0_.last_modified_time as last_mod4_0_1_, addresslis0_.last_modified_user_id as last_mod5_0_1_, addresslis0_.version as version6_0_1_, addresslis0_.city as city7_0_1_, addresslis0_.user_info_id as user_inf8_0_1_from address addresslis0_where addresslis0_.user_info_id = ? 通过 SQL 可以看得出来，当取 UserInfo 的时候，有多少条 UserInfo 数据就会触发多少条查询 Address 的 SQL。 那么所谓的 N+1 的 SQL，此时 1 代表的是一条 SQL 查询 UserInfo 信息；N 条 SQL 查询 Address 的信息。想象一下，如果有 100 条 UserInfo 信息，可能会触发 100 条查询 Address 的 SQL，性能多差呀。这里使用的是 EAGER 模式，当使用 LAZY 的时候也是一样的道理，只是生成 N 条 SQL 的时机是不一样的。 上面演示了 @OneToMany 的情况，那么再看一下 @ManyToOne 的情况。利用 AddressRepository 查询所有的 Address 信息，代码如下： 1addressRepository.findAll(); 控制台会产生如下 SQL： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485org.hibernate.SQL :select address0_.id as id1_0_, address0_.create_time as create_t2_0_, address0_.create_user_id as create_u3_0_, address0_.last_modified_time as last_mod4_0_, address0_.last_modified_user_id as last_mod5_0_, address0_.version as version6_0_, address0_.city as city7_0_, address0_.user_info_id as user_inf8_0_from address address0_ org.hibernate.SQL :select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id = ? org.hibernate.SQL :select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id = ? org.hibernate.SQL :select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id = ? 这里通过 SQL 可以看得出来，当取 Address 的时候，Address 里面有多少个 user_info_id，就会触发多少条查询 UserInfo 的 SQL。 那么所谓的 N+1 的 SQL，此时 1 就代表一条 SQL 查询 Address 信息；N 条 SQL 查询 UserInfo 的信息。同样，想象一下，如果有 100 条 Address 信息，分别有不同的 user_info_id 可能会触发 100 条查询 UserInfo 的 SQL，性能依然很差。 这只是演示了 @OneToMany 和 @ManyToOne 的情况，@ManyToMany 和 @OneToOne 也是一样的道理，都是当我们查询主体信息时候，1 条 SQL 会衍生出来关联关系的 N 条 SQL。 现在认识了这个问题，下一步该思考，怎么解决才更合理呢？有没有什么办法可以减少 SQL 条数呢？ 减少 N+1 SQL 的条数最容易想到，就是有没有什么机制可以减少 N 对应的 SQL 条数呢？从原理分析会知道，不管是 LAZY 还是 EAGER 都是没有用的，因为这两个只是决定了 N 条 SQL 的触发时机，而不能减少 SQL 的条数。 hibernate.default_batch_fetch_size 配置hibernate.default_batch_fetch_size 配置在 AvailableSettings.class 里面，指的是批量获取数据的大小，默认是 -1，表示默认没有匹配取数据。那么把这个值改成 20 看一下效果，只需要在 application.properties 里面增加如下配置即可。 12# 更改批量取数据的大小为20spring.jpa.properties.hibernate.default_batch_fetch_size=20 在实体类不发生任何改变的前提下，再执行如下两个方法，分别看一下 SQL 的生成情况。 1userInfoRepository.findAll(); 还是先查询所有的 UserInfo 信息，看一下 SQL 的执行情况，代码如下所示。 12345678910111213141516171819202122232425org.hibernate.SQL :select userinfo0_.id as id1_1_, userinfo0_.create_time as create_t2_1_, userinfo0_.create_user_id as create_u3_1_, userinfo0_.last_modified_time as last_mod4_1_, userinfo0_.last_modified_user_id as last_mod5_1_, userinfo0_.version as version6_1_, userinfo0_.ages as ages7_1_, userinfo0_.email_address as email_ad8_1_, userinfo0_.last_name as last_nam9_1_, userinfo0_.name as name10_1_, userinfo0_.telephone as telepho11_1_from user_info userinfo0_ org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_1_, addresslis0_.id as id1_0_1_, addresslis0_.id as id1_0_0_, addresslis0_.create_time as create_t2_0_0_, addresslis0_.create_user_id as create_u3_0_0_, addresslis0_.last_modified_time as last_mod4_0_0_, addresslis0_.last_modified_user_id as last_mod5_0_0_, addresslis0_.version as version6_0_0_, addresslis0_.city as city7_0_0_, addresslis0_.user_info_id as user_inf8_0_0_from address addresslis0_where addresslis0_.user_info_id in (?, ?, ?) 可以看到 SQL 直接减少到两条了，其中查询 Address 的地方查询条件变成了 in(?,?,?)。 想象一下，如果有 20 条 UserInfo 信息，那么产生的 SQL 也是两条，此时要比 20+1 条 SQL 性能高太多了。 接着再执行另一个方法，看一下 @ManyToOne 的情况，代码如下所示。 1addressRepository.findAll() 关于执行的 SQL 情况如下所示。 12345678910111213141516171819202122232425262728293031323334352020-11-29 23:11:27.381 DEBUG 30870 --- [nio-8087-exec-5] org.hibernate.SQL : select address0_.id as id1_0_, address0_.create_time as create_t2_0_, address0_.create_user_id as create_u3_0_, address0_.last_modified_time as last_mod4_0_, address0_.last_modified_user_id as last_mod5_0_, address0_.version as version6_0_, address0_.city as city7_0_, address0_.user_info_id as user_inf8_0_from address address0_2020-11-29 23:11:27.383 DEBUG 30870 --- [nio-8087-exec-5] org.hibernate.SQL : select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id in (?, ?, ?) 从代码中可以看到，查询所有的 Address 信息也只产生了 2 条 SQL；而当我们查询 UserInfo 的时候，SQL 最后的查询条件也变成了 in (？，？，？)，同样的道理这样也会提升不少性能。 而 hibernate.default_batch_fetch_size 的经验参考值，可以设置成 20、30、50、100 等，太高了也没有意义。一个请求执行一次，产生的 SQL 数量为 3-5 条基本上都算合理情况，这样通过设置 default_batch_fetch_size 就可以很好地避免大部分业务场景下的 N+1 条 SQL 的性能问题了。 此时还需要注意一点就是，在实际工作中，一定要知道一次操作会产生多少 SQL，有没有预期之外的 SQL 参数，这是需要关注的重点，这种情况可以利用如下配置来开启打印 SQL，请看代码。 12## 显示sql的执行日志，如果开了这个,show_sql就可以不用了，show_sql没有上下文，多线程情况下，分不清楚是谁打印的，所有我推荐如下配置项：logging.level.org.hibernate.SQL=debug 但是这种配置也有个缺陷，就是只能全局配置，没办法针对不通过的实体管理关系配置不同的 Fetch Size 的值。 而与之类似的 Hibernate 里面也提供了一个注解 @BatchSize 可以解决此问题。 @BatchSize 注解@BatchSize 注解是 Hibernate 提供的用来解决查询关联关系的批量处理大小，默认无，可以配置在实体上，也可以配置在关联关系上面。此注解里面只有一个属性 size，用来指定关联关系 LAZY 或者是 EAGER 一次性取数据的大小。 还是将上面的例子中的 UserInfo 实体做一下改造，在里面增加两次 @BatchSize 注解，代码如下所示。 123456789101112131415@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@Table@ToString(exclude = "addressList")@BatchSize(size = 2)//实体类上加@BatchSize注解，用来设置当被关联关系的时候一次查询的大小，设置成2，方便演示Address关联UserInfo的时候的效果public class UserInfo extends BaseEntity &#123; private String name; private String telephone; @OneToMany(mappedBy = "userInfo",cascade = CascadeType.PERSIST,fetch = FetchType.EAGER) @BatchSize(size = 20)//关联关系的属性上加@BatchSize注解，用来设置当通过UserInfo加载Address的时候一次取数据的大小 private List&lt;Address&gt; addressList;&#125; 通过改造 UserInfo 实体，可以直接演示 @BatchSize 应用在实体类和属性字段上的效果，所以 Address 实体可以不做任何改变，hibernate.default_batch_fetch_size 还改成默认值 -1，再分别执行一下两个 findAll 方法，看一下效果。 第一种：查询所有 UserInfo，代码如下面这行所示。 1userInfoRepository.findAll() 看一下控制台 SQL 。 12345678910111213141516171819202122232425 org.hibernate.SQL :select userinfo0_.id as id1_1_, userinfo0_.create_time as create_t2_1_, userinfo0_.create_user_id as create_u3_1_, userinfo0_.last_modified_time as last_mod4_1_, userinfo0_.last_modified_user_id as last_mod5_1_, userinfo0_.version as version6_1_, userinfo0_.ages as ages7_1_, userinfo0_.email_address as email_ad8_1_, userinfo0_.last_name as last_nam9_1_, userinfo0_.name as name10_1_, userinfo0_.telephone as telepho11_1_from user_info userinfo0_ org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_1_, addresslis0_.id as id1_0_1_, addresslis0_.id as id1_0_0_, addresslis0_.create_time as create_t2_0_0_, addresslis0_.create_user_id as create_u3_0_0_, addresslis0_.last_modified_time as last_mod4_0_0_, addresslis0_.last_modified_user_id as last_mod5_0_0_, addresslis0_.version as version6_0_0_, addresslis0_.city as city7_0_0_, addresslis0_.user_info_id as user_inf8_0_0_from address addresslis0_where addresslis0_.user_info_id in (?, ?, ?) 和刚才设置 hibernate.default_batch_fetch_size=20 的效果一模一样，所以可以利用 @BatchSize 这个注解针对不同的关联关系，配置不同的大小，从而提升 N+1 SQL 的性能。 第二种：查询一下所有 Address，如下面这行代码所示。 1addressRepository.findAll(); 控制台的 SQL 情况，如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960org.hibernate.SQL :select address0_.id as id1_0_, address0_.create_time as create_t2_0_, address0_.create_user_id as create_u3_0_, address0_.last_modified_time as last_mod4_0_, address0_.last_modified_user_id as last_mod5_0_, address0_.version as version6_0_, address0_.city as city7_0_, address0_.user_info_id as user_inf8_0_from address address0_ org.hibernate.SQL :select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id in (?, ?) org.hibernate.SQL :select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id = ? 这里可以看到，由于在 UserInfo 的实体上设置了 @BatchSize(size = 2)，表示所有关联关系到 UserInfo 的时候一次取两条数据，所以就会发现这次查询 Address 加载 UserInfo 的时候，产生了 3 条 SQL。 其中通过关联关系查询 UserInfo 产生了 2 条 SQL，由于我们 UserInfo 在数据库里面有三条数据，所以第一条 UserInfo 的 SQL 受 @BatchSize(size = 2) 控制，从而 in (?,?) 只支持了两个参数，同时也产生了第二条查 UserInfo 的 SQL。 从上面的例子中可以看到 @BatchSize 和 hibernate.default_batch_fetch_size 的效果是一样的，只不过一个是全局配置、一个是局部设置，这是可以减少 N+1 SQL 最直接、最方便的两种方式。 注意事项： @BatchSize 的使用具有局限性，不能作用于 @ManyToOne 和 @OneToOne 的关联关系上，那样代码是不起作用的 123456public class Address extends BaseEntity &#123; private String city; @ManyToOne(cascade = CascadeType.PERSIST,fetch = FetchType.EAGER) @BatchSize(size = 30) //由于是@ManyToOne的关联关系所有没有作用 private UserInfo userInfo;&#125; @BatchSize 只能作用在 @ManyToMany、@OneToMany、实体类这三个地方。 此外，Hibernate 中还提供了一种 FetchMode 的策略，包含三种模式，分别为 FetchMode.SELECT、FetchMode.JOIN，以及 FetchMode.SUBSELECT。 Hibernate 中 @Fetch 数据的策略Hibernate 提供了一个 @Fetch 注解，用来改变获取数据的策略。代码如下所示。 12345678910111213141516// fetch注解只能用在方法和字段上面@Target(&#123;ElementType.METHOD, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface Fetch &#123; //注解里面，只有一个属性获取数据的模式 FetchMode value();&#125;//其中FetchMode的值有如下几种：public enum FetchMode &#123; //默认模式，就是会有N+1 sql的问题； SELECT, //通过join的模式，用一个sql把主体数据和关联关系数据一口气查出来 JOIN, //通过子查询的模式，查询关联关系的数据 SUBSELECT&#125; 需要注意的是，不要把这个注解和 JPA 协议里面的 FetchType.EAGER、FetchType.LAZY 搞混了，JPA 协议的关联关系中的 FetchType 解决的是取关联关系数据时机的问题，也就是说 EAGER 代表的是立即获得关联关系的数据，LAZY 是需要的时候再获得关联关系的数据。 这和 Hibernate 的 FetchMode 是两回事，FetchMode 解决的是获得数据策略的问题，也就是说，获得关联关系数据的策略有三种模式：SELECT（默认）、JOIN、SUBSELECT。 FetchMode.SELECT更改一下 UserInfo 实体，将 @Fetch(value = FetchMode.SELECT) 作为获取数据的策略，使用 FetchType.EAGER 作为获取数据的时机，代码如下所示。 1234567891011121314@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@Table@ToString(exclude = "addressList")public class UserInfo extends BaseEntity &#123; private String name; private String telephone; @OneToMany(mappedBy = "userInfo",cascade = CascadeType.PERSIST,fetch = FetchType.EAGER) @Fetch(value = FetchMode.SELECT) private List&lt;Address&gt; addressList;&#125; 然后还是执行 userInfoRepository.findAll(); 这个方法，看一下打印的 SQL 有哪些。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152org.hibernate.SQL :select userinfo0_.id as id1_1_, userinfo0_.create_time as create_t2_1_, userinfo0_.create_user_id as create_u3_1_, userinfo0_.last_modified_time as last_mod4_1_, userinfo0_.last_modified_user_id as last_mod5_1_, userinfo0_.version as version6_1_, userinfo0_.ages as ages7_1_, userinfo0_.email_address as email_ad8_1_, userinfo0_.last_name as last_nam9_1_, userinfo0_.name as name10_1_, userinfo0_.telephone as telepho11_1_from user_info userinfo0_ org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_0_, addresslis0_.id as id1_0_0_, addresslis0_.id as id1_0_1_, addresslis0_.create_time as create_t2_0_1_, addresslis0_.create_user_id as create_u3_0_1_, addresslis0_.last_modified_time as last_mod4_0_1_, addresslis0_.last_modified_user_id as last_mod5_0_1_, addresslis0_.version as version6_0_1_, addresslis0_.city as city7_0_1_, addresslis0_.user_info_id as user_inf8_0_1_from address addresslis0_where addresslis0_.user_info_id = ? org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_0_, addresslis0_.id as id1_0_0_, addresslis0_.id as id1_0_1_, addresslis0_.create_time as create_t2_0_1_, addresslis0_.create_user_id as create_u3_0_1_, addresslis0_.last_modified_time as last_mod4_0_1_, addresslis0_.last_modified_user_id as last_mod5_0_1_, addresslis0_.version as version6_0_1_, addresslis0_.city as city7_0_1_, addresslis0_.user_info_id as user_inf8_0_1_from address addresslis0_where addresslis0_.user_info_id = ? org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_0_, addresslis0_.id as id1_0_0_, addresslis0_.id as id1_0_1_, addresslis0_.create_time as create_t2_0_1_, addresslis0_.create_user_id as create_u3_0_1_, addresslis0_.last_modified_time as last_mod4_0_1_, addresslis0_.last_modified_user_id as last_mod5_0_1_, addresslis0_.version as version6_0_1_, addresslis0_.city as city7_0_1_, addresslis0_.user_info_id as user_inf8_0_1_from address addresslis0_where addresslis0_.user_info_id = ? 从上述 SQL 中可以看出，这依然是 N+1 的 SQL 问题，FetchMode.Select 是默认策略，加与不加是同样的效果，代表获取关系的时候新开一个 SQL 进行查询。 FetchMode.JOINFetchMode.JOIN 的意思是主表信息和关联关系通过一个 SQL JOIN 的方式查出来，看一下例子。 首先，将 UserInfo 里面的 FetchMode 改成 JOIN 模式，关键代码如下。 1234567public class UserInfo extends BaseEntity &#123; private String name; private String telephone; @OneToMany(mappedBy = "userInfo",cascade = CascadeType.PERSIST,fetch = FetchType.EAGER) @Fetch(value = FetchMode.JOIN) //唯一变化的地方采用JOIN模式 private List&lt;Address&gt; addressList;&#125; 然后，调用一下 userInfoRepository.findAll(); 这个方法，发现依然是这三条 SQL，如下图所示。 这是因为 FetchMode.JOIN 只支持通过 ID 或者联合唯一键获取数据才有效，这正是 JOIN 策略模式的局限性所在。 那么再调用一下 userInfoRepository.findById(id)，看看控制台的 SQL 执行情况，代码如下。 123456789101112131415161718192021222324select userinfo0_.id as id1_1_0_, userinfo0_.create_time as create_t2_1_0_, userinfo0_.create_user_id as create_u3_1_0_, userinfo0_.last_modified_time as last_mod4_1_0_, userinfo0_.last_modified_user_id as last_mod5_1_0_, userinfo0_.version as version6_1_0_, userinfo0_.ages as ages7_1_0_, userinfo0_.email_address as email_ad8_1_0_, userinfo0_.last_name as last_nam9_1_0_, userinfo0_.name as name10_1_0_, userinfo0_.telephone as telepho11_1_0_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.id as id1_0_1_, addresslis1_.id as id1_0_2_, addresslis1_.create_time as create_t2_0_2_, addresslis1_.create_user_id as create_u3_0_2_, addresslis1_.last_modified_time as last_mod4_0_2_, addresslis1_.last_modified_user_id as last_mod5_0_2_, addresslis1_.version as version6_0_2_, addresslis1_.city as city7_0_2_, addresslis1_.user_info_id as user_inf8_0_2_from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_idwhere userinfo0_.id = ? 这时会发现，当查询 UserInfo 的时候，它会通过 left outer join 把 Address 的信息也查询出来，虽然 SQL 上会有冗余信息，但是你会发现我们之前的 N+1 的 SQL 直接变成 1 条 SQL 了。 此时修改 UserInfo 里面的 @OneToMany，这个 @Fetch(value = FetchMode.JOIN) 同样适用于 @ManyToOne；然后再改一下 Address 实例，用 @Fetch(value = FetchMode.JOIN) 把 Address 里面的 UserInfo 关联关系改成 JOIN 模式；接着用 LAZY 获取数据的时机，会发现其对获取数据的策略没有任何影响。 这里只是给你演示获取数据时机的不同情况，关键代码如下。 1234567891011121314@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "userInfo")public class Address extends BaseEntity &#123; private String city; @ManyToOne(cascade = CascadeType.PERSIST,fetch = FetchType.LAZY) @JsonBackReference @Fetch(value = FetchMode.JOIN) private UserInfo userInfo;&#125; 同样的道理，JOIN 对列表性的查询是没有效果的，我们调用一下 addressRepository.findById(id)，产生的 SQL 如下所示。 1234567891011121314151617181920212223org.hibernate.SQL : select address0_.id as id1_0_0_, address0_.create_time as create_t2_0_0_, address0_.create_user_id as create_u3_0_0_, address0_.last_modified_time as last_mod4_0_0_, address0_.last_modified_user_id as last_mod5_0_0_, address0_.version as version6_0_0_, address0_.city as city7_0_0_, address0_.user_info_id as user_inf8_0_0_, userinfo1_.id as id1_1_1_, userinfo1_.create_time as create_t2_1_1_, userinfo1_.create_user_id as create_u3_1_1_, userinfo1_.last_modified_time as last_mod4_1_1_, userinfo1_.last_modified_user_id as last_mod5_1_1_, userinfo1_.version as version6_1_1_, userinfo1_.ages as ages7_1_1_, userinfo1_.email_address as email_ad8_1_1_, userinfo1_.last_name as last_nam9_1_1_, userinfo1_.name as name10_1_1_, userinfo1_.telephone as telepho11_1_1_from address address0_ left outer join user_info userinfo1_ on address0_.user_info_id = userinfo1_.idwhere address0_.id = ? 发现此时只会产生一个 SQL，即通过 from address left outer join user_info 一次性把所有信息都查出来，然后 Hibernate 再根据查询出来的结果组合到不同的实体里面。 也就是说 FetchMode.JOIN 对于关联关系的查询 LAZY 是不起作用的，因为 JOIN 的模式是通过一条 SQL 查出来所有信息，所以 FetchMode.JOIN 会忽略 FetchType。 FetchMode.SUBSELECT这种模式很简单，就是将关联关系通过子查询的形式查询出来，结合例子来理解一下。 首先，将 UserInfo 里面的关联关系改成 @Fetch(value = FetchMode.SUBSELECT)，关键代码如下。 12345public class UserInfo extends BaseEntity &#123; @OneToMany(mappedBy = "userInfo",cascade = CascadeType.PERSIST,fetch = FetchType.LAZY) //这里测试一下LAZY情况 @Fetch(value = FetchMode.SUBSELECT) //唯一变化之处 private List&lt;Address&gt; addressList;&#125; 接着，像上面的做法一样，执行一下 userInfoRepository.findAll()；方法，看一下控制台的 SQL 情况，如下所示。 1234567891011121314151617181920212223242526org.hibernate.SQL :select userinfo0_.id as id1_1_, userinfo0_.create_time as create_t2_1_, userinfo0_.create_user_id as create_u3_1_, userinfo0_.last_modified_time as last_mod4_1_, userinfo0_.last_modified_user_id as last_mod5_1_, userinfo0_.version as version6_1_, userinfo0_.ages as ages7_1_, userinfo0_.email_address as email_ad8_1_, userinfo0_.last_name as last_nam9_1_, userinfo0_.name as name10_1_, userinfo0_.telephone as telepho11_1_from user_info userinfo0_ org.hibernate.SQL :select addresslis0_.user_info_id as user_inf8_0_1_, addresslis0_.id as id1_0_1_, addresslis0_.id as id1_0_0_, addresslis0_.create_time as create_t2_0_0_, addresslis0_.create_user_id as create_u3_0_0_, addresslis0_.last_modified_time as last_mod4_0_0_, addresslis0_.last_modified_user_id as last_mod5_0_0_, addresslis0_.version as version6_0_0_, addresslis0_.city as city7_0_0_, addresslis0_.user_info_id as user_inf8_0_0_from address addresslis0_where addresslis0_.user_info_id in (select userinfo0_.id from user_info userinfo0_) 这个时候会发现，查询 Address 信息是直接通过 addresslis0_.user_info_id in (select userinfo0_.id from user_info userinfo0_) 子查询的方式进行的，也就是说 N+1 SQL 变成了 1+1 的 SQL，这有点类似我们配置 @BatchSize 的效果。 FetchMode.SUBSELECT 支持 ID 查询和各种条件查询，唯一的缺点是只能配置在 @OneToMany 和 @ManyToMany 的关联关系上，不能配置在 @ManyToOne 和 @OneToOne 的关联关系上，所以我们在 Address 里面关联 UserInfo 的时候就没有办法做实验了。 总之，@Fetch 的不同模型，都有各自的优缺点： FetchMode.SELECT 默认，和不配置的效果一样； FetchMode.JOIN 只支持类似 findById(id) 的方法，只能根据 ID 查询才有效果； FetchMode.SUBSELECT 虽然不限使用方式，但是只支持 **ToMany 的关联关系。 所以在使用 @Fetch 的时候需要注意一下它的局限性，个人是比较推荐 @BatchSize 的方式。 那么除了上面的处理方式，也可以采用之前写 Mybatis 的思路来查询关联关系，下面来看一下该如何转变思路。 转化解决问题的思路这时需要在思想上进行转变，利用 JPA 的优势，摒弃它的缺陷。想想没有用 JPA 的时候是怎么做的？难道一定要用实体之间的关联关系吗？如果用的是 Mybatis，你在给前端返回关联关系数据的时候一般怎么写呢？ 答案肯定是写成 1+1 SQL 的形式，也就是一条主 SQL、一条查关联关系的 SQL。还用 UserInfo 和 Address 实体来演示，代码如下。 12345678910111213141516171819202122232425@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@Tablepublic class UserInfo extends BaseEntity &#123; private String name; private String telephone; @Transient //在UserInfo实体中，不利用JPA来关联实体的关联关系了，而是把它设置成@Transisent，只维护java对象的关系，不维护DB之间的关联关系 private List&lt;Address&gt; addressList;&#125;@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "userInfo")public class Address extends BaseEntity &#123; private String city; private String userId; @Transient //同样Address里面也可以不维护UserInfo的关联关系 private UserInfo userInfo;&#125; 当查询所有 UserInfo 信息的时候，又想把每个 UserInfo 的 Address 信息都带上，应该怎么做呢？请看如下代码。 123456789101112131415161718/** * 自己实现一套 Batch fetch的逻辑 */@Transactionalpublic List&lt;UserInfo&gt; getAllUserWithAddress() &#123; //先查出来所有的UserInfo信息 List&lt;UserInfo&gt; userInfos = userInfoRepository.findAll(); //再查出来上面userInfos里面的所有userId列表，再查询出来上面的查询结果所对应的所有Address信息 List&lt;Address&gt; addresses = addressRepository.findByUserIdIn(userInfos.stream().map(userInfo -&gt; userInfo.getId()).collect(Collectors.toList())); //我们自己再写一个转化逻辑，把各自user info的address信息放置到响应的UserInfo实例里面； Map&lt;Long,List&lt;Address&gt;&gt; addressMaps = addresses .stream() .collect(Collectors.groupingBy(Address::getUserId));//里面Map结构方便获取 return userInfos.stream().map(userInfo -&gt; &#123; userInfo.setAddressList(addressMaps.get(userInfo.getId())); return userInfo; &#125;).collect(Collectors.toList());&#125; 你会发现，这要比原来的方式稍微复杂一点，但是如果做框架的话，上面有些逻辑可以抽到一个 Util 类里面去。 不过需要注意的是，实际工作中肯定不是 findAll()，而是会根据一些业务逻辑查询一个 UserInfo 的 List 信息，然后再根据查询出来的 userInfo 的 ID 列表去二次查询 Address 信息，这样最多只需要 2 个 SQL 就可完成实际业务逻辑。 那么反向思考，通过 Address 对象查询 UserInfo 也是一样的道理，可以先查询出 List，再查询出 List里面包含的所有 UserInfoId 列表，然后再去查询 UserInfo 信息，通过 Map 组装到 Address 里面。 Tips：实体里面如果关联关系有非常多的请求，想维护关联关系是一件非常难的事情。我们可以利用 Mybatis 的思想、JPA 的快捷查询语法，来组装想要的任何关联关系的对象。这样的代码虽然比起原生的 JPA 语法较复杂，但是比起 Mybatis 还是要简单很多，理解起来也更容易，问题反倒会更少一点。 @EntityGraph 使用详解JPA 协议也提供了另外一种解题思路：利用 @EntityGraph 注解来解决 众所周知，实体与实体之间的关联关系错综复杂，就像一个大网图一样，网状分布交叉引用。而 JPA 协议在 2.1 版本之后企图用 Entity Graph 的方式，描绘出一个实体与实体之间的关联关系。 普通做法为，通过 @ManyToOne/@OneToMany/@ManyToMany/@OneToOne 这些关联关系注解表示它们之间的关系时，只能配置 EAGER 或者 LAZY，没办法根据不同的配置、不同的关联关系加载时机。 而 JPA 协议企图通过 @NamedEntityGraph 注解来描述实体之间的关联关系，当被 @EntityGraph 使用的时候进行 EAGER 加载，以减少 N+1 的 SQL，我们来看一下具体用法。 @NamedEntityGraph 和 @EntityGraph 用法还是直接通过一个例子来说明，请看下面的代码。 123456789101112131415161718//可以被@NamedEntityGraphs注解重复使用，只能配置在类上面，用来声明不同的EntityGraph；@Repeatable(NamedEntityGraphs.class)@Target(&#123;TYPE&#125;)@Retention(RUNTIME)public @interface NamedEntityGraph &#123; //指定一个名字 String name() default ""; //哪些关联关系属性可以被EntityGraph包含进去，默认一个没有。可以配置多个 NamedAttributeNode[] attributeNodes() default &#123;&#125;; //是否所有的关联关系属性自动包含在内，默认false; boolean includeAllAttributes() default false; //配置subgraphs，子实体图(可以理解为关联关系实体图，即如果算层级，可以配置第二层级)，可以被NamedAttributeNode引用 NamedSubgraph[] subgraphs() default &#123;&#125;; //配置subclassSubgraphs的namedSubgraph有哪些。即如果算层级，可以配置第三层级 NamedSubgraph[] subclassSubgraphs() default &#123;&#125;;&#125; 上述代码中，可以看到 @NamedEntityGraphs 能够配置多个 @NamedEntityGraph。接着往下看。 123456//只能使用在实体类上面@Target(&#123;TYPE&#125;)@Retention(RUNTIME)public @interface NamedEntityGraphs&#123; NamedEntityGraph[] value();//可以同时指定多个NamedEntityGraph&#125; 上面这段代码中，NamedSubgraph 用来指定关联关系的策略，也就关联关系有两层。 再看一下 @NamedEntityGraph 里面的 NamedAttributeNode 属性有哪些值，代码如下。 1234567891011// 用来进行属性节点的描述@Target(&#123;&#125;)@Retention(RUNTIME)public @interface NamedAttributeNode &#123; //要包含的关联关系的属性的名字，必填 String value(); //如果我们在@NamedEntityGraph里面配置了子关联关系，这个是配置subgraph的名字 String subgraph() default ""; //当关联关系是被Map结构引用的时候，可以指定key的方式，一般很少用 String keySubgraph() default "";&#125; 上面就是对 @NamedAttributeNode 的介绍，再看一下 @EntityGraph 里面的 @NamedSubgraph 的结构，代码如下。 12345678910@Target(&#123;&#125;)@Retention(RUNTIME)public @interface NamedSubgraph &#123; //指定一个名字 String name(); //子关联关系的类的class Class type() default void.class; //二层关联关系的要包含的关联关系的属性的名字 NamedAttributeNode[] attributeNodes();&#125; 其中，@NamedEntityGraph 的注解都是配置在实体本身上面的，而 @EntityGraph 是用在 ***Repository 接口里的方法中的。 了解一下 @EntityGraph 注解的语法，如下所示。 12345678910111213141516171819202122232425@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.METHOD, ElementType.ANNOTATION_TYPE &#125;)//EntityGraph 作用在Repository的接口里面的方法上面public @interface EntityGraph &#123; //指@EntityGraph注解引用的@NamedEntityGraph里面定义的name，如果是空EntityGraph就不会起作用，如果为空相当于没有配置； String value() default ""; //EntityGraph的类型，默认是EntityGraphType.FETCH类型 EntityGraphType type() default EntityGraphType.FETCH; //可以指定attributePaths用来覆盖@NamedEntityGraph里面的attributeNodes的配置，默认配置是空，以@NamedEntityGraph里面的为准； String[] attributePaths() default &#123;&#125;; //JPA 2.1支持的EntityGraphType对应的枚举值 public enum EntityGraphType &#123; //LOAD模式，当被指定了这种模式、被@EntityGraph管理的attributes的时候，原来的FetchType的类型直接忽略变成Eager模式，而不被@EntityGraph管理的attributes还是保持默认的FetchType LOAD("javax.persistence.loadgraph"), //FETCH模式，当被指定了这种模式、被@EntityGraph管理的attributes的时候，原来的FetchType的类型直接忽略变成Eager模式，而不被@EntityGraph管理的attributes将会变成Lazy模式，和LOAD的区别就是对不被@NamedEntityGraph配置的关联关系的属性的FetchType不一样； FETCH("javax.persistence.fetchgraph"); private final String key; private EntityGraphType(String value) &#123; this.key = value; &#125; public String getKey() &#123; return key; &#125; &#125;&#125; @EntityGraph 使用实例通过改造 Address 和 UserInfo 实体，来分别测试一下 @NamedEntityGraph 和 @EntityGraph 的用法。 第一步：在实体里面配置 @EntityGraph，关键代码如下。 1234567891011121314151617181920212223242526272829303132333435@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "userInfo")//这里直接使用@NamedEntityGraph，因为只需要配置一个@NamedEntityGraph，指定一个名字getAllUserInfo，指定被这个名字的实体试图关联的关联关系属性是userInfo@NamedEntityGraph(name = "getAllUserInfo",attributeNodes = @NamedAttributeNode(value = "userInfo"))public class Address extends BaseEntity &#123; private String city; @JsonBackReference //防止JSON死循环 @ManyToOne(cascade = CascadeType.PERSIST,fetch = FetchType.LAZY)//采用默认的lazy模式 private UserInfo userInfo;&#125;@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@Table@ToString(exclude = "addressList")//UserInfo对应的关联关系，利用@NamedEntityGraphs配置了两个，一个是针对Address的关联关系，一个是name叫rooms的实体图包含了rooms属性；在UserInfo里面增加了两个关联关系；@NamedEntityGraphs(value = &#123;@NamedEntityGraph(name = "addressGraph",attributeNodes = @NamedAttributeNode(value = "addressList")),@NamedEntityGraph(name = "rooms",attributeNodes = @NamedAttributeNode(value = "rooms"))&#125;)public class UserInfo extends BaseEntity &#123; private String name; private String telephone; private Integer ages; //默认LAZY模式 @OneToMany(mappedBy = "userInfo",cascade = CascadeType.PERSIST,fetch = FetchType.LAZY) private List&lt;Address&gt; addressList; //默认EAGER模式 @OneToMany(cascade = CascadeType.PERSIST,fetch = FetchType.EAGER) private List&lt;Room&gt; rooms;&#125; 第二步：在我们需要的 *Repository 的方法上面直接使用 @EntityGraph，关键代码如下。 12345678//因为要用findAll()做测试，所以可以覆盖JpaRepository里面的findAll()方法，加上@EntityGraph注解public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt;&#123; @Override //指定EntityGraph引用的是，在UserInfo实例里面配置的name=addressGraph的NamedEntityGraph； // 这里采用的是LOAD的类型，也就是说被addressGraph配置的实体图属性address采用的fetch会变成 FetchType.EAGER模式，而没有被addressGraph实体图配置关联关系属性room还是采用默认的EAGER模式@EntityGraph(value = "addressGraph",type = EntityGraph.EntityGraphType.LOAD) List&lt;UserInfo&gt; findAll();&#125; 同样的道理，其对于 AddressRepository 也是适用的，代码如下。 123456public interface AddressRepository extends JpaRepository&lt;Address, Long&gt;&#123;@Override //可以覆盖原始方法，添加上不同的@EntityGraph策略//使用@EntityGraph查询所有Address的时候，指定name = "getAllUserInfo"的@NamedEntityGraph，采用默认的EntityGraphType.FETCH，如果Address里面有多个关联关系的时候，只有在name = "getAllUserInfo"的实体图配置的userInfo属性上采用Eager模式，其他关联关系属性没有指定，默认采用LAZY模式；@EntityGraph(value = "getAllUserInfo")List&lt;Address&gt; findAll();&#125; 第三步：看一下上面的两个方法执行的 SQL。 再次执行 userInfoRepository.findAll(); 这个方法的时候会发现，被配置 EntityGraph 的 Address 和 user_info 通过 left join 一条 SQL 就把所有的信息都查出来了，SQL 如下所示。 123456789101112131415161718192021222324org.hibernate.SQL :select userinfo0_.id as id1_2_0_, addresslis1_.id as id1_0_1_, userinfo0_.create_time as create_t2_2_0_, userinfo0_.create_user_id as create_u3_2_0_, userinfo0_.last_modified_time as last_mod4_2_0_, userinfo0_.last_modified_user_id as last_mod5_2_0_, userinfo0_.version as version6_2_0_, userinfo0_.ages as ages7_2_0_, userinfo0_.email_address as email_ad8_2_0_, userinfo0_.last_name as last_nam9_2_0_, userinfo0_.name as name10_2_0_, userinfo0_.telephone as telepho11_2_0_, addresslis1_.create_time as create_t2_0_1_, addresslis1_.create_user_id as create_u3_0_1_, addresslis1_.last_modified_time as last_mod4_0_1_, addresslis1_.last_modified_user_id as last_mod5_0_1_, addresslis1_.version as version6_0_1_, addresslis1_.city as city7_0_1_, addresslis1_.user_info_id as user_inf8_0_1_, addresslis1_.user_info_id as user_inf8_0_0__, addresslis1_.id as id1_0_0__from user_info userinfo0_ left outer join address addresslis1_ on userinfo0_.id = addresslis1_.user_info_id 而没有配置 rooms 这个关联关系的属性时，rooms 的查询还是会触发 N+1 的 SQL。 从中可以看到 @EntityGraph 的效果有点类似 Hibernate 里面提供的 FetchModel.JOIN 的模式，但不同的是 @EntityGraph 可以搭配任何的查询情况，只需要在查询方法上直接加 @EntityGraph 注解即可。 这种方法还有个优势就是 @EntityGraph 和 @NamedEntityGraph 是 JPA 协议规定的，这样可以对 Hibernate 无感。 那么再看一下 @ManyToOne 的模式是否同样奏效，访问 addressRepository.findAll() 这个方法看一下 SQL，如下所示。 12345678910111213141516171819202122org.hibernate.SQL :select address0_.id as id1_0_0_, userinfo1_.id as id1_2_1_, address0_.create_time as create_t2_0_0_, address0_.create_user_id as create_u3_0_0_, address0_.last_modified_time as last_mod4_0_0_, address0_.last_modified_user_id as last_mod5_0_0_, address0_.version as version6_0_0_, address0_.city as city7_0_0_, address0_.user_info_id as user_inf8_0_0_, userinfo1_.create_time as create_t2_2_1_, userinfo1_.create_user_id as create_u3_2_1_, userinfo1_.last_modified_time as last_mod4_2_1_, userinfo1_.last_modified_user_id as last_mod5_2_1_, userinfo1_.version as version6_2_1_, userinfo1_.ages as ages7_2_1_, userinfo1_.email_address as email_ad8_2_1_, userinfo1_.last_name as last_nam9_2_1_, userinfo1_.name as name10_2_1_, userinfo1_.telephone as telepho11_2_1_from address address0_ left outer join user_info userinfo1_ on address0_.user_info_id = userinfo1_.id 可以看到 address left join 的模式中，一个 SQL 把所有的 address 和 user_info 都查询出来了。 综上所述，@EntityGraph 可以用在任何 ***Repository 的查询方法上，针对不同的场景配置不同的关联关系策略，就可以减少 N+1 的 SQL，成为一条 SQL。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa Lazy Exception 解决方法]]></title>
    <url>%2F2021%2F01%2F12%2FSpringDataJpa%E7%9A%84LazyException%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa Lazy Exception 解决方法什么是 LazyInitializationException 异常通过 4 个步骤看一下什么是 Lazy 异常 第一步：为了方便测试，把 spring.jpa.open-in-view 设置成 false，代码如下： 1spring.jpa.open-in-view=false 第二步：新建一个一对多的关联实体：UserInfo 用户信息，一个用户有多个地址 Address。代码如下： 1234567891011121314151617181920212223242526272829@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@Tablepublic class UserInfo extends BaseEntity &#123; private String name; private Integer ages; private String lastName; private String emailAddress; private String telephone; //假设一个用户有多个地址，取数据的方式用 lazy 的模式(默认也是 lazy 的)；采用 CascadeType.PERSIST 方便插入演示数据； @OneToMany(mappedBy = "userInfo", cascade = CascadeType.PERSIST, fetch = FetchType.LAZY) private List&lt;Address&gt; addressList;&#125;@Entity@Table@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructorpublic class Address extends BaseEntity &#123; private String city; //维护关联关系的一方，默认都是 lazy 模式 @ManyToOne private UserInfo userInfo;&#125; 第三步：再新建一个 Controller，取用户的基本信息，并且查看一下 Address 的地址信息，代码如下： 1234567@GetMapping("/user/info/&#123;id&#125;")public UserInfo getUserInfoFromPath(@PathVariable("id") Long id) &#123; UserInfo u1 = userInfoRepository.findById(id).get(); //触发 lazy 加载，取 userInfo 里面的地址信息 System.out.println(u1.getAddressList().get(0).getCity()); return u1;&#125; 第四步：启动项目，直接发起如下请求： 12345### get user info的接口GET /user/info/1 HTTP/1.1Host: 127.0.0.1:8087Content-Type: application/jsonCache-Control: no-cache 然后就可以如期得到 Lazy 异常，如下所示: 1234567org.hibernate.LazyInitializationException: failed to lazily initialize a collection of role: com.example.jpa.demo.db.UserInfo.addressList, could not initialize proxy - no Session at org.hibernate.collection.internal.AbstractPersistentCollection.throwLazyInitializationException(AbstractPersistentCollection.java:606) ~[hibernate-core-5.4.20.Final.jar:5.4.20.Final] at org.hibernate.collection.internal.AbstractPersistentCollection.withTemporarySessionIfNeeded(AbstractPersistentCollection.java:218) ~[hibernate-core-5.4.20.Final.jar:5.4.20.Final] at org.hibernate.collection.internal.AbstractPersistentCollection.initialize(AbstractPersistentCollection.java:585) ~[hibernate-core-5.4.20.Final.jar:5.4.20.Final] at org.hibernate.collection.internal.AbstractPersistentCollection.read(AbstractPersistentCollection.java:149) ~[hibernate-core-5.4.20.Final.jar:5.4.20.Final] at org.hibernate.collection.internal.PersistentBag.get(PersistentBag.java:561) ~[hibernate-core-5.4.20.Final.jar:5.4.20.Final] at com.example.jpa.demo.web.UserInfoController.getUserInfoFromPath(UserInfoController.java:29) ~[main/:na] 通过上面的异常信息基本可以看到， UserInfo 实体对象加载 Address 的时候，产生了 Lazy 异常，是因为 no session。那么发生异常的根本原因是什么呢？它的加载原理是什么样的？ Lazy 加载机制的原理分析JPA 里有 Lazy 的机制，所谓的 Lazy 就是指，当我们使用关联关系的时候，只有用到被关联关系的一方才会请求数据库去加载数据，也就是说关联关系的真实数据不是立马加载的，只有用到的时候才会加载。 而 Hibernate 的实现机制中提供了 PersistentCollection 的机制，利用代理机制改变了关联关系的集合类型，从而实现了懒加载机制。 PersistentCollection 集合类PersistentCollection 是一个集合类的接口，实现类包括如下几种，如下图所示。 也就是说 Hibernate 通过 PersistentCollection 的实现类 AbstractPersistentCollection 的所有子类，对 JDK 里面提供的 List、Map、SortMap、Array、Set、SortedSet 进行了扩展，从而实现了具有懒加载的特性。所以在 Hibernate 里面支持的关联关系的类型只有下面五种。 java.util.List java.util.Set java.util.Map java.util.SortedSet java.util.SortedMap 关于这几种类型，Hibernate 官方也提供了扩展 AbstractPersistentCollection 的方法。 PersistentBag 为例详解原理通过 PersistentBag 的关键源码，来看一下集合类 List 是怎么实现的，代码如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051//PersistentBag 继承 AbstractPersistentCollection，从而继承了 PersistenceCollection 的一些公共功能、session的持有、lazy的特性、entity的状态转化等功能；同时 PersistentBag 也实现了 java.util.List 的所有方法，即对 List 进行读写的时候包装 lazy 逻辑public class PersistentBag extends AbstractPersistentCollection implements List &#123; //PersistentBag构造方法，当初始化实体对象对集合初始化的时候，把当前的Session保持住 public PersistentBag(SessionImplementor session) &#123; this( (SharedSessionContractImplementor) session ); &#125; // 从这个方法可以看出来其对List和ArrayList的支持 @SuppressWarnings("unchecked") public PersistentBag(SharedSessionContractImplementor session, Collection coll) &#123; super( session ); providedCollection = coll; if ( coll instanceof List ) &#123; bag = (List) coll; &#125; else &#123; bag = new ArrayList( coll ); &#125; setInitialized(); setDirectlyAccessible( true ); &#125; //以下是一些关键的 List 的实现方法，基本上都是在原有的 List 功能的基础上增加调用父类 AbstractPersistentCollection 里面的 read() 和 write() 方法 @Override @SuppressWarnings("unchecked") public Object remove(int i) &#123; write(); return bag.remove( i ); &#125; @Override @SuppressWarnings("unchecked") public Object set(int i, Object o) &#123; write(); return bag.set( i, o ); &#125; @Override @SuppressWarnings("unchecked") public List subList(int start, int end) &#123; read(); return new ListProxy( bag.subList( start, end ) ); &#125; @Override public boolean entryExists(Object entry, int i) &#123; return entry != null; &#125; // toString被调用的时候会触发read() @Override public String toString() &#123; read(); return bag.toString(); &#125; .....//其他方法类似，就不一一举例了&#125; 那么再看一下 AbstractPersistentCollection 的关键实现，在 AbstractPersistentCollection 中会有大量通过 Session 来初始化关联关系的方法，这些方法基本是利用当前 Session 和当前 Session 中持有的 Connection 来重新操作 DB，从而取到数据库里面的数据。 所以 LazyInitializationExcetion 基本都是从这个类里面抛出来的，从源码里面可以看到其严重依赖当前的 Session，关键源码如下图所示。 所以在默认的情况下，如果把 Session 关闭了，想利用 Lazy 的机制加载管理关系，就会发生异常了。通过实例看一下，在上面例子的 Controller 上加一个 debug 断点，可以看到如下图显示的内容：我们的 Address 指向了 PersistentBag 代理实例类。 同时再设置断点的话也可以看到，PersistentBag 被初始化的时候，会传进来 Session 的上下文，即包含 Datasource 和需要执行 Lazy 的 sql。 而需要执行 Lazy 的 sql，通过 debug 的栈信息可以看到其中有个 instantiate，关键断点信息如下图所示： 再继续 debug 的话，也会看到调用 AbstractPersistentCollection 的初始化 Lazy 的方法，如下所示。 通过源码分析和实例讲解，已经基本上知道了 Lazy 的原理，也就是需要 Lazy 的关联关系会初始化成 PersistentCollection，并且依赖持有的 Session。而当操作 List、Map 等集合类的一些基本方法的时候会触发 read()，并利用当前的 Session 进行懒加载。 Lazy 异常的常见场景与解决方法场景一：跨事务，事务之外的场景 当 spring.jpa.open-in-view=false 的时候，每个事务就会独立持有 Session；那么在事务之外操作 lazy 的关联关系的时候，就容易发生 Lazy 异常。 正如上面列举的 Demo 一样，一开始就将 open-in-view 设置成了 false，而 userInfoRepository.findById(id) 又是一个独立事务，方法操作结束之后事务结束，事务结束之后 session close。所以再操作 UserInfo 中 Address 对象的时候，就发生了 Lazy 异常。 实际工作中这种情况比较多见，应该如何解决呢？ 第一种方式：简单粗暴地设置为 spring.jpa.open-in-view=true 通过上面的分析，可以知道无非就是 Session 的关闭导致了 Lazy 异常，所以简单粗暴的办法就是加大 Session 的生命周期，将 Session 的生命周期和请求的生命周期设置成一样。但是 open-in-view 可能会带来的副作用必须要牢记于心，有以下几点。 它对 Connection 的影响是什么？连接池有没有很好地监控？利用率是怎么样的？ 实体的状态在整个 Session 的生命周期之间的变更都是有效的，数据的更新是不是预期的，你要心里有数。 N+1 的 SQL 是不是我们期望的？（这个下一讲我会详细介绍）性能有没有影响？等等。 第二种方式：也是简单粗暴改成 Eager 模式 直接采用 Eager 的模式，这样也不会有 Lazy 异常的问题。如下述代码所示。 12345public class UserInfo extends BaseEntity &#123; private String name; //直接采用eager模式 @OneToMany(mappedBy = "userInfo",cascade = CascadeType.PERSIST,fetch = FetchType.EAGER) private List&lt;Address&gt; addressList; 但是这种做法不推荐使用，因为本来不想查 Address 信息，这样会白白地触发对 Address 的查询，导致性能有点浪费。 第三种方式：将可能发生 Lazy 的操作和取数据放在同一个事务里面 改造一下上面 Demo 的 Controller 的写法，代码如下所示： 123456789101112@RestController@Log4j2public class UserInfoController &#123; @Autowired private UserInfoService userInfoService; @GetMapping("/user/info/&#123;id&#125;") public UserInfo getUserInfoFromPath(@PathVariable("id") Long id) &#123; //controller 里面改调用 service 方法，这个 service 明确地返回了 UserInfo 和 Address 信息 UserInfo u1 = userInfoService.getUserInfoAndAddress(id); System.out.println(u1.getAddressList().get(0).getCity()); return u1; &#125; Service 的实现如下所示，在里面用事务包装，利用事务，让可能触发 Lazy 的操作提前在事务里面发生。 1234567891011/** * 把逻辑封装在service方法里面，方法名字语义要清晰，就是说这个方法会取UserInfo的信息和Address的信息 * @param id */@Override@Transactionalpublic UserInfo getUserInfoAndAddress(Long id) &#123; UserInfo u1 = userInfoRepository.findById(id).get(); u1.getAddressList().size();//在同一个事务里面触发lazy；不需要查询address的地方就不需要触发了 return u1;&#125; 这个时候就要求对方法名的语义和注释比较清晰了，这个方法还有个缺点，就是 Service 返回的依然还是 UserInfo 的实体，如果在关联关系多的情况下，依然有犯错的可能性发生。 第四种方式：Service 层之外都用 DTO 或者其他 POJO，而不是 Entity 这种是最复杂的，但却是最有效的、不会出问题的方式，在 Service 层返回 DTO，改造一下 Service 方法，代码如下所示。 123456@Transactionalpublic UserInfoDto getUserInfoAndAddress(Long id) &#123; UserInfo u1 = userInfoRepository.findById(id).get(); //按照业务要求，需要什么返回什么就可以了，让实体在service层之外是不可见的 return UserInfoDto.builder().name(u1.getName()).addressList(u1.getAddressList()).build();/&#125; 而 UserInfoDto 也就是根据业务需要创建不同的 DTO 即可。例如，只需要 name 和 address 的时候，代码如下。 123456@Data@Builderpublic class UserInfoDto &#123; private String name; private List&lt;Address&gt; addressList;&#125; 除了 DTO 还可以采用任何语义的 POJO，宗旨就是 Entity 对 Service 层之外是不可见的。也可以采用 Projection 的方式，返回接口类型的 POJO，这样控制的粒度更细，读写都可以分开。将 Entity 控制在 Service 层还有个好处就是，有的时候会使用各种 RPC 框架进行远程方法调度，可以直接调用 Service 方法通过 TCP 协议，例如 Dubbo，这样也就天然支持了。 场景二：异步线程的时候既然跨事务容易发生问题，那么异步线程的时候更容易发生 Lazy 异常 异步线程的时候，再套用上面的四种方式，会发现第一种方式就不适用了，因为异步开启的事务和 DB 操作默认是不受 open-in-view 控制的。所以可以明确地知道，开启的异步方法会用到实体参数的哪些关联关系，是否需要按照上面的第三种和第四种方式进行提前处理呢？这些都是需要心中有数的，而不是简单地把异步开启就完事了。 场景三：Controller 直接返回实体也会产生 Lazy Exception工作中经常为了省事，直接在 Contoller 里面返回 Entity，这个时候很容易发生 Lazy 异常，例如下面这个场景。 1234@GetMapping("/user/info/&#123;id&#125;")public UserInfo getUserInfoFromPath(@PathVariable("id") Long id) &#123; return userInfoRepository.findById(id).get();//controller 层直接将 UserInfo 返回给 view 层了；&#125; 直接将 UserInfo 实体对象当成 VO 对象，且直接当成返回结果了，当请求上面的 API 的时候也会发生 Lazy 异常，看下代码。 12o.hibernate.LazyInitializationException : failed to lazily initialize a collection of role: com.example.jpa.demo.db.UserInfo.addressList, could not initialize proxy - no Session org.hibernate.LazyInitializationException: failed to lazily initialize a collection of role: com.example.jpa.demo.db.UserInfo.addressList, could not initialize proxy - no Session 此 VO 发生的异常与其他 Lazy 异常不同的时候，仔细观察，会发现如下信息。 1 Resolved [org.springframework.http.converter.HttpMessageNotWritableException: Could not write JSON: failed to lazily initialize a collection of role: com.example.jpa.demo.db.UserInfo.addressList 通过日志可以知道，此时发生 Lazy 异常的主要原因是 JSON 系列化的时候会触发 Lazy 加载。这个时候就有了第五种解决 Lazy 异常的方式，就是利用 @JsonIgnoreProperties(“addressList”) 排除我们不想序列化的属性即可。 但是这种方式的弊端是用这个集合的只能全局配置，没办法有特例配置。因此最佳实践还是采用上面所说的第一种方式和第四种方式。 场景四：自定义的拦截器和 filter 中无意的 toString 操作第四个 Lazy 异常的场景就是打印一些日志，或者无意间触发 toString 的操作也会发生 Lazy 异常，这种处理方法也很简单（第六种处理 Lazy 异常的方式）：toString 里面排除掉不需要 Lazy 加载的关联关系即可。如果我们用 lombok 的话，直接 @ToString(exclude = “addressList”) 排除掉就好了，完整例子如下所示。 1234@ToString(exclude = "addressList")@JsonIgnoreProperties("addressList")public class UserInfo extends BaseEntity &#123;......&#125; 以上介绍的四个 Lazy 异常的场景和六种处理方式，在实际工作中可以灵活运用，其中最主要的是要知道背后的原理和触发 Lazy 产生的性能影响是什么（意外的 SQL 执行）。 Hibernate 官方还提供了第七种处理 Lazy 异常的方式：利用 Hibernate 的配置 hibernate.enable_lazy_load_no_trans 配置 Hibernate 官方提供了 hibernate.enable_lazy_load_no_trans 配置，是否允许在关闭之后依然支持 Lazy 加载，此非 JPA 标准，所以在用的时候需要关注版本变化。 其使用方法很简单，直接在 application.properties 里面增加如下配置即可，请看下面的代码： 12## 运行在session关闭之后，重新lazy操作spring.jpa.properties.hibernate.enable_lazy_load_no_trans=true 此时不需要做任何其他修改，当在事务之外，甚至是 Session 之外，触发 Lazy 操作的时候也不会报错，也会正常地进行取数据。 但是建议不要用这个方法，因为一旦开启了这个对 Lazy 的操作就不可控了，会发生预期之外的 Lazy 异常，然后只能通过上面所说的处理 Lazy 异常的第三种和第四种方式解决成预期之内的，否则的话，还会带来很多预期之外的 SQL 执行。这就会造成一种误解，即使用 Hibernate 或者 JPA 会导致性能变差，其实本质原因是不了解原理，没能正确使用。 所以到目前为止，Spring Data JPA 中，hibernate.enable_lazy_load_no_trans 默认是 false，这和 spring.jpa.open-in-view 默认是 true 是相同的道理。所以如果都采用 Spring Boot 的默认配置，一般是没有任何问题的；而有的时候为了更优的配置，需要知道底层的原理，这样才能判断出来业务场景的最佳实践是什么。 Javax.persistence.PersistenceException 异常类型顺藤摸瓜，可以看到 LazyInitializationException 是 HibernateException 里面的，也可以看到 HibernateException 的父类 Javax.persistence.PersistenceException 下面有很多细分的异常，如下图所示。 当我们遇到异常的时候不要慌张，仔细看日志基本就能知道是什么问题了，不同的异常有不同的处理方式，比如 OptimisticLockException 就需要进行重试；而针对 NoSuchBeanException 的异常，就要检查实体配置是否妥当，遇到实际情况再实际分析即可。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 在CompletableFuture异步线程中正确使用JPA]]></title>
    <url>%2F2021%2F01%2F09%2FSpringDataJpa%E5%9C%A8CompletableFuture%E5%BC%82%E6%AD%A5%E7%BA%BF%E7%A8%8B%E4%B8%AD%E6%AD%A3%E7%A1%AE%E4%BD%BF%E7%94%A8JPA%2F</url>
    <content type="text"><![CDATA[在 CompletableFuture 异步线程中正确使用JPACompletableFuture 的使用实际案例在实际开发过程中，难免会用到异步方法，这里列举一个异步方法的例子，经典地还原一些在异步方法里面经常会犯的错误。 模拟一个 Service 方法，通过异步操作，更新 UserInfo 信息，并且可能一个方法里面有不同的业务逻辑，会多次更新 UserInfo 信息，模拟的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839@RestControllerpublic class UserInfoController &#123; //异步操作必须要建立线程池 @Autowired private Executor executor; /** * 模拟一个业务service方法，里面有一些异步操作，一些业务方法里面可能修改了两次用户信息 * @param name * @return */ @PostMapping("test/async/user") @Transactional // 模拟一个 service 方法，期待是一个事务 public String testSaveUser(String name) &#123; CompletableFuture&lt;Void&gt; cf = CompletableFuture.runAsync(() -&gt; &#123; UserInfo user = userInfoRepository.findById(1L).get(); //..... 此处模拟一些业务操作，第一次改变 UserInfo 里面的值； try &#123; Thread.sleep(200L);// 加上复杂业务耗时 200 毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; user.setName(RandomUtils.nextInt(1,100000)+ "_first"+name); //模拟一些业务操作，改变了 UserInfo 里面的值 userInfoRepository.save(user); //..... 此处模拟一些业务操作，第二次改变 UserInfo 里面的值； try &#123; Thread.sleep(300L);// 加上复杂业务耗时 300 毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; user.setName(RandomUtils.nextInt(1,100000)+ "_second"+name);//模拟一些业务操作，改变了 UserInfo 里面的值 userInfoRepository.save(user); &#125;, executor).exceptionally(throwable -&gt; &#123; throwable.printStackTrace(); return null; &#125;); //... 实际业务中，可能还有会其他异步方法 cf.isDone(); return "Success";&#125;&#125; 在 testSaveUser 方法里面开了一个异步线程，异步线程采用 CompletableFuture 方法，在里面执行了两次 UserInfo 的 Save 操作 那么上面的代码问题的表象是什么呢？ 表现出来的问题现状是什么样的？那么实际工作中，如果写出来类似的代码，会发生什么样的问题呢？ 整个请求非常正常，永远都是 200；也没有任何报错信息，但是发现数据库里面第二次的 save(user) 永远不生效，永远不会出现 name 包含 “_second” 的记录，这个是必现的； 整个请求非常正常，永远都是 200；也没有任何报错信息，有的时候会发现数据库里面没有任何变化，甚至第一次 save(user) 都没有生效，但是这个是偶发的。 步骤拆解CompletableFuture 使用最佳实践CompletableFuture 主要的功能是实现了 Future 和 CompletionStage 的接口，主要的方法如下述代码所示: 123456789101112//通过给定的线程池，异步执行 Runnable 方法，不带返回结果public static CompletableFuture&lt;Void&gt; runAsync(Runnable runnable, Executor executor)//通过给定的线程池，异步执行 Runnable 方法，带返回结果public static &lt;U&gt; CompletableFuture&lt;U&gt; supplyAsync(Supplier&lt;U&gt; supplier, Executor executor)//当上面的异步方法执行完之后需要执行的回调方法public CompletableFuture&lt;Void&gt; thenAccept(Consumer&lt;? super T&gt; action)//阻塞等待 future 执行完结果boolean isDone();//阻塞获取结果V get();//当异步操作发生异常的时候执行的方法public CompletionStage&lt;T&gt; exceptionally(Function&lt;Throwable, ? extends T&gt; fn); CompletableFuture 还有更多的方法，其功能也非常强大，所以一般开发过程中用此类的场景还非常多。 其实上面的 Demo 只是利用 runAsync 做了异步操作，并利用 isDone 做了阻塞等待的动作，而没有使用 Exceptionally 处理异常信息。 所以如果想打印异常信息，基本上可以利用 Exceptionally。改进一下 Demo 代码，把异常信息打印一下，看看是否发生了异常。代码如下所示: 123456CompletableFuture&lt;Void&gt; cf = CompletableFuture.runAsync(() -&gt; &#123; ......这里的代码不变&#125;, executor).exceptionally(e -&gt; &#123; log.error(e);//把异常信息打印出来 return null;&#125;); 再请求上面的 Controller 方法的时候，发现控制台就会打印出如下所示的 Error 信息: 1234567891011121314151617java.util.concurrent.CompletionException: org.springframework.orm.ObjectOptimisticLockingFailureException: Object of class [com.example.jpa.demo.db.UserInfo] with identifier [1]: optimistic locking failed; nested exception is org.hibernate.StaleObjectStateException: Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect) : [com.example.jpa.demo.db.UserInfo#1] at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314) at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319) at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1739) at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1167) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:641) at java.base/java.lang.Thread.run(Thread.java:844)Caused by: org.springframework.orm.ObjectOptimisticLockingFailureException: Object of class [com.example.jpa.demo.db.UserInfo] with identifier [1]: optimistic locking failed; nested exception is org.hibernate.StaleObjectStateException: Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect) : [com.example.jpa.demo.db.UserInfo#1] at org.springframework.orm.jpa.vendor.HibernateJpaDialect.convertHibernateAccessException(HibernateJpaDialect.java:337) at org.springframework.orm.jpa.vendor.HibernateJpaDialect.translateExceptionIfPossible(HibernateJpaDialect.java:255) at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:186) at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:212) at com.sun.proxy.$Proxy116.save(Unknown Source) at com.example.jpa.demo.web.UserInfoController.lambda$testSaveUser$0(UserInfoController.java:57) at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run$$$capture(CompletableFuture.java:1736) ... 4 more 通过报错信息，可以发现其实就是发生了乐观锁异常，导致上面实例中的第二次 save(user) 必然失败；而第一次 save(user) 的失败，主要是因为在并发的情况下有其他请求线程改变了 UserInfo 的值，也就是改变了 Version。 来看一下完整的 UserInfo 对象实体。 12345678910111213141516171819202122232425262728@Entity@Data@SuperBuilder@AllArgsConstructor@NoArgsConstructor@ToString(callSuper = true)@Table@EntityListeners(&#123;AuditingEntityListener.class&#125;)public class UserInfo&#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; @Version private Integer version; @CreatedBy private Integer createUserId; @CreatedDate private Instant createTime; @LastModifiedBy private Integer lastModifiedUserId; @LastModifiedDate private Instant lastModifiedTime; private String name; private Integer ages; private String lastName; private String emailAddress; private String telephone;&#125; 通过 @Version 乐关锁机制就是防止数据被覆盖；而实际生产过程中其实很难发现类似问题。 所以当使用任何的异步线程处理框架的时候，一定要想好异常情况下怎么打印日志，否则就像黑洞一样，完全不知道发生了什么。 那么既然知道发生了乐观锁异常，这里就有个疑问了：在 UserInfoController 的 testSaveUser 方法上面加了 @Transaction 的注解，为什么事务没有回滚？ 通过日志查看事务的执行过程看看异步请求的情况下，事务应该怎么做呢？先打开事务的日志，看看上面方法的事务执行过程是什么样的。 1234567## 在 db 的连接中开启 logger=Slf4JLogger&amp;profileSQL=true 看一下每个事务里执行的 sql 有哪些spring.datasource.url=jdbc:mysql://localhost:3306/test?logger=Slf4JLogger&amp;profileSQL=true## 打开下面这些类的日志级别，观察一下事务的开启和关闭时机logging.level.org.springframework.orm.jpa=DEBUGlogging.level.org.springframework.transaction=DEBUGlogging.level.org.springframework.orm.jpa.JpaTransactionManager=tracelogging.level.org.hibernate.engine.transaction.internal.TransactionImpl=DEBUG 再请求一下刚才的测试接口：POST http://localhost:8087/test/async/user?name=jack 就会产生下图所示的日志。 先看一下上半部分，通过日志可以看到，首先执行这个方法的时候开启了两个事务，分别做如下解释。 线程 1：[nio-8087-exec-1] 开启了 UserInfoController.testSaveUser 方法上面的事务，也就是 http 的请求线程，开启了一个 Controller 请求事务。这是因为在 testSaveUser 的方法上面加了 @Transaction 的注解，所以开启了一个事务。 而通过日志可以发现，事务 1 里面什么都没有做，随后就进行了 Commit 操作，所以看得出来，默认不做任何处理的情况下，事务是不能跨线程的。每个线程里面的事务相互隔离、互不影响。 线程 2：[ task-1]，通过异步线程池开启了 SimpleJpaRepository.findById 方法上面的只读事务。这是因为默认的 SimpleJpaRepository 类上面加了 @Transaction(readOnly=true) 产生的结果。通过 MySQL 的日志也可以看得出来，此次事务里面只做了和代码相关的 select user_info 的操作。 再看一下后半部分的日志，如图所示。 通过后半部分日志，可以看到两次 save(user) 方法，也分别开启了各自的事务，这是因为 SimpleJpaRepository.save 方法上面有 @Transaction 注解起了作用，而第二次事务因为 JPA 的实现方法判断了数据库这条数据的 Version 和 UserInfo 的对象中的 Version 不一致，从而第二次进行了回滚操作。 两次 save(user) 的操作里面分别有一次 Select 和 Update 语义。两次事务，分别开启了两个 Session，所以对象对于这两次 Session 来说分别是从游离态（Detached）转成持久态（Persistent）的过程，所以两个独立的事务里面，一次 Select，一次 Update。 通过日志可以看到，上面一个简单的方法中一共发生了四次事务，都是采用的默认隔离级别和传播机制。那么如果想让异步方法里面只有一个事务应该怎么办呢？ 异步事务的正确使用方法异步方法里面的事务是独立的，那么直接把异步的代码块用独立的事务包装起来即可，做法有如下几种： 第一种处理方法：把其中的异步代码块，移到一个外部类里面。这里放到 UserInfoService 中，同时方法中加上 @Transaction 注解用来开启事务，加上 @Retryable 注解进行乐观锁重试，代码如下。 1234567891011121314151617181920212223//加上事务，这样可以做到原子性，解决事务加到异常方法之外没有任何作用的问题@Transactional//加上重试机制，这样当发生乐观锁异常的时候，重新尝试下面的逻辑，减少请求的失败次数@Retryable(value = ObjectOptimisticLockingFailureException.class,backoff = @Backoff(multiplier = 1.5,random = true))public void businessUserMethod(String name) &#123; UserInfo user = userInfoRepository.findById(1L).get(); //..... 此处模拟一些业务操作，第一次改变UserInfo里面的值； try &#123; Thread.sleep(200L);// 加上复杂业务耗时 200 毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; user.setName(RandomUtils.nextInt(1,100000)+ "_first"+name); //模拟一些业务操作，改变了 UserInfo 里面的值 userInfoRepository.save(user); //..... 此处模拟一些业务操作，第二次改变 UserInfo 里面的值； try &#123; Thread.sleep(300L);// 加上复杂业务耗时 300 毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; user.setName(RandomUtils.nextInt(1,100000)+ "_second"+name);//模拟一些业务操作，改变了 UserInfo 里面的值 userInfoRepository.save(user);&#125; 那么 Controller 里面只需要变成如下写法即可。 1234567891011121314151617/** * 模拟一个业务service方法，里面有一些异步操作，一些业务方法里面可能修改了两次用户信息 * @param name * @return */@PostMapping("test/async/user")public String testSaveUser(String name) &#123; CompletableFuture&lt;Void&gt; cf = CompletableFuture.runAsync(() -&gt; &#123; userInfoService.businessUserMethod(name); &#125;, executor).exceptionally(e -&gt; &#123; log.error(e);//把异常信息打印出来 return null; &#125;); //... 实际业务中，可能还有会其他异步方法 cf.isDone(); return "Success";&#125; 再次发起一下请求，看一下日志。 通过上图的日志，可以知道两个重要信息： 这个时候只有 UserInfoServiceImpl.businessUserMethod 开启了一个事务，这是因为 findById 和 Save 方法中，事务的传播机制都是“如果存在事务就利用当前事务”的原理，所以就不会像上面一样创建四次事务了； 而此时两次 save(user) 只产生了一个 update 的 sql 语句，并且也很难出现乐观锁异常了，因为这是 Session 的机制，将两次对 UserInfo 实体的操作进行了合并；所以使用 JPA 的时候某种程度上也会降低 db 的压力，增加代码的执行性能。 而另外一个侧论，就是当事务的生命周期执行越快的时候，发生异常的概率就会越低，因为可以减少并发处理的机会。 第二种处理方法：TransactionTemplate 方法开启事务 第三种处理方法：可以建一个自己的 TransactionHelper，并带上重试机制，代码如下： 1234567891011121314/** * 利用spring进行管理 */@Componentpublic class TransactionHelper &#123; /** * 利用 spring 机制和 jdk8 的 Consumer 机制实现只消费的事务 */ @Transactional(rollbackFor = Exception.class) //可以根据实际业务情况，指定明确的回滚异常 @Retryable(value = ObjectOptimisticLockingFailureException.class,backoff = @Backoff(multiplier = 1.5,random = true)) public void transactional(Consumer consumer,Object o) &#123; consumer.accept(o); &#125;&#125; Controller 里面的写法可以变成如下方式，也可以达到同样效果。 123456789101112131415161718192021222324252627282930@PostMapping("test/async/user")public String testSaveUser(String name) &#123; CompletableFuture&lt;Void&gt; cf = CompletableFuture.runAsync(() -&gt; &#123; transactionHelper.transactional((param)-&gt;&#123; // 通过lambda实现事务管理 UserInfo user = userInfoRepository.findById(1L).get(); //..... 此处模拟一些业务操作，第一次改变UserInfo里面的值； try &#123; Thread.sleep(200L);// 加上复杂业务耗时200毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; user.setName(RandomUtils.nextInt(1,100000)+ "_first"+name); //模拟一些业务操作，改变了UserInfo里面的值 userInfoRepository.save(user); //..... 此处模拟一些业务操作，第二次改变UserInfo里面的值； try &#123; Thread.sleep(300L);// 加上复杂业务耗时300毫秒 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; user.setName(RandomUtils.nextInt(1,100000)+ "_second"+name);//模拟一些业务操作，改变了UserInfo里面的值 userInfoRepository.save(user); &#125;,name); &#125;, executor).exceptionally(e -&gt; &#123; log.error(e);//把异常信息打印出来 return null; &#125;); //... 实际业务中，可能还有会其他异步方法 cf.isDone(); return "Success";&#125; 这种方式主要是通过 Lambda 表达式解决事务问题。 总之，不管是以上哪种方法，都可以解决所说的异步事务的问题。所以搞清楚事务的背后实现逻辑，就很容易解决类似问题了。 还有一个问题就是，为什么当异步方法中是同一个事务的时候，第二次 save(user) 就成功了？而异步代码块里面的两个 save(user) 分别在两个事务里面，第二次就不成功了呢？ Session 的机制与 Repository.save(entity) 是什么关系？Entity 有不同的状态。 在一个 Session 里面，如果通过 findById(id) 得到一个 Entity，它就会变成 Manager（persist） 持久态。那么同一个 Session 里面，同一个 Entity 多次操作 Hibernate 就会进行 Merge 操作。 所以上面的实例中，当在 businessUserMethod 方法上面加 @Transaction 的时候，会造成异步代码的整块逻辑处于同一个事务里面， 同一个事务就会共享同一个 Session，所以同一个事务里面的 findById、save、save 的多次操作都是同一个实例。 什么意思呢？可以通过设置 Debug 断点，查看一下对象的内存对象地址是否一样，就可以看得出来。如下图所示，findById 之后和两次 save 之后都是同一个对象。 而如果跨 Session 传递实体对象，那么在一个 Session 里面持久态的对象，对于另外一个 Session 来说就是一个Detached（游离态）的对象。 而根据 Session 里面的 Persistent Context 的原理，一旦这个游离态的对象进行 db 操作，Session 会 Copy 一个新的实体对象。也就是说，当我们不在异步代码中加事务的时候，即去掉异步代码块 businessUserMethod 方法中的 @Transaction 注解，findById 之后就会产生一个新的事务、新的 Session，那么返回的就是对象 1；第一次 Save 之后，由于又是一个新的事务、新的 Session，那么返回的实体 u2 就是对象 2。 知道这个原理之后，对代码做如下改动。 12345678// @Transactional 去掉事务 public void businessUserMethod(String name) &#123; UserInfo user = userInfoRepository.findById(1L).get(); user.setName(RandomUtils.nextInt(1,100000)+ "_first"+name); //模拟一些业务操作，改变了UserInfo里面的值 UserInfo u2 = userInfoRepository.save(user); user.setName(RandomUtils.nextInt(1,100000)+ "_second"+name); //模拟一些业务操作，改变了UserInfo里面的值 UserInfo u3 = userInfoRepository.save(u2);// 第二次save采用第一次save的返回结果，这样里面带有了最新的version的值，所以也就会保存成功&#125; 异步里面调用这个方法也是成功的，因为乐观锁的原理是 Version 变了，我们用最新的对象，也就是最新的 Version 就可以了。 设置一个断点看一下 user、u2、u3 在不同的 Session 作用域之后，就变成不同的实例了，如下所示。 思考 在上面 Demo 中的异步场景下设置 open-in-view 等于 true / false，会对上面的测试结果有影响吗？ 答案是 肯定没有影响的，spring.jpa.open-in-view 的本质还是开启 Session，而保持住 Session 的本质还是利用 ThreadLocal，也就是必须为同一个线程的情况下才适用。所以异步场景不受 spring.jpa.open-in-view 控制。 如果是大量的异步操作 db connection 的持有模式，应该配置成哪一种比较合适？ 答案是 DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION，因为这样可以做到对 db 连接最大的利用率。用的时候就获取，事务提交完就释放，这样就不用关心业务逻辑执行多长时间了。 总结在开发业务代码的时候，需要思考一下几个问题 一个请求，开启了几次事务？几次 Session？在什么时机开启的？ 事务和 Session 分别会对实体的状态有什么影响？ 一个请求，对 db 连接池里面的连接持有时间是多久？ 一个请求，性能指标都有哪些决定因素？]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa Session 的 OpenInView 对事务的影响]]></title>
    <url>%2F2021%2F01%2F08%2FSpringDataJpa%E7%9A%84Session%E7%9A%84OpenInView%E5%AF%B9%E4%BA%8B%E5%8A%A1%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[Session 的 OpenInView 对事务的影响Spring 新增了一个 spring.jpa.open-in-view 的配置，但是 Hibernate 本身却没有这个配置，不过其又是和 Hibernate 中的 Session 相关的 Session 是什么 其中，SessionImpl 是 Hibernate 实现 JPA 协议的 EntityManager 的一种实现方式，即实现类；而 Session 是 Hibernate 中的概念，完全符合 EntityManager 的接口协议，同时又完成了 Hibernate 的特殊实现。 在 Spring Data JPA 的框架中，可以狭隘地把 Session 理解为 EntityManager，因为其对于 JPA 的任何操作都是通过 EntityManager 的接口进行的，可以把 Session 里面的复杂逻辑当成一个黑盒子。即使 SessionImpl 能够实现 Hibernate 的 Session 接口，但如果使用的是 Spring Data JPA，那么实现再多的接口也和 Spring Data JPA 没有任何关系。 除非不用 JPA 的接口，直接用 Hibernate 的 Native 来实现，但是不建议这么做，因为过程太复杂了。 SessionImpl 解决了什么问题？通过源码来看一下，请看下面这张图。 通过 SessionImpl 的源码和 Structure 的视图，可以“简单粗暴”地得出如下结论。 SessionImpl 是 EntityManager 的实现类，其实现了 JPA 协议规定的 EntityManager 的所有功能；如：EntityManager 暴露的 flushModel 的设置；EntityManager 对 Transaction 做了“是否开启新事务”“是否关闭当前事务”的逻辑。 如上图所示，SessionImpl 实现了 PersistenceContext 对象实例化的过程，使得 PersistenceContext 生命周期就是 Session 的生命周期。所以可以抽象地理解为，Session 是对一些数据库的操作，需要放在同一个上下文的集合中，就是常说的一级缓存。 Session 的 open 和 close 操作： open 的时候做了“是否开启事务”“是否获取连接”等逻辑； close 的时候做了“是否关闭事务”“释放连接”等动作； Session 的任何操作都离不开事务和连接，那么肯定用当前线程保存了这些资源。 JPA 里面的 open-in-view 是做什么的？open-in-view 是 Spring Boot 自动加载 Spring Data JPA 提供的一个配置，全称为 spring.jpa.open-in-view=true，它只有 true 和 false 两个值，默认是 true。 open-in-view 的作用在 JpaBaseConfiguration 中找到关键源码，通过源码来看一下 open-in-view 都做了哪些事情，如下所示。 12345678910111213141516171819202122232425262728293031323334353637public abstract class JpaBaseConfiguration implements BeanFactoryAware &#123;@Configuration(proxyBeanMethods = false)@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass(WebMvcConfigurer.class)//这个提供了一种自定义注册 OpenEntityManagerInViewInterceptor 或者 OpenEntityManagerInViewFilter 的可能，同时我们可以看到在 Web 的 MVC 层打开 session 的两种方式，一种是 Interceptor，另外一种是 Filter；这两个类任选其一即可，默认用的是 OpenEntityManagerInViewInterceptor.class;@ConditionalOnMissingBean(&#123; OpenEntityManagerInViewInterceptor.class, OpenEntityManagerInViewFilter.class &#125;)@ConditionalOnMissingFilterBean(OpenEntityManagerInViewFilter.class)//这里使用了 spring.jpa.open-in-view 的配置，只有为 true 的时候才会执行这个配置类，当什么都没配置的时候，默认就是 true，也就是默认此配置文件就会自动加载；我们可以设置成 false，关闭加载；@ConditionalOnProperty(prefix = "spring.jpa", name = "open-in-view", havingValue = "true", matchIfMissing = true)protected static class JpaWebConfiguration &#123; private static final Log logger = LogFactory.getLog(JpaWebConfiguration.class); private final JpaProperties jpaProperties; protected JpaWebConfiguration(JpaProperties jpaProperties) &#123; this.jpaProperties = jpaProperties; &#125;//关键逻辑在 OpenEntityManagerInViewInterceptor 类里面；加载 OpenEntityManagerInViewInterceptor 用来在MVC的拦截器里面打开 EntityManager，而当我们没有配置 spring.jpa.open-in-view 的时候，看下面代码 spring 容器会打印 warn 日志警告我们，默认开启了 open-in-view，提醒我们需要注意影响面。 @Bean public OpenEntityManagerInViewInterceptor openEntityManagerInViewInterceptor() &#123; if (this.jpaProperties.getOpenInView() == null) &#123; logger.warn("spring.jpa.open-in-view is enabled by default. " + "Therefore, database queries may be performed during view " + "rendering. Explicitly configure spring.jpa.open-in-view to disable this warning"); &#125; return new OpenEntityManagerInViewInterceptor(); &#125; //利用 WebMvcConfigurer 加载上面的 OpenEntityManagerInViewInterceptor 拦截器进入到MVC里面； @Bean public WebMvcConfigurer openEntityManagerInViewInterceptorConfigurer( OpenEntityManagerInViewInterceptor interceptor) &#123; return new WebMvcConfigurer() &#123; @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addWebRequestInterceptor(interceptor); &#125; &#125;; &#125;&#125;.....//其他不重要的代码省略 通过上面的源码可以看到，spring.jpa.open-in-view 的主要作用就是帮我们加载 OpenEntityManagerInViewInterceptor 这个类 OpenEntityManagerInViewInterceptor 源码分析打开这一源码后，可以看到下图所示的界面。 可以发现，OpenEntityManagerInViewInterceptor 实现了 WebRequestInterceptor 的接口中的两个方法： public void preHandle(WebRequest request) ：里面实现了在每次的 Web MVC 请求之前，通过 createEntityManager 方法创建 EntityManager 和 EntityManagerHolder 的逻辑； public void afterCompletion(WebRequest request, @Nullable Exception ex) ：里面实现了在每次 Web MVC 的请求结束之后，关闭 EntityManager 的逻辑。 继续看 createEntityManager 方法的实现，找到如下关键代码： 上图可以看到，通过 SessionFactoryImpl 中的 createEntityManager() 方法， 创建了一个 EntityManager 的实现 Session； 通过拦截器创建了 EntityManager 事务处理逻辑，默认是 Join 类型（即有事务存在会加入）； builder.openSession() 逻辑就是 new SessionImpl(sessionFactory, this)。 所以：通过 open-in-view 配置的拦截器，会帮我们的每个请求都创建一个 SessionImpl 实例；而 SessionImpl 里面存储了整个 PersistenceContext 和各种事务连接状态，可以判断出来 Session 的实例对象比较大。并且，我们打开 spring.jap.open-in-view=true 会发现，如果一个请求处理的逻辑比较耗时，牵涉到的对象比较多，这个时候就比较考验我们对 jvm 的内存配置策略了，如果配置不好就会经常出现内存溢出的现象。因此当处理比较耗时的请求和批量处理请求的时候，需要考虑到这一点。 EntityManager(Session) 的打开时机及扩展场景通过 IDEA ，直接点击右键查 public Session createEntityManager() 此方法被使用到的地方即可，如下图所示： 其中，EntityManagerFactoryAccessor 是 OpenEntityManagerInViewInterceptor 的父类，从图上可以看得出来，Session 的创建（也可以说是 EntityManager 的创建）对我们有用的时机，目前就有三种。 第一种：Web View Interceptor，通过 spring.jpa.open-in-view 控制。 第二种：Web Filter，这种方式是 Spring 给我们提供的另外一种应用场景，比如有些耗时的、批量处理的请求，我们不想在请求的时候开启 Session，而是想在处理简单逻辑后，需要用到延迟加载机制的请求时 Open Session。因为开启 Session 后，我们写框架代码的时候可以利用 lazy 机制。而这个时候我们就可以考虑使用 OpenEntityManagerInViewFilter，配置请求 filter 的过滤机制，实现不同的请求以及不同 Open Session 的逻辑了。 第三种：JPA Transaction，这种方式就是利用 JpaTransactionManager，实现在事务开启的时候打开 Session，在事务结束的时候关闭 Session。 默认情况下 Session 的开启时机有两个： 每个请求之前 新的事务开启之前 Session 的关闭时机是两个： 每个请求结束之后 事务关闭之后 此外，EntityManager(Session) 打开之后，资源存储在当前线程里面 （ThreadLocal），所以一个 Session 中即使开启了多个事务，也不会创建多个 EntityManager 或者 Session。 而事务在关闭之前，也会检查一下此 EntityManager / Session 是不是我这个事务创建的，如果是就关闭，如果不是就不关闭，不过其不会关闭在事务范围之外创建的 EntityManager / Session。 这个机制其实还给我们一些额外思考：我们是不是可以自由选择开启 / 关闭 Session 呢？不一定是 view / filter / 事务，任何多事务组合的代码模块都可以。只要我们知道什么时间开启，保证一定能 close 就没有问题。 下面通过日志来看一下两种打开、关闭 EntityManager 的时机。 验证 EntityManager 的创建和释放的日志第一步：新建一个 UserController 的方法，用来模拟请求两段事务的情况，代码如下所示: 1234567891011@PostMapping("/user/info")public UserInfo saveUserInfo(@RequestBody UserInfo userInfo) &#123; UserInfo u2 = userInfoRepository.findById(1L).orElse(null); if (u2!=null) &#123; u2.setLastName("jack"+userInfo.getLastModifiedTime()); //更新u2，新开启一个事务 userInfoRepository.save(u2); &#125; //更新userInfo，新开启一个事务 return userInfoRepository.save(userInfo);&#125; 可以看到，里面调用了两个 save 操作，没有指定事务。因为 userInfoRepository 的实现类 SimpleJpaRepository 的 save 方法上面有 @Transactional 注解，所以每个 userInfoRepository.save() 方法就会开启新的事务。我们利用这个机制在上面的 Controller 里面模拟了两个事务。 第二步：打开 open-in-view，同时修改一些日志级别，方便观察，配置如下述代码所示： 123456## 打开open-in-viewspring.jpa.open-in-view=true## 修改日志级别logging.level.org.springframework.orm.jpa.JpaTransactionManager=tracelogging.level.org.hibernate.internal=tracelogging.level.org.hibernate.engine.transaction.internal=trace 第三步：启动项目，发送如下请求。 123456#### updatePOST /user/info HTTP/1.1Host: 127.0.0.1:8087Content-Type: application/jsonCache-Control: no-cache&#123;"ages":10,"id":3,"version":0&#125; 然后查看一下日志，关键日志如下图所示。 可以看到，我们请求了 user/info 之后就开启了 Session，然后在 Controller 方法执行的过程中开启了两段事务，每个事务结束之后都没有关闭 Session，而是等两个事务都结束之后，并且 Controller 方法执行完毕之后，才 Closing Session 的。中间过程只创建了一次 Session。 第四步：其他都不变的前提下，我们把 open-in-view 改成 false，如下面这行代码所示。 1spring.jpa.open-in-view=false 我们再执行刚才的请求，会得到如下日志。 通过日志可以看到，其中开启了两次事务，每个事务创建之后都会创建一个 Session，即开启了两个 Session，每个 Session 的 ID 是不一样的；在每个事务结束之后关闭了 Session，关闭了 EntityManager。 通过上面的事例和日志，我们可以看到 spring.jpa.open-in-view 对 session 和事务的影响，那么它对数据库的连接有什么影响呢？ hibernate.connection.handling_mode 详解AvailableSettings 类，可以找到如下三个关键配置： 123456// 指定获得 db 连接的方式，hibernate5.2 之后已经不推荐使用，改用 hibernate.connection.handling_mode 配置形式String ACQUIRE_CONNECTIONS = "hibernate.connection.acquisition_mode";// 释放连接的模式有哪些？hibernate5.2 之后也不推荐使用，改用 hibernate.connection.handling_mode 配置形式String RELEASE_CONNECTIONS = "hibernate.connection.release_mode";//指定获取连接和释放连接的模式，hibernate5.2 之后新增的配置项，代替上面两个旧的配置String CONNECTION_HANDLING = "hibernate.connection.handling_mode"; 那么 hibernate.connection.handling_mode 对应的配置有哪些呢？Hibernate 5 提供了五种模式。 PhysicalConnectionHandlingMode 的五种模式在 Hibernate 5.2 里面，hibernate.connection.handling_mode 这个 Key 对应的值在 PhysicalConnectionHandlingMode 枚举类里面有定义，核心代码如下所示。 12345678910111213141516public enum PhysicalConnectionHandlingMode &#123; IMMEDIATE_ACQUISITION_AND_HOLD( IMMEDIATELY, ON_CLOSE ), DELAYED_ACQUISITION_AND_HOLD( AS_NEEDED, ON_CLOSE ), DELAYED_ACQUISITION_AND_RELEASE_AFTER_STATEMENT( AS_NEEDED, AFTER_STATEMENT ), DELAYED_ACQUISITION_AND_RELEASE_BEFORE_TRANSACTION_COMPLETION( AS_NEEDED, BEFORE_TRANSACTION_COMPLETION ), DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION( AS_NEEDED, AFTER_TRANSACTION ) ; private final ConnectionAcquisitionMode acquisitionMode; private final ConnectionReleaseMode releaseMode; PhysicalConnectionHandlingMode( ConnectionAcquisitionMode acquisitionMode, ConnectionReleaseMode releaseMode) &#123; this.acquisitionMode = acquisitionMode; this.releaseMode = releaseMode; &#125;......//不重要代码先省略&#125; 可以看到一共有五组值，也就是把原来的 ConnectionAcquisitionMode 和 ConnectionReleaseMode 分开配置的模式进行了组合配置管理 IMMEDIATE_ACQUISITION_AND_HOLD：立即获取，一直保持连接到 Session 关闭。 其可以代表如下几层含义： Session 一旦打开就会获取连接； Session 关闭的时候释放连接； 如果 open-in-view=true 的时候，也就是说即使我们的请求里面没有做任何操作，或者有一些耗时操作，会导致数据库的连接释放不及时，从而导致 DB 连接不够用，如果请求频繁的话，会产生不必要的 DB 连接的上下文切换，浪费 CPU 性能； 容易产生 DB 连接获取时间过长的现象，从而导致请求响应时间变长。 DELAYED_ACQUISITION_AND_HOLD：延迟获取，一直保持连接到 Session 关闭。 其可以代表如下几层含义： 表示需要的时候再获取连接，需要的时候是指进行 DB 操作的时候，这里主要是指事务打开的时候，就需要获取连接了（因为开启事务的时候要执行“AUTOCOMMIT=0”的操作，所以这里的按需就是指开启事务；我们也可以关闭事务开启的时候改变 AUTOCOMMIT 的行为，那么这个时候的按需就是指执行 DB 操作的时候，不一定开启事务就会获得 DB 的连接）； 关闭连接的时机是 Session Close 的时候； 一个 Session 里面只有一个连接，而一个连接里面可以有多段事务；比较适合一个请求有多段事务的场景； 这个配置解决了，当没有 DB 操作的时候，即没有事务的时候不会获取数据库连接的问题；从而可以减少不必要的 DB 连接切换； 但是一旦一个 Session 在进行了 DB 操作之后，又做了一些耗时的操作才关闭，那么也会导致 DB 连接释放不及时，从而导致 DB 连接的利用率低、高并发的时候请求性能下降。 DELAYED_ACQUISITION_AND_RELEASE_AFTER_STATEMENT：延迟获取，Statement 执行完释放。 其可以代表如下几层含义： 表示等需要的时候再获取连接，不是 Session 一打开就会获取连接； 在每个 Statement 的 SQL 执行完就释放连接，一旦有事务每个 SQL 执行完释放满足不了业务逻辑，我们常用的事务模式就不生效了； 这种方式适合没有事务的情景，工作中不常见，可能分布式事务中有场景需要。 DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION：延迟获取，事务执行之后释放。 其可以代表如下几层含义： 表示等需要的时候再获取连接，不是 Session 一打开就会获取连接； 在事务执行完之后释放连接，同一个事务共享一个连接； 这种情况下 open-in-view 的模式对 DB 连接的持有和事务一样了，比较适合一个请求里面事务模块不多请求的情况； 如果事务都控制在 Service 层，这个配置就非常好用，其对 Connection 的利用率比较高，基本上可以做到不浪费； 这个配置不适合一个 Session 生命周期里面有很多独立事务的业务模块，因为这样就会使一个请求里面产生大量没必要的获取连接、释放连接的过程。 DELAYED_ACQUISITION_AND_RELEASE_BEFORE_TRANSACTION_COMPLETION：延迟获取，事务执行之前释放。 其可以代表如下几层含义： 表示等需要的时候再获取连接，不是 Session 一打开就会获取连接； 在事务执行完之前释放连接，这种不保险，也比较少用。 默认的模式是哪个？如何修改默认值？打开源码 HibernateJpaVendorAdapter 类里面可以看到如下加载方式。 Hibernate 5.2 以上使用的是 DELAYED_ACQUISITION_AND_HOLD 模式，即按需获取、Session 关闭释放，如下面这段代码。 1jpaProperties.put("hibernate.connection.handling_mode", "DELAYED_ACQUISITION_AND_HOLD"); 而 Hibernate 5.1 以前是通过设置 release_mode 等于 ON_CLOSE 的方式，也是 Session 关闭释放，如下面这段代码。 1jpaProperties.put("hibernate.connection.release_mode", "ON_CLOSE"); 那么，如何修改默认值呢？直接在 application.properties 文件里面做如下修改即可。 12## 我们可以修改成按需获取连接，事务执行完之后释放连接spring.jpa.properties.hibernate.connection.handling_mode=DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION handling_mode 的配置对连接的影响第一步：验证一下 DELAYED_ACQUISITION_AND_HOLD，即默认情况下，连接池的情况是什么样的？ 我们对配置文件做如下配置。 123456## 在拦截MVC层开启 Session，模拟默认情况，这条可以不需要配置spring.jpa.open-in-view=true## 采用默认情况 DELAYED_ACQUISITION_AND_HOLD，这条也不需要配置spring.jpa.properties.hibernate.connection.handling_mode=DELAYED_ACQUISITION_AND_HOLD## 开启 hikari 的数据库连接池的监控：logging.level.com.zaxxer.hikari=TRACE 在 UserInfoController 的如下方法里面，通过 Thread.sleep（2 分钟）模拟耗时操作，代码如下。 123456789101112@PostMapping("/user/info")public UserInfo saveUserInfo(@RequestBody UserInfo userInfo) throws InterruptedException &#123; UserInfo u2 = userInfoRepository.findById(1L).orElse(null); if (u2!=null) &#123; u2.setLastName("jack"+userInfo.getLastModifiedTime()); userInfoRepository.save(u2); System.out.println("模拟事务执行完之后耗时操作........"); Thread.sleep(1000*60*2L); System.out.println("耗时操作执行完毕......."); &#125; return userInfoRepository.save(userInfo);&#125; 项目启动，我们做如下请求。 123456#### updatePOST /user/info HTTP/1.1Host: 127.0.0.1:8087Content-Type: application/jsonCache-Control: no-cache&#123;"ages":10,"id":3,"version":0&#125; 这个时候打开日志控制台，可以看到如下日志。 在 save 之后，即事务提交之后，HikariPool 里面的数据库连接一直没有归还，而如果我们继续等待的话，在整个 Session 关闭之后，数据库连接才会归还到连接池里面。 试想一下，如果我们实际工作中有这样的耗时操作，是不是用不了几个这样的请求，连接池就不够用了？但其实数据库连接没做任何 DB 相关的操作，白白被浪费了。 第二步：验证一下 DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION 模式。 只需要对配置文件做如下修改。 1spring.jpa.properties.hibernate.connection.handling_mode=DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION 其他代码都不变，我们再请求刚才的 API 请求，这个时候可以得到如下日志。 从日志中可以看到，当执行完 save(u2)，事务提交之后，做一些耗时操作的时候，发现此时整个 Session 生命周期是没有持有数据库连接的，也就是事务结束之后就进行了释放，这样大大提高了数据库连接的利用率，即使大量请求也不会造成数据库连接不够用。 下面是一些 Hikari 数据源连接池下， DB 连接获得的时间参考值。 其中，对连接的池的持有情况如下图所示，这是正常情况，几乎监控不到 DB 连接不够用的情况。 对 DB 连接利用率的监控，如下图所示，连接的 Creation、Acquire 基本上是正常的，但是连接的 Usage &gt; 500ms 就有些不正常了，说明里面有一些耗时操作。 🎯所以，一般在实际工作中，我们会在 DELAYED_ACQUISITION_AND_HOLD 和 DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION 之间做选择；通过日志和监控，也可以看得出来 DELAYED_ACQUISITION_AND_HOLD 比较适合一个 Session 里面有大量事务的业务场景，这样不用频繁切换数据库连接。 而 DELAYED_ACQUISITION_AND_RELEASE_AFTER_TRANSACTION 比较适合日常的 API 业务请求，没有大量的事务，事务结束就释放连接的场景。 总结Connection 和 Transaction 的关系 事务是建立在 Connection 之上的，没有连接就没有事务。 以 MySQL InnoDB 为例，新开一个连接默认开启事务，默认每个 SQL 执行完之后自动提交事务。 一个连接里面可以有多次串行的事务段；一个事务只能属于一个 Connection。 事务与事务之间是相互隔离的，那么自然不同连接的不同事务也是隔离的。 EntityManager、Connection 和 Transaction 的关系 EntityManager 里面有 DataSource，当 EntityManager 里面开启事务的时候，先判断当前线程里面是否有数据库连接，如果有直接用。 开启事务之前先开启连接；关闭事务，不一定关闭连接。 开启 EntityManager，不一定立马获得连接；获得连接，不一定立马开启事务。 关闭 EntityManager，一定关闭事务，释放连接；反之不然。 Session、EntityManager、Connection 和 Transaction 的关系 Session 是 EntityManager 的子类，SessionImpl 是 Session 和 EntityManager 的实现类。那么自然 EntityManager 和 Connection、Transaction 的关系同样适用 Session、EntityManager、Connection 和 Transaction 的关系。 Session 的生命周期决定了 EntityManager 的生命周期。 Session 和 Transaction 的关系 在 Hibernate 的 JPA 实现里面，开启 Transaction 之前，必须要先开启 Session。 默认情况下，Session 的生命周期由 open-in-view 决定是请求之前开启，还是事务之前开启。 事务关闭了，Session 不一定关闭。 Session 关闭了，事务一定关闭。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot开启Gizp压缩]]></title>
    <url>%2F2021%2F01%2F02%2FSpringboot%E5%BC%80%E5%90%AFGizp%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[Springboot开启Gizp压缩背景 在优化接口时间的过程中，发现很多接口的Content Download时间较长，除了网络问题，就是接口请求的数据太大了，有的达到了几百kb。控制返回参数收效甚微，这时开启 Gzip 就非常有用了，可以压缩接口请求的数据，一般的json文本压缩比率很大，开启之后接口时间大幅下降！ 启用步骤Spring boot 项目配置比较简单： 123server: compression: enabled: true 默认只压缩 Http body **超过2KB**的数据，可以通过一下配置修改默认属性 123server: compression: min-response-size: 1KB 默认情况下，只有在响应内容类型为以下内容时才会压缩响应 text/html text/xml text/plain text/css 接口返回的是 json 数据，所以修改下面的配置： 1234server: compression: enabled: true mime-types: application/json 如何查看 Gzip 是否开启成功Google 浏览器打开F12，切换到 NetWork 下，右键表头选择 Response Headers 下的Content-Encoding，如果开启了Gzip，对应接口中的Content-Encoding中会有显示。 注意： 在同时开启 https 和 http 的工程中，Gzip 配置只对主端口生效！ 启用后的效果开启前：数据 Size 为 511KB，右侧时间蓝色部分（Content Download）较长 开启前：数据 Size 为 21.3KB，Content-Encoding 中显示 Gzip，右侧时间蓝色部分消失 对比效果非常明显，接口时间由1.03s降到164ms，完美！]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[7大缓存经典问题]]></title>
    <url>%2F2020%2F11%2F23%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98-7%E5%A4%A7%E7%BC%93%E5%AD%98%E7%BB%8F%E5%85%B8%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[7大缓存经典问题 第一个经典问题-缓存失效问题描述 服务系统查数据，首先会查缓存，如果缓存数据不存在，就进一步查 DB，最后查到数据后回种到缓存并返回。 缓存的性能比 DB 高 50~100 倍以上，所以我们希望数据查询尽可能命中缓存，这样系统负荷最小，性能最佳。 缓存里的数据存储基本上都是以 key 为索引进行存储和获取的。业务访问时，如果大量的 key 同时过期，很多缓存数据访问都会 miss，进而穿透到 DB，DB 的压力就会明显上升，由于 DB 的性能较差，只在缓存的 1%~2% 以下，这样请求的慢查率会明显上升。这就是缓存失效的问题。 原因分析 导致缓存失效，特别是很多 key 一起失效的原因，跟我们日常写缓存的过期时间息息相关。 在写缓存时，我们一般会根据业务的访问特点，给每种业务数据预置一个过期时间，在写缓存时把这个过期时间带上，让缓存数据在这个固定的过期时间后被淘汰。一般情况下，因为缓存数据是逐步写入的，所以也是逐步过期被淘汰的。 但在某些场景，一大批数据会被系统主动或被动从 DB 批量加载，然后写入缓存。这些数据写入缓存时，由于使用相同的过期时间，在经历这个过期时间之后，这批数据就会一起到期，从而被缓存淘汰。此时，对这批数据的所有请求，都会出现缓存失效，从而都穿透到 DB，DB 由于查询量太大，就很容易压力大增，请求变慢。 业务场景 很多业务场景，稍不注意，就出现大量的缓存失效，进而导致系统 DB 压力大、请求变慢的情况。比如同一批火车票、飞机票，当可以售卖时，系统会一次性加载到缓存，如果缓存写入时，过期时间按照预先设置的过期值，那过期时间到期后，系统就会因缓存失效出现变慢的问题。类似的业务场景还有很多，比如微博业务，会有后台离线系统，持续计算热门微博，每当计算结束，会将这批热门微博批量写入对应的缓存。还比如，很多业务，在部署新 IDC 或新业务上线时，会进行缓存预热，也会一次性加载大批热数据。 解决方案对于批量 key 缓存失效的问题，原因既然是预置的固定过期时间，那解决方案也从这里入手。设计缓存的过期时间时，使用公式：过期时间=baes 时间+随机时间。即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对 DB 造成过大压力，如下图所示。]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa Hibernate加载过程与配置项]]></title>
    <url>%2F2020%2F11%2F20%2FSpringDataJpa%E7%9A%84Hibernate%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%E4%B8%8E%E9%85%8D%E7%BD%AE%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa Hibernate加载过程与配置项Hibernate 架构分析首先看一下 Hibernate 官方提供的架构图 从架构图上可以知道 Hibernate 实现的 ORM 的接口有两种： Hibernate 自己的 API 接口 Java Persistence API 的接口 Hibernate 比 Java Persistence API 早几年发展的，后来才有了 Java 的持久化协议，Hibernate 是 Spring Data JPA 持久化操作的核心。 再从类上面具体看一下，关键类的图如下所示： 结合类的关系图来看，Session 接口和 SessionFactory 接口都是 Hibernate 的概念，而 EntityManger 和 EntityManagerFactory 都是 Java Persistence API 协议规定的接口。 不过 HibernateEntityManger 从 Hibernate 5.2 之后就开始不推荐使用了，而是建议直接使用 EntityManager 接口即可。那么我们看看 Hibernate 在 Spring BOOT 里面是如何被加载进去的。 Hibernate 5 在 Spring Boot 2 里面的加载过程不同的 Spring Boot 版本，可能加载类的实现逻辑是不一样的，但是分析过程都是相同的。 先打开 spring.factories 文件，如下图所示，其中可以自动加载 Hibernate 的只有一个类，那就是 HibernateJpaAutoConfiguration。 HibernateJpaAutoConfiguration 就是 Spring Boot 加载 Hibernate 的主要入口，所以可以直接打开这个类看一下。 1234567@Configuration(proxyBeanMethods = false)@ConditionalOnClass(&#123; LocalContainerEntityManagerFactoryBean.class, EntityManager.class, SessionImplementor.class &#125;)@EnableConfigurationProperties(JpaProperties.class)//JPAProperties的配置@AutoConfigureAfter(&#123; DataSourceAutoConfiguration.class &#125;)@Import(HibernateJpaConfiguration.class) //hibernate加载的关键类public class HibernateJpaAutoConfiguration &#123;&#125; 其中，第一个需要关注的就是 JpaProperties 类，因为通过这个类可以间接知道，application.properties 里面可以配置的 spring.jpa 的属性有哪些。 JpaProperties 属性打开 JpaProperties 类看一下，如下图所示。 通过这个类，我们可以在 application.properties 里面得到如下配置项。 12345678910111213141516171819# 可以配置JPA的实现者的原始属性的配置，如：这里用的JPA的实现者是hibernate# 那么hibernate里面的一些属性设置就可以通过如下方式实现spring.jpa.properties.hibernate.hbm2ddl.auto=none#hibernate的persistence.xml文件有哪些，目前已经不推荐使用#spring.jpa.mapping-resources=# 指定数据源的类型，如果不指定，Spring Boot加载Datasource的时候会根据URL的协议自己判断# 如：spring.datasource.url=jdbc:mysql://localhost:3306/test 上面可以明确知道是mysql数据源，所以这个可以不需要指定；# 应用场景，当我们通过代理的方式，可能通过datasource.url没办法判断数据源类型的时候，可以通过如下方式指定，可选的值有：DB2,H2,HSQL,INFORMIX,MYSQL,ORACLE,POSTGRESQL,SQL_SERVER,SYBASE)spring.jpa.database=mysql# 是否在启动阶段根据实体初始化数据库的schema，默认false，当我们用内存数据库做测试的时候可以打开，很有用spring.jpa.generate-ddl=false# 和spring.jpa.database用法差不多，指定数据库的平台，默认会自己发现；一般不需要指定，database-platform指定的必须是org.hibernate.dialect.Dialect的子类，如mysql默认是用下面的platformspring.jpa.database-platform=org.hibernate.dialect.MySQLInnoDBDialect# 是否在view层打开session，默认是true，其实大部分场景不需要打开，可以设置成false，spring.jpa.open-in-view=false# 是否显示sql，当执行JPA的数据库操作的时候，默认是false，在本地开发的时候我们可以把这个打开，有助于分析sql是不是我们预期的# 在生产环境的时候建议给这个设置成false，改由logging.level.org.hibernate.SQL=DEBUG代替，这样的话日志默认是基于logback输出的# 而不是直接打印到控制台的，有利于增加traceId和线程ID等信息，便于分析spring.jpa.show-sql=true 其中，spring.jpa.show-sql=true 输出的 sql 效果如下所示。 1Hibernate: insert into user_info (create_time, create_user_id, last_modified_time, last_modified_user_id, version, ages, email_address, last_name, telephone, id) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?) 上面是单线程的 System.out.println 的效果，如果是在线上环境，多线程的情况下就不知道是哪个线程输出来的，而 logging.level.org.hibernate.SQL=DEBUG 输出的 sql 效果如下所示。 12020-11-08 16:54:22.275 DEBUG 6589 --- [nio-8087-exec-1] org.hibernate.SQL 这样可以轻易知道线程 ID 和执行时间，甚至可以有 tranceID 和 spanID 进行日志跟踪，方便分析是哪个线程打印的。 HibernateJpaConfiguration 分析它是通过 HibernateJpaAutoConfiguration 里面的 @Import(HibernateJpaConfiguration.class) 导入进来加载的。 123456@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(HibernateProperties.class)@ConditionalOnSingleCandidate(DataSource.class)class HibernateJpaConfiguration extends JpaBaseConfiguration &#123;...... &#125; 通过源码可以得到 Hibernate 在 JPA 中配置的三个重要线索 第一个线索：HibernateProperties 这个配置类对应的是 spring.jpa.hibernate 的配置 @EnableConfigurationProperties(HibernateProperties.class) 启用了 HibernateProperties 的配置类，如下图所示。 其中可以看到 application.properties 的配置项，如下所示： 12345678# naming 的物理策略值有：org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy(默认)和org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImplspring.jpa.hibernate.naming.physical-strategy=# ddl的生成策略，默认none；如果我们没有指定任何数据源的url，采用的是spring的集成数据源，也就是内存数据源H2的时候，默认值是create-drop;# 所以当我们每次用H2的时候什么都没做，它就会自动帮我们创建表等，内存数据库和写测试用的时候，create-drop就非常方便了；不过，生产数据库一定要设置成none;spring.jpa.hibernate.ddl-auto=none# 当 @Id 配置成 @GeneratedValue(strategy= GenerationType.AUTO) 的时候# 是否采用 hibernate 的 Id-generator-mappings(即会默认帮我们创建一张表hibernate_sequence来存储和生成ID)，默认是truespring.jpa.hibernate.use-new-id-generator-mappings=true 第二个线索：HibernateJpaConfiguration 的父类 JpaBaseConfiguration 也会优先加载，此类就是 Spring Boot 加载 JPA 的核心逻辑 打开 JpaBaseConfiguration 类看一下源码。 123456789101112131415161718192021222324252627282930@Configuration(proxyBeanMethods = false)@EnableConfigurationProperties(JpaProperties.class)// DataSourceInitializedPublisher 用来进行数据源的初始化操作@Import(DataSourceInitializedPublisher.Registrar.class)public abstract class JpaBaseConfiguration implements BeanFactoryAware &#123;protected JpaBaseConfiguration(DataSource dataSource, JpaProperties properties, ObjectProvider&lt;JtaTransactionManager&gt; jtaTransactionManager) &#123; this.dataSource = dataSource; this.properties = properties; //jtaTransactionManager 赋值，正常情况下我们用不到，一般用来解决分布式事务的场景才会用到。 this.jtaTransactionManager = jtaTransactionManager.getIfAvailable(); &#125;//加载 JPA 的实现方式@Bean@ConditionalOnMissingBeanpublic JpaVendorAdapter jpaVendorAdapter() &#123; //createJpaVendorAdapter 是由子类 HibernateJpaConfiguration 实现的，创建JPA的实现类 AbstractJpaVendorAdapter adapter = createJpaVendorAdapter(); adapter.setShowSql(this.properties.isShowSql()); if (this.properties.getDatabase() != null) &#123; adapter.setDatabase(this.properties.getDatabase()); &#125; if (this.properties.getDatabasePlatform() != null) &#123; adapter.setDatabasePlatform(this.properties.getDatabasePlatform()); &#125; adapter.setGenerateDdl(this.properties.isGenerateDdl()); return adapter;&#125;......&#125; @Import(DataSourceInitializedPublisher.Registrar.class) 是用来初始化数据的；从构造函数中也可以看到其是否有用到 jtaTransactionManager（这个是分布式事务才会用到）；而 createJpaVendorAdapter() 是在 HibernateJpaConfiguration 里面实现的，关键代码如下： 12345678class HibernateJpaConfiguration extends JpaBaseConfiguration &#123;//这里是hibernate和Jpa的结合，可以看到使用的HibernateJpaVendorAdapter作为JPA的实现者，可以打开HibernateJpaVendorAdapter里面设置一些断点，就会知道Spring boot是如何一步一步加载Hibernate的了；@Overrideprotected AbstractJpaVendorAdapter createJpaVendorAdapter() &#123; return new HibernateJpaVendorAdapter();&#125;......&#125; 现在已经知道了 HibernateJpaVendorAdapter 的加载逻辑，而 HibernateJpaVendorAdapter 里面实现了 Hibernate 的初始化逻辑 第三个线索：spring.jpa.properties 配置项有哪些 如果接着在 HibernateJpaConfiguration 类里面 debug 查看关键代码的话，可以找到如下代码： 上图中的代码显示，JpaProperties 类里面的 properties 属性，也就是 spring.jpa.properties 的配置加载到了 vendorProperties 里面。而 properties 里面是 HashMap 结构，那么它都可以支持哪些配置呢？ 打开 org.hibernate.cfg.AvailableSettings 可以看到 Hibernate 支持的配置项大概有 100 多个配置信息，如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225String JPA_PERSISTENCE_PROVIDER = &quot;javax.persistence.provider&quot;;String JPA_TRANSACTION_TYPE = &quot;javax.persistence.transactionType&quot;;String JPA_JTA_DATASOURCE = &quot;javax.persistence.jtaDataSource&quot;;String JPA_NON_JTA_DATASOURCE = &quot;javax.persistence.nonJtaDataSource&quot;;String JPA_JDBC_DRIVER = &quot;javax.persistence.jdbc.driver&quot;;String JPA_JDBC_URL = &quot;javax.persistence.jdbc.url&quot;;String JPA_JDBC_USER = &quot;javax.persistence.jdbc.user&quot;;String JPA_JDBC_PASSWORD = &quot;javax.persistence.jdbc.password&quot;;String JPA_SHARED_CACHE_MODE = &quot;javax.persistence.sharedCache.mode&quot;;String JPA_SHARED_CACHE_RETRIEVE_MODE =&quot;javax.persistence.cache.retrieveMode&quot;;String JPA_SHARED_CACHE_STORE_MODE =&quot;javax.persistence.cache.storeMode&quot;;String JPA_VALIDATION_MODE = &quot;javax.persistence.validation.mode&quot;;String JPA_VALIDATION_FACTORY = &quot;javax.persistence.validation.factory&quot;;String JPA_PERSIST_VALIDATION_GROUP = &quot;javax.persistence.validation.group.pre-persist&quot;;String JPA_UPDATE_VALIDATION_GROUP = &quot;javax.persistence.validation.group.pre-update&quot;;String JPA_REMOVE_VALIDATION_GROUP = &quot;javax.persistence.validation.group.pre-remove&quot;;String JPA_LOCK_SCOPE = &quot;javax.persistence.lock.scope&quot;;String JPA_LOCK_TIMEOUT = &quot;javax.persistence.lock.timeout&quot;;String CDI_BEAN_MANAGER = &quot;javax.persistence.bean.manager&quot;;String CLASSLOADERS = &quot;hibernate.classLoaders&quot;;String TC_CLASSLOADER = &quot;hibernate.classLoader.tccl_lookup_precedence&quot;;String APP_CLASSLOADER = &quot;hibernate.classLoader.application&quot;;String RESOURCES_CLASSLOADER = &quot;hibernate.classLoader.resources&quot;;String HIBERNATE_CLASSLOADER = &quot;hibernate.classLoader.hibernate&quot;;String ENVIRONMENT_CLASSLOADER = &quot;hibernate.classLoader.environment&quot;;String JPA_METAMODEL_GENERATION = &quot;hibernate.ejb.metamodel.generation&quot;;String JPA_METAMODEL_POPULATION = &quot;hibernate.ejb.metamodel.population&quot;;String STATIC_METAMODEL_POPULATION = &quot;hibernate.jpa.static_metamodel.population&quot;;String CONNECTION_PROVIDER =&quot;hibernate.connection.provider_class&quot;;String DRIVER =&quot;hibernate.connection.driver_class&quot;;String URL =&quot;hibernate.connection.url&quot;;String USER =&quot;hibernate.connection.username&quot;;String PASS =&quot;hibernate.connection.password&quot;;String ISOLATION =&quot;hibernate.connection.isolation&quot;;String AUTOCOMMIT = &quot;hibernate.connection.autocommit&quot;;String POOL_SIZE =&quot;hibernate.connection.pool_size&quot;;String DATASOURCE =&quot;hibernate.connection.datasource&quot;;String CONNECTION_PROVIDER_DISABLES_AUTOCOMMIT= &quot;hibernate.connection.provider_disables_autocommit&quot;;String CONNECTION_PREFIX = &quot;hibernate.connection&quot;;String JNDI_CLASS =&quot;hibernate.jndi.class&quot;;String JNDI_URL =&quot;hibernate.jndi.url&quot;;String JNDI_PREFIX = &quot;hibernate.jndi&quot;;String DIALECT =&quot;hibernate.dialect&quot;;String DIALECT_RESOLVERS = &quot;hibernate.dialect_resolvers&quot;;String STORAGE_ENGINE = &quot;hibernate.dialect.storage_engine&quot;;String SCHEMA_MANAGEMENT_TOOL = &quot;hibernate.schema_management_tool&quot;;String TRANSACTION_COORDINATOR_STRATEGY = &quot;hibernate.transaction.coordinator_class&quot;;String JTA_PLATFORM = &quot;hibernate.transaction.jta.platform&quot;;String PREFER_USER_TRANSACTION = &quot;hibernate.jta.prefer_user_transaction&quot;;String JTA_PLATFORM_RESOLVER = &quot;hibernate.transaction.jta.platform_resolver&quot;;String JTA_CACHE_TM = &quot;hibernate.jta.cacheTransactionManager&quot;;String JTA_CACHE_UT = &quot;hibernate.jta.cacheUserTransaction&quot;;String JDBC_TYLE_PARAMS_ZERO_BASE = &quot;hibernate.query.sql.jdbc_style_params_base&quot;;String DEFAULT_CATALOG = &quot;hibernate.default_catalog&quot;;String DEFAULT_SCHEMA = &quot;hibernate.default_schema&quot;;String DEFAULT_CACHE_CONCURRENCY_STRATEGY = &quot;hibernate.cache.default_cache_concurrency_strategy&quot;;String USE_NEW_ID_GENERATOR_MAPPINGS = &quot;hibernate.id.new_generator_mappings&quot;;String FORCE_DISCRIMINATOR_IN_SELECTS_BY_DEFAULT = &quot;hibernate.discriminator.force_in_select&quot;;String IMPLICIT_DISCRIMINATOR_COLUMNS_FOR_JOINED_SUBCLASS = &quot;hibernate.discriminator.implicit_for_joined&quot;;String IGNORE_EXPLICIT_DISCRIMINATOR_COLUMNS_FOR_JOINED_SUBCLASS = &quot;hibernate.discriminator.ignore_explicit_for_joined&quot;;String USE_NATIONALIZED_CHARACTER_DATA = &quot;hibernate.use_nationalized_character_data&quot;;String SCANNER_DEPRECATED = &quot;hibernate.ejb.resource_scanner&quot;;String SCANNER = &quot;hibernate.archive.scanner&quot;;String SCANNER_ARCHIVE_INTERPRETER = &quot;hibernate.archive.interpreter&quot;;String SCANNER_DISCOVERY = &quot;hibernate.archive.autodetection&quot;;String IMPLICIT_NAMING_STRATEGY = &quot;hibernate.implicit_naming_strategy&quot;;String PHYSICAL_NAMING_STRATEGY = &quot;hibernate.physical_naming_strategy&quot;;String ARTIFACT_PROCESSING_ORDER = &quot;hibernate.mapping.precedence&quot;;String KEYWORD_AUTO_QUOTING_ENABLED = &quot;hibernate.auto_quote_keyword&quot;;String XML_MAPPING_ENABLED = &quot;hibernate.xml_mapping_enabled&quot;;String SESSION_FACTORY_NAME = &quot;hibernate.session_factory_name&quot;;String SESSION_FACTORY_NAME_IS_JNDI = &quot;hibernate.session_factory_name_is_jndi&quot;;String SHOW_SQL =&quot;hibernate.show_sql&quot;;String FORMAT_SQL =&quot;hibernate.format_sql&quot;;String USE_SQL_COMMENTS =&quot;hibernate.use_sql_comments&quot;;String MAX_FETCH_DEPTH = &quot;hibernate.max_fetch_depth&quot;;String DEFAULT_BATCH_FETCH_SIZE = &quot;hibernate.default_batch_fetch_size&quot;;String USE_STREAMS_FOR_BINARY = &quot;hibernate.jdbc.use_streams_for_binary&quot;;String USE_SCROLLABLE_RESULTSET = &quot;hibernate.jdbc.use_scrollable_resultset&quot;;String USE_GET_GENERATED_KEYS = &quot;hibernate.jdbc.use_get_generated_keys&quot;;String STATEMENT_FETCH_SIZE = &quot;hibernate.jdbc.fetch_size&quot;;String STATEMENT_BATCH_SIZE = &quot;hibernate.jdbc.batch_size&quot;;String BATCH_STRATEGY = &quot;hibernate.jdbc.factory_class&quot;;String BATCH_VERSIONED_DATA = &quot;hibernate.jdbc.batch_versioned_data&quot;;String JDBC_TIME_ZONE = &quot;hibernate.jdbc.time_zone&quot;;String AUTO_CLOSE_SESSION = &quot;hibernate.transaction.auto_close_session&quot;;String FLUSH_BEFORE_COMPLETION = &quot;hibernate.transaction.flush_before_completion&quot;;String ACQUIRE_CONNECTIONS = &quot;hibernate.connection.acquisition_mode&quot;;String RELEASE_CONNECTIONS = &quot;hibernate.connection.release_mode&quot;;String CONNECTION_HANDLING = &quot;hibernate.connection.handling_mode&quot;;String CURRENT_SESSION_CONTEXT_CLASS = &quot;hibernate.current_session_context_class&quot;;String USE_IDENTIFIER_ROLLBACK = &quot;hibernate.use_identifier_rollback&quot;;String USE_REFLECTION_OPTIMIZER = &quot;hibernate.bytecode.use_reflection_optimizer&quot;;String ENFORCE_LEGACY_PROXY_CLASSNAMES = &quot;hibernate.bytecode.enforce_legacy_proxy_classnames&quot;;String ALLOW_ENHANCEMENT_AS_PROXY = &quot;hibernate.bytecode.allow_enhancement_as_proxy&quot;;String QUERY_TRANSLATOR = &quot;hibernate.query.factory_class&quot;;String QUERY_SUBSTITUTIONS = &quot;hibernate.query.substitutions&quot;;String QUERY_STARTUP_CHECKING = &quot;hibernate.query.startup_check&quot;;String CONVENTIONAL_JAVA_CONSTANTS = &quot;hibernate.query.conventional_java_constants&quot;;String SQL_EXCEPTION_CONVERTER = &quot;hibernate.jdbc.sql_exception_converter&quot;;String WRAP_RESULT_SETS = &quot;hibernate.jdbc.wrap_result_sets&quot;;String NATIVE_EXCEPTION_HANDLING_51_COMPLIANCE = &quot;hibernate.native_exception_handling_51_compliance&quot;;String ORDER_UPDATES = &quot;hibernate.order_updates&quot;;String ORDER_INSERTS = &quot;hibernate.order_inserts&quot;;String JPA_CALLBACKS_ENABLED = &quot;hibernate.jpa_callbacks.enabled&quot;;String DEFAULT_NULL_ORDERING = &quot;hibernate.order_by.default_null_ordering&quot;;String LOG_JDBC_WARNINGS = &quot;hibernate.jdbc.log.warnings&quot;;String BEAN_CONTAINER = &quot;hibernate.resource.beans.container&quot;;String C3P0_CONFIG_PREFIX = &quot;hibernate.c3p0&quot;;String C3P0_MAX_SIZE = &quot;hibernate.c3p0.max_size&quot;;String C3P0_MIN_SIZE = &quot;hibernate.c3p0.min_size&quot;;String C3P0_TIMEOUT = &quot;hibernate.c3p0.timeout&quot;;String C3P0_MAX_STATEMENTS = &quot;hibernate.c3p0.max_statements&quot;;String C3P0_ACQUIRE_INCREMENT = &quot;hibernate.c3p0.acquire_increment&quot;;String C3P0_IDLE_TEST_PERIOD = &quot;hibernate.c3p0.idle_test_period&quot;;String PROXOOL_CONFIG_PREFIX = &quot;hibernate.proxool&quot;;String PROXOOL_PREFIX = PROXOOL_CONFIG_PREFIX;String PROXOOL_XML = &quot;hibernate.proxool.xml&quot;;String PROXOOL_PROPERTIES = &quot;hibernate.proxool.properties&quot;;String PROXOOL_EXISTING_POOL = &quot;hibernate.proxool.existing_pool&quot;;String PROXOOL_POOL_ALIAS = &quot;hibernate.proxool.pool_alias&quot;;String CACHE_REGION_FACTORY = &quot;hibernate.cache.region.factory_class&quot;;String CACHE_KEYS_FACTORY = &quot;hibernate.cache.keys_factory&quot;;String CACHE_PROVIDER_CONFIG = &quot;hibernate.cache.provider_configuration_file_resource_path&quot;;String USE_SECOND_LEVEL_CACHE = &quot;hibernate.cache.use_second_level_cache&quot;;String USE_QUERY_CACHE = &quot;hibernate.cache.use_query_cache&quot;;String QUERY_CACHE_FACTORY = &quot;hibernate.cache.query_cache_factory&quot;;String CACHE_REGION_PREFIX = &quot;hibernate.cache.region_prefix&quot;;String USE_MINIMAL_PUTS = &quot;hibernate.cache.use_minimal_puts&quot;;String USE_STRUCTURED_CACHE = &quot;hibernate.cache.use_structured_entries&quot;;String AUTO_EVICT_COLLECTION_CACHE = &quot;hibernate.cache.auto_evict_collection_cache&quot;;String USE_DIRECT_REFERENCE_CACHE_ENTRIES = &quot;hibernate.cache.use_reference_entries&quot;;String DEFAULT_ENTITY_MODE = &quot;hibernate.default_entity_mode&quot;;String GLOBALLY_QUOTED_IDENTIFIERS = &quot;hibernate.globally_quoted_identifiers&quot;;String GLOBALLY_QUOTED_IDENTIFIERS_SKIP_COLUMN_DEFINITIONS = &quot;hibernate.globally_quoted_identifiers_skip_column_definitions&quot;;String CHECK_NULLABILITY = &quot;hibernate.check_nullability&quot;;String BYTECODE_PROVIDER = &quot;hibernate.bytecode.provider&quot;;String JPAQL_STRICT_COMPLIANCE= &quot;hibernate.query.jpaql_strict_compliance&quot;;String PREFER_POOLED_VALUES_LO = &quot;hibernate.id.optimizer.pooled.prefer_lo&quot;;String PREFERRED_POOLED_OPTIMIZER = &quot;hibernate.id.optimizer.pooled.preferred&quot;;String QUERY_PLAN_CACHE_MAX_STRONG_REFERENCES = &quot;hibernate.query.plan_cache_max_strong_references&quot;;String QUERY_PLAN_CACHE_MAX_SOFT_REFERENCES = &quot;hibernate.query.plan_cache_max_soft_references&quot;;String QUERY_PLAN_CACHE_MAX_SIZE = &quot;hibernate.query.plan_cache_max_size&quot;;String QUERY_PLAN_CACHE_PARAMETER_METADATA_MAX_SIZE = &quot;hibernate.query.plan_parameter_metadata_max_size&quot;;String NON_CONTEXTUAL_LOB_CREATION = &quot;hibernate.jdbc.lob.non_contextual_creation&quot;;String HBM2DDL_AUTO = &quot;hibernate.hbm2ddl.auto&quot;;String HBM2DDL_DATABASE_ACTION = &quot;javax.persistence.schema-generation.database.action&quot;;String HBM2DDL_SCRIPTS_ACTION = &quot;javax.persistence.schema-generation.scripts.action&quot;;String HBM2DDL_CONNECTION = &quot;javax.persistence.schema-generation-connection&quot;;String HBM2DDL_DB_NAME = &quot;javax.persistence.database-product-name&quot;;String HBM2DDL_DB_MAJOR_VERSION = &quot;javax.persistence.database-major-version&quot;;String HBM2DDL_DB_MINOR_VERSION = &quot;javax.persistence.database-minor-version&quot;;String HBM2DDL_CREATE_SOURCE = &quot;javax.persistence.schema-generation.create-source&quot;;String HBM2DDL_DROP_SOURCE = &quot;javax.persistence.schema-generation.drop-source&quot;;String HBM2DDL_CREATE_SCRIPT_SOURCE = &quot;javax.persistence.schema-generation.create-script-source&quot;;String HBM2DDL_DROP_SCRIPT_SOURCE = &quot;javax.persistence.schema-generation.drop-script-source&quot;;String HBM2DDL_SCRIPTS_CREATE_TARGET = &quot;javax.persistence.schema-generation.scripts.create-target&quot;;String HBM2DDL_SCRIPTS_DROP_TARGET = &quot;javax.persistence.schema-generation.scripts.drop-target&quot;;String HBM2DDL_IMPORT_FILES = &quot;hibernate.hbm2ddl.import_files&quot;;String HBM2DDL_LOAD_SCRIPT_SOURCE = &quot;javax.persistence.sql-load-script-source&quot;;String HBM2DDL_IMPORT_FILES_SQL_EXTRACTOR = &quot;hibernate.hbm2ddl.import_files_sql_extractor&quot;;String HBM2DDL_CREATE_NAMESPACES = &quot;hibernate.hbm2ddl.create_namespaces&quot;;String HBM2DLL_CREATE_NAMESPACES = &quot;hibernate.hbm2dll.create_namespaces&quot;;String HBM2DDL_CREATE_SCHEMAS = &quot;javax.persistence.create-database-schemas&quot;;String HBM2DLL_CREATE_SCHEMAS = HBM2DDL_CREATE_SCHEMAS;String HBM2DDL_FILTER_PROVIDER = &quot;hibernate.hbm2ddl.schema_filter_provider&quot;;String HBM2DDL_JDBC_METADATA_EXTRACTOR_STRATEGY = &quot;hibernate.hbm2ddl.jdbc_metadata_extraction_strategy&quot;;String HBM2DDL_DELIMITER = &quot;hibernate.hbm2ddl.delimiter&quot;;String HBM2DDL_CHARSET_NAME = &quot;hibernate.hbm2ddl.charset_name&quot;;String HBM2DDL_HALT_ON_ERROR = &quot;hibernate.hbm2ddl.halt_on_error&quot;;String JMX_ENABLED = &quot;hibernate.jmx.enabled&quot;;String JMX_PLATFORM_SERVER = &quot;hibernate.jmx.usePlatformServer&quot;;String JMX_AGENT_ID = &quot;hibernate.jmx.agentId&quot;;String JMX_DOMAIN_NAME = &quot;hibernate.jmx.defaultDomain&quot;;String JMX_SF_NAME = &quot;hibernate.jmx.sessionFactoryName&quot;;String JMX_DEFAULT_OBJ_NAME_DOMAIN = &quot;org.hibernate.core&quot;;String CUSTOM_ENTITY_DIRTINESS_STRATEGY = &quot;hibernate.entity_dirtiness_strategy&quot;;String USE_ENTITY_WHERE_CLAUSE_FOR_COLLECTIONS = &quot;hibernate.use_entity_where_clause_for_collections&quot;;String MULTI_TENANT = &quot;hibernate.multiTenancy&quot;;String MULTI_TENANT_CONNECTION_PROVIDER = &quot;hibernate.multi_tenant_connection_provider&quot;;String MULTI_TENANT_IDENTIFIER_RESOLVER = &quot;hibernate.tenant_identifier_resolver&quot;;String INTERCEPTOR = &quot;hibernate.session_factory.interceptor&quot;;String SESSION_SCOPED_INTERCEPTOR = &quot;hibernate.session_factory.session_scoped_interceptor&quot;;String STATEMENT_INSPECTOR = &quot;hibernate.session_factory.statement_inspector&quot;;String ENABLE_LAZY_LOAD_NO_TRANS = &quot;hibernate.enable_lazy_load_no_trans&quot;;String HQL_BULK_ID_STRATEGY = &quot;hibernate.hql.bulk_id_strategy&quot;;String BATCH_FETCH_STYLE = &quot;hibernate.batch_fetch_style&quot;;String DELAY_ENTITY_LOADER_CREATIONS = &quot;hibernate.loader.delay_entity_loader_creations&quot;;String JTA_TRACK_BY_THREAD = &quot;hibernate.jta.track_by_thread&quot;;String JACC_CONTEXT_ID = &quot;hibernate.jacc_context_id&quot;;String JACC_PREFIX = &quot;hibernate.jacc&quot;;String JACC_ENABLED = &quot;hibernate.jacc.enabled&quot;;String ENABLE_SYNONYMS = &quot;hibernate.synonyms&quot;;String EXTRA_PHYSICAL_TABLE_TYPES = &quot;hibernate.hbm2ddl.extra_physical_table_types&quot;;String DEPRECATED_EXTRA_PHYSICAL_TABLE_TYPES = &quot;hibernate.hbm2dll.extra_physical_table_types&quot;;String UNIQUE_CONSTRAINT_SCHEMA_UPDATE_STRATEGY = &quot;hibernate.schema_update.unique_constraint_strategy&quot;;String GENERATE_STATISTICS = &quot;hibernate.generate_statistics&quot;;String LOG_SESSION_METRICS = &quot;hibernate.session.events.log&quot;;String LOG_SLOW_QUERY = &quot;hibernate.session.events.log.LOG_QUERIES_SLOWER_THAN_MS&quot;;String AUTO_SESSION_EVENTS_LISTENER = &quot;hibernate.session.events.auto&quot;;String PROCEDURE_NULL_PARAM_PASSING = &quot;hibernate.proc.param_null_passing&quot;;String CREATE_EMPTY_COMPOSITES_ENABLED = &quot;hibernate.create_empty_composites.enabled&quot;;String ALLOW_JTA_TRANSACTION_ACCESS = &quot;hibernate.jta.allowTransactionAccess&quot;;String ALLOW_UPDATE_OUTSIDE_TRANSACTION = &quot;hibernate.allow_update_outside_transaction&quot;;String COLLECTION_JOIN_SUBQUERY = &quot;hibernate.collection_join_subquery&quot;;String ALLOW_REFRESH_DETACHED_ENTITY = &quot;hibernate.allow_refresh_detached_entity&quot;;String MERGE_ENTITY_COPY_OBSERVER = &quot;hibernate.event.merge.entity_copy_observer&quot;;String USE_LEGACY_LIMIT_HANDLERS = &quot;hibernate.legacy_limit_handler&quot;;String VALIDATE_QUERY_PARAMETERS = &quot;hibernate.query.validate_parameters&quot;;String CRITERIA_LITERAL_HANDLING_MODE = &quot;hibernate.criteria.literal_handling_mode&quot;;String PREFER_GENERATOR_NAME_AS_DEFAULT_SEQUENCE_NAME = &quot;hibernate.model.generator_name_as_sequence_name&quot;;String JPA_TRANSACTION_COMPLIANCE = &quot;hibernate.jpa.compliance.transaction&quot;;String JPA_QUERY_COMPLIANCE = &quot;hibernate.jpa.compliance.query&quot;;String JPA_LIST_COMPLIANCE = &quot;hibernate.jpa.compliance.list&quot;;String JPA_CLOSED_COMPLIANCE = &quot;hibernate.jpa.compliance.closed&quot;;String JPA_PROXY_COMPLIANCE = &quot;hibernate.jpa.compliance.proxy&quot;;String JPA_CACHING_COMPLIANCE = &quot;hibernate.jpa.compliance.caching&quot;;String JPA_ID_GENERATOR_GLOBAL_SCOPE_COMPLIANCE = &quot;hibernate.jpa.compliance.global_id_generators&quot;;String TABLE_GENERATOR_STORE_LAST_USED = &quot;hibernate.id.generator.stored_last_used&quot;;String FAIL_ON_PAGINATION_OVER_COLLECTION_FETCH = &quot;hibernate.query.fail_on_pagination_over_collection_fetch&quot;;String IMMUTABLE_ENTITY_UPDATE_QUERY_HANDLING_MODE = &quot;hibernate.query.immutable_entity_update_query_handling_mode&quot;;String IN_CLAUSE_PARAMETER_PADDING = &quot;hibernate.query.in_clause_parameter_padding&quot;;String QUERY_STATISTICS_MAX_SIZE = &quot;hibernate.statistics.query_max_size&quot;;String SEQUENCE_INCREMENT_SIZE_MISMATCH_STRATEGY = &quot;hibernate.id.sequence.increment_size_mismatch_strategy&quot;;String OMIT_JOIN_OF_SUPERCLASS_TABLES = &quot;hibernate.query.omit_join_of_superclass_tables&quot;; AvailableSettings 里面的配置项的用法只需要将 AvailableSettings 变量的值放到 spring.jpa.properties 里面即可，如下这些是我们常用的。 123456789101112131415161718192021##开启hibernate statistics的信息，如session、连接等日志：spring.jpa.properties.hibernate.generate_statistics=true# 格式化 SQLspring.jpa.properties.hibernate.format_sql: true# 显示 SQLspring.jpa.properties.hibernate.show_sql: true# 添加 HQL 相关的注释信息spring.jpa.properties.hibernate.use_sql_comments: true# hbm2ddl的策略 validate, update, create, create-drop, none，建议配置成validate，# 这样在我们启动项目的时候就知道生产数据库的表结构是否正确的了，而不用等到运行期间才发现问题。spring.jpa.properties.hibernate.hbm2ddl.auto=validate# 关联关系的时候取数据的深度，默认是3层，我们可以设置成2级，防止其他开发乱用，提高sql性能spring.jpa.properties.hibernate.max_fetch_depth=2# 批量fetch大小默认 -1spring.jpa.properties.hibernate.default_batch_fetch_size= 100# 事务完成之前是否进行flush操作，即同步到db里面去，默认是truespring.jpa.properties.hibernate.transaction.flush_before_completion=true# 事务结束之后是否关闭session，默认falsespring.jpa.properties.hibernate.transaction.auto_close_session=false# 有的时候不只要批量查询，也会批量更新，默认batch size是15，可以根据实际情况自由调整，可以提高批量更新的效率；spring.jpa.properties.hibernate.jdbc.batch_size=100 自动加载过程类之间的关系图 从上图中，可以看出以下几点内容。 JpaBaseConfiguration 是 Jpa 和 Hibernate 被加载的基石，里面通过 BeanFactoryAware 的接口的 bean 加载生命周期也实现了一些逻辑。 HibernateJpaConfiguration 是 JpaBaseConfiguration 的子类，覆盖了一些父类里面的配置相关的特殊逻辑，并且里面引用了 JpaPropeties 和 HibernateProperties 的配置项。 HibernateJpaAutoConfiguration 是 Spring Boot 自动加载 HibernateJpaConfiguration 的桥梁，起到了导入和加载 HibernateJpaConfiguration 的作用。 JpaRepositoriesAutoConfiguration 和 HibernateJpaAutoConfiguration、DataSourceAutoConfiguration 分别加载 JpaRepositories 的逻辑和 Hibernate/JPA、数据源，都是被 spring.factories 自动装配进入到 Spring Boot 里面的，而三者之间有加载的先后顺序。 上图的 UML 还展示了几个 Configuration 类的加载顺序和依赖关系，顺序是从上到下进行加载的 DataSourceAutoConfiguration 最先加载 HibernateJpaAutoConfiguration 第二顺序加载 JpaRepositoriesAutoConfiguration 最后加载 Spring Data JPA Repositories Bootstrap Mode了解完了 Hibernate 在 Spring Boot 里面的加载过程，那么来看下 JpaRepositoriesAutoConfiguration 的主要作用有哪些？ 通过上面分享的整个加载过程可以发现： DataSourceAutoConfiguration 完成了数据源的加载 HibernateJpaAutoConfiguration 完成了 Hibernate 的加载过程 JpaRepositoriesAutoConfiguration 完成了各种 JpaRepositories 的加载过程，这是 Spring Data JPA 的主要实现逻辑，和 Hibernate、数据源没什么关系。 可以通过 JpaRepositoriesAutoConfiguration 的源码发现其主要职责和实现方式，利用异步线程池初始化 repositories，关键源码如下： 而其中加载 repositories 有三种方式，即 spring.data.jpa.repositories.bootstrap-mode 的三个值，分别为 deferred、 lazy、 default deferred：是默认值，表示在启动的时候会进行数据库字段的检查，而 repositories 相关的实例的初始化是 lazy 模式，也就是在第一次用到 repositories 实例的时候再进行初始化。这个比较适合用在测试环境和生产环境中，因为测试不可能覆盖所有场景，万一谁多加个字段或者少一个字段，这样在启动的阶段就可以及时发现问题，不能等进行到生产环境才暴露。 lazy：表示启动阶段不会进行数据库字段的检查，也不会初始化 repositories 相关的实例，而是在第一次用到 repositories 实例的时候再进行初始化。这个比较适合用在开发的阶段，可以加快应用的启动速度。如果生产环境中，我们为了提高业务高峰期间水平来扩展应用的启动速度，也可以采用这种模式。 default：默认加载方式，但从 Spring Boot 2.0 之后就不是默认值了，表示立即验证、立即初始化 repositories 实例，这种方式启动的速度最慢，但是最保险，运行期间的请求最快，因为避免了第一次请求初始化 repositories 实例的过程。 我们通过在 application.properties 里面修改这一行代码，来测试一下 lazy 的加载方式。 1spring.data.jpa.repositories.bootstrap-mode=lazy 然后启动项目，就会发现在 tomcat 容器加载完之后，没有用到 UserInfoRepository 之前，这个 UserInfoRepository 是不会进行初始化的。而当我们发一个请求用到了 UserInfoRepository，就进行了初始化。 通过日志也可以看到，启动的线程和初始化的线程是不一样的，而初始化的线程是 NIO 线程的名字，表示 request 的 http 线程池里面的线程，具体如下图所示。 Debug 时候，日志的配置12345678910111213141516171819202122232425262728293031323334353637### 日志级别的灵活运用## hibernate相关# 显示sql的执行日志，如果开了这个,show_sql就可以不用了logging.level.org.hibernate.SQL=debug# hibernate id的生成日志logging.level.org.hibernate.id=debug# hibernate所有的操作都是PreparedStatement，把sql的执行参数显示出来logging.level.org.hibernate.type.descriptor.sql.BasicBinder=TRACE# sql执行完提取的返回值logging.level.org.hibernate.type.descriptor.sql=trace# 请求参数logging.level.org.hibernate.type=debug# 缓存相关logging.level.org.hibernate.cache=debug# 统计hibernate的执行状态logging.level.org.hibernate.stat=debug# 查看所有的缓存操作logging.level.org.hibernate.event.internal=tracelogging.level.org.springframework.cache=trace# hibernate 的监控指标日志logging.level.org.hibernate.engine.internal.StatisticalLoggingSessionEventListener=DEBUG### 连接池的相关日志## hikari连接池的状态日志，以及连接池是否完好 #连接池的日志效果：HikariCPPool - Pool stats (total=20, active=0, idle=20, waiting=0)logging.level.com.zaxxer.hikari=TRACE#开启 debug可以看到 AvailableSettings里面的默认配置的值都有哪些，会输出类似下面的日志格式# org.hibernate.cfg.Settings : Statistics: enabled# org.hibernate.cfg.Settings : Default batch fetch size: -1logging.level.org.hibernate.cfg=debug#hikari数据的配置项日志logging.level.com.zaxxer.hikari.HikariConfig=TRACE### 查看事务相关的日志，事务获取，释放日志logging.level.org.springframework.orm.jpa=DEBUGlogging.level.org.springframework.transaction=TRACElogging.level.org.hibernate.engine.transaction.internal.TransactionImpl=DEBUG### 分析connect 以及 orm和 data的处理过程更全的日志logging.level.org.springframework.data=tracelogging.level.org.springframework.orm=trace 上面是在分析复杂问题和原理的时候常用的日志配置项目 🎯技巧： 当我们分析一个问题的时候，如果不知道日志具体在哪个类里面，通过设置 logging.level.root=trace 的话，日志又非常多几乎没有办法看，那么我们可以缩小范围，不如说我们分析的是 hikari 包里面相关的问题。 我们可以把整个日志级别 logging.level.root=info 设置成 info，把其他所有的日志都关闭，并把 logging.level.com.zaxxer=trace 设置成最大的，保持日志不受干扰，然后观察日志再逐渐减少查看范围。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 事务与连接池]]></title>
    <url>%2F2020%2F11%2F18%2FSpringDataJpa%E4%BA%8B%E5%8A%A1%E4%B8%8E%E8%BF%9E%E6%8E%A5%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa 事务与连接池事务的基本原理以 MySQL 5.7 为例： 当 MySQL 使用 InnoDB 数据库引擎的时候，数据库是对事务有支持的。而事务最主要的作用是保证数据 ACID 的特性，即原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。分别做如下解释： 原子性： 是指一个事务（Transaction）中的所有操作，要么全部完成，要么全部回滚，而不会有中间某个数据单独更新的操作。事务在执行过程中一旦发生错误，会被回滚（Rollback）到此次事务开始之前的状态，就像这个事务从来没有执行过一样。 一致性： 是指事务操作开始之前，和操作异常回滚以后，数据库的完整性没有被破坏。数据库事务 commit 之后，数据也是按照我们预期正确执行的。即要通过事务保证数据的正确性。 持久性： 是指事务处理结束后，对数据的修改进行了持久化的永久保存，即便系统故障也不会丢失，其实就是保存到硬盘。 隔离性： 是指数据库允许多个连接，同时并发多个事务，又对同一个数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时，由于交叉执行而导致数据不一致的现象。而 MySQL 里面就是我们经常说的事务的四种隔离级别，即读未提交（Read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（Serializable）。 MySQL 事务的隔离级别 Read Uncommitted（读取未提交内容）：此隔离级别，表示所有正在进行的事务都可以看到其他未提交事务的执行结果。不同事务之间读取到其他事务中未提交的数据，通常这种情况也被称之为脏读（Dirty Read），会造成数据的逻辑处理错误，也就是我们在多线程里面经常说的数据不安全了。在业务开发中，几乎很少见到使用的，因为它的性能也不比其他级别要好多少。 Read Committed（读取提交内容）： 此隔离级别是指，在一个事务相同的两次查询可能产生的结果会不一样，也就是第二次查询能读取到其他事务已经提交的最新数据。也就是我们常说的不可重复读（Unrepeatable Read）的事务隔离级别。因为同一事务的其他实例在该实例处理期间，可能会对其他事务进行新的 commit，所以在同一个事务中的同一 select 上，多次执行可能返回不同结果。这是大多数数据库系统的默认隔离级别（但不是 MySQL 默认的隔离级别）。 Repeatable Read（可重读）： 这是 MySQL 的默认事务隔离级别，它确保同一个事务多次查询相同的数据，能读到相同的数据。即使多个事务的修改已经 commit，本事务如果没有结束，永远读到的是相同数据，要注意它与Read Committed 的隔离级别的区别，是正好相反的。这会导致另一个棘手的问题：幻读 （Phantom Read），即读到的数据可能不是最新的。这个是最常见的，举个例子来说明。 第一步：用工具打开一个数据库的 DB 连接，如图所示。 查看一下数据库的事务隔离级别。 然后开启一个事务，查看一下 user_info 的数据，我们在 user_info 表里面插入了三条数据，如下图所示： 第二步：我们打开另外一个相同数据库的 DB 连接，删除一条数据，SQL 如下： 当删除执行成功之后，我们可以开启第三个连接，看一下数据库里面确实少了一条 ID=1 的数据。那么这个时候我们再返回第一个连接，第二次执行 select * from user_info，如下图所示，查到的还是三条数据。这就是我们经常说的可重复读。 Serializable（可串行化）：这是最高的隔离级别，它保证了每个事务是串行执行的，即强制事务排序，所有事务之间不可能产生冲突，从而解决幻读问题。如果配置在这个级别的事务，处理时间比较长，并发比较大的时候，就会导致大量的 db 连接超时现象和锁竞争，从而降低了数据处理的吞吐量。也就是这个性能比较低，所以除了某些财务系统之外，用的人不是特别多。 MySQL 事务与连接的关系要搞清楚事务和连接池的关系，必须要先知道二者存在的前提条件。 事务必须在同一个连接里面的，离开连接没有事务可言； MySQL 数据库默认 autocommit=1，即每一条 SQL 执行完自动提交事务； 数据库里面的每一条 SQL 执行的时候必须有事务环境； MySQL 创建连接的时候默认开启事务，关闭连接的时候如果存在事务没有 commit 的情况，则自动执行 rollback 操作； 不同的 connect 之间的事务是相互隔离的。 知道了这些条件，就可以继续探索二者的关系了。在 connection 当中，操作事务的方式只有两种。 MySQL 事务的两种操作方式第一种：用 BEGIN、ROLLBACK、COMMIT 来实现。 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 第二种：直接用 SET 来改变 MySQL 的自动提交模式。 SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 MySQL 数据库的最大连接数是什么？而任何数据库的连接数都是有限的，受内存和 CPU 限制。 查看数据库最大连接数： show variables like &#39;max_connections&#39; 查看正在使用的连接数： show global status like &#39;Max_used_connections&#39; 设置数据库最大连接数： set global max_connections=1500 注意： 再观察数据库的连接数的的同时，需要观察CPU和内存的使用情况，以此来判断当前系统数据库中server的连接数最佳大小是多少 MySQL 连接的超时时间，默认是 8 小时 Spring 里面事务的配置方法Spring Boot会通过 TransactionAutoConfiguration 加载 @EnableTransactionManagement 注解帮我们默认开启事务，关键代码如下图所示： Spring 里面的事务有两种使用方式，常见的是直接通过 @Transaction 的方式进行配置，打开 SimpleJpaRepository 源码类会看到如下界面： 12345678Repository@Transactional(readOnly = true)public class SimpleJpaRepository&lt;T, ID&gt; implements JpaRepositoryImplementation&lt;T, ID&gt; &#123;...@Transactional@Overridepublic void deleteAll(Iterable&lt;? extends T&gt; entities) &#123;..... 默认情况下，所有 SimpleJpaRepository 里面的方法都是只读事务，而一些更新的方法都是读写事务。 所以每个 Repository 的方法是都是有事务的，即使没有使用任何加 @Transactional 注解的方法，按照上面所讲的 MySQL 的 Transactional 开启原理，也会有数据库的事务。 默认 @Transactional 注解式事务注解式事务又称显式事务，需要手动显式注解声明，@Transactional 的源码，如下所示： 123456789101112131415161718@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Inherited@Documentedpublic @interface Transactional &#123; @AliasFor("transactionManager") String value() default ""; @AliasFor("value") String transactionManager() default ""; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default TransactionDefinition.TIMEOUT_DEFAULT; boolean readOnly() default false; Class&lt;? extends Throwable&gt;[] rollbackFor() default &#123;&#125;; String[] rollbackForClassName() default &#123;&#125;; Class&lt;? extends Throwable&gt;[] noRollbackFor() default &#123;&#125;; String[] noRollbackForClassName() default &#123;&#125;;&#125; @Transactional 注解中常用的参数： 参数名称 功能描述 readOnly 该属性用于设置当前事务是否为只读事务，设置为true表示只读，false则表示可读写，默认值为false。例如：@Transactional(readOnly=true) rollbackFor 该属性用于设置需要进行回滚的异常类数组，当方法中抛出指定异常数组中的异常时，则进行事务回滚。例如：指定单一异常类：@Transactional(rollbackFor=RuntimeException.class)指定多个异常类：@Transactional(rollbackFor={RuntimeException.class, Exception.class}) rollbackForClassName 该属性用于设置需要进行回滚的异常类名称数组，当方法中抛出指定异常名称数组中的异常时，则进行事务回滚。例如：指定单一异常类名称：@Transactional(rollbackForClassName=”RuntimeException”)指定多个异常类名称：@Transactional(rollbackForClassName={“RuntimeException”,”Exception”}) noRollbackFor 该属性用于设置不需要进行回滚的异常类数组，当方法中抛出指定异常数组中的异常时，不进行事务回滚。例如：指定单一异常类：@Transactional(noRollbackFor=RuntimeException.class)指定多个异常类：@Transactional(noRollbackFor={RuntimeException.class, Exception.class}) noRollbackForClassName 该属性用于设置不需要进行回滚的异常类名称数组，当方法中抛出指定异常名称数组中的异常时，不进行事务回滚。例如：指定单一异常类名称：@Transactional(noRollbackForClassName=”RuntimeException”)指定多个异常类名称：@Transactional(noRollbackForClassName={“RuntimeException”,”Exception”}) propagation 该属性用于设置事务的传播行为。例如：@Transactional(propagation=Propagation.NOT_SUPPORTED,readOnly=true) isolation 该属性用于设置底层数据库的事务隔离级别，事务隔离级别用于处理多事务并发的情况，通常使用数据库的默认隔离级别即可，基本不需要进行设置 timeout 该属性用于设置事务的超时秒数，默认值为-1表示永不超时 隔离级别和事务的传播机制隔离级别 Isolation isolation() default Isolation.DEFAULT：默认采用数据库的事务隔离级别。其中，Isolation 是个枚举值： READ_UNCOMMITTED：读取未提交数据(会出现脏读, 不可重复读) 基本不使用 READ_COMMITTED：读取已提交数据(会出现不可重复读和幻读) REPEATABLE_READ：可重复读(会出现幻读) SERIALIZABLE：串行化 MYSQL：默认为 REPEATABLE_READ 级别SQLSERVER：默认为 READ_COMMITTED propagation：代表的是事务的传播机制，这个是 Spring 事务的核心业务逻辑，是 Spring 框架独有的，它和 MySQL 数据库没有一点关系。所谓事务的传播行为是指在同一线程中，在开始当前事务之前，需要判断一下当前线程中是否有另外一个事务存在，如果存在，提供了七个选项来指定当前事务的发生行为。 org.springframework.transaction.annotation.Propagation 这类有 7 个表示传播行为的枚举值如下： 123456789public enum Propagation &#123; REQUIRED(0), SUPPORTS(1), MANDATORY(2), REQUIRES_NEW(3), NOT_SUPPORTED(4), NEVER(5), NESTED(6)&#125; REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这个值是默认的。 SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于 REQUIRED。 设置方法：通过使用 propagation 属性设置，例如： 1@Transactional(propagation = Propagation.REQUIRES_NEW) @Transactional 的局限性列举一个当前对象调用对象自己里面的方法不起作用的场景。 在 UserInfoServiceImpl 的 save 方法中调用了带事务的 calculate 方法，请看代码： 1234567891011121314151617181920212223@Componentpublic class UserInfoServiceImpl implements UserInfoService &#123; @Autowired private UserInfoRepository userInfoRepository; /** * 根据UserId产生的一些业务计算逻辑 */ @Override @Transactional(transactionManager = "db2TransactionManager") public UserInfo calculate(Long userId) &#123; UserInfo userInfo = userInfoRepository.findById(userId).get(); userInfo.setAges(userInfo.getAges()+1); //.....等等一些复杂事务内的操作 userInfo.setTelephone(Instant.now().toString()); return userInfoRepository.saveAndFlush(userInfo); &#125; /** * 此方法调用自身对象的方法，就会发现calculate方法上面的事务是失效的 */ public UserInfo save(Long userId) &#123; return this.calculate(userId); &#125;&#125; 当在 UserInfoServiceImpl 类的外部调用 save 方法的时候，此时 save 方法里面调用了自身的 calculate 方法，就会发现 calculate 方法上面的事务是没有效果的，这个是 Spring 的代理机制的问题。那么我们应该如何解决这个问题呢？可以引入一个类 TransactionTemplate，我们看下它的用法。 TransactionTemplate 的用法此类是通过 TransactionAutoConfiguration 加载配置进去的，如下图所示： 通过源码可以看到此类提供了一个关键 execute 方法，如下图所示： 这里面会帮我们处理事务开始、rollback、commit 的逻辑，所以用的时候就非常简单，把上面的方法做如下改动： 123public UserInfo save(Long userId) &#123; return transactionTemplate.execute(status -&gt; this.calculate(userId));&#125; 此时外部再调用 save 方法的时候，calculate 就会进入事务管理里面去了。也可以通过下面代码中的方法设置事务的属性。 123456789transactionTemplate = new TransactionTemplate(transactionManager);//设置隔离级别transactionTemplate.setIsolationLevel(TransactionDefinition.ISOLATION_REPEATABLE_READ);//设置传播机制transactionTemplate.setPropagationBehavior(TransactionDefinition.PROPAGATION_REQUIRES_NEW);//设置超时时间transactionTemplate.setTimeout(1000);//设置是否只读transactionTemplate.setReadOnly(true); 可以根据 transactionTemplate 的实现原理，自己实现一个 TransactionHelper 自定义 TransactionHelper第一步：新建一个 TransactionHelper 类，进行事务管理 12345678910111213/** * 利用spring进行管理 */@Componentpublic class TransactionHelper &#123; /** * 利用spring 的机制和jdk8的function机制实现事务 */ @Transactional(rollbackFor = Exception.class) //可以根据实际业务情况，指定明确的回滚异常 public &lt;T, R&gt; R transactional(Function&lt;T, R&gt; function, T t) &#123; return function.apply(t); &#125;&#125; 第二步：直接在 service 中就可以使用了 12345678@Autowiredprivate TransactionHelper transactionHelper;/** * 调用外部的transactionHelper类，利用transactionHelper方法上面的@Transaction注解使事务生效 */public UserInfo save(Long userId) &#123; return transactionHelper.transactional((uid)-&gt;this.calculate(uid), userId);&#125; 隐式事务 / AspectJ 事务配置只需要在项目中新增一个类 AspectjTransactionConfig 即可，代码如下： 123456789101112131415161718192021222324252627@Configuration@EnableTransactionManagementpublic class AspectjTransactionConfig &#123; public static final String transactionExecution = "execution (* com.example..service.*.*(..))";//指定拦截器作用的包路径 @Autowired private PlatformTransactionManager transactionManager; @Bean public DefaultPointcutAdvisor defaultPointcutAdvisor() &#123; //指定一般要拦截哪些类 AspectJExpressionPointcut pointcut = new AspectJExpressionPointcut(); pointcut.setExpression(transactionExecution); //配置advisor DefaultPointcutAdvisor advisor = new DefaultPointcutAdvisor(); advisor.setPointcut(pointcut); //根据正则表达式，指定上面的包路径里面的方法的事务策略 Properties attributes = new Properties(); attributes.setProperty("get*", "PROPAGATION_REQUIRED,-Exception"); attributes.setProperty("add*", "PROPAGATION_REQUIRED,-Exception"); attributes.setProperty("save*", "PROPAGATION_REQUIRED,-Exception"); attributes.setProperty("update*", "PROPAGATION_REQUIRED,-Exception"); attributes.setProperty("delete*", "PROPAGATION_REQUIRED,-Exception"); //创建Interceptor TransactionInterceptor txAdvice = new TransactionInterceptor(transactionManager, attributes); advisor.setAdvice(txAdvice); return advisor; &#125;&#125; 这种方式，只要符合我们上面的正则表达规则的 service 方法，就会自动添加事务了；如果我们在方法上添加 @Transactional，也可以覆盖上面的默认规则。 不过这种方法近两年使用的团队越来越少了，因为注解的方式其实很方便，并且注解 @Transactional 的方式更容易让人理解，代码也更简单。 通过日志分析配置方法的过程一个方法经历的 SQL 和过程大致可以分为以下几个步骤。 第一步，我们在数据连接中加上 logger=Slf4JLogger&amp;profileSQL=true，用来显示 MySQL 执行的 SQL 日志 12##########datasource1 采用Mysql数据库spring.datasource1.url=jdbc:mysql://localhost:3306/test?logger=Slf4JLogger&amp;profileSQL=true 第二步，打开 Spring 的事务处理日志，用来观察事务的执行过程 1234567# Log Transactions Detailslogging.level.org.springframework.orm.jpa=DEBUGlogging.level.org.springframework.transaction=TRACElogging.level.org.hibernate.engine.transaction.internal.TransactionImpl=DEBUG# 监控连接的情况logging.level.org.hibernate.resource.jdbc=tracelogging.level.com.zaxxer.hikari=DEBUG 第三步，执行一个 saveOrUpdate 的操作，请看详细的执行日志。 通过日志可以发现，执行一个 saveUserInfo 的动作，由于在其中配置了一个事务，所以可以看到 JpaTransactionManager 获得事务的过程，图上黄色的部分是同一个连接里面执行的 SQL 语句，其执行的整体过程如下： get connection：从事务管理里面，获得连接就 begin 开始事务了。没有看到显示的 begin 的 SQL，基本上可以断定它利用了 MySQL 的 connection 初始化事务的特性。 set autocommit=0：关闭自动提交模式，这个时候必须要在程序里面 commit 或者 rollback。 select user_info：看看 user_info 数据库里面是否存在我们要保存的数据。 update user_info：发现数据库里面存在，执行更新操作。 commit：执行提交事务。 set autocommit=1：事务执行完，改回 autocommit 的默认值，每条 SQL 是独立的事务。 这里采用的是数据库默认的隔离级别，如果通过下面这行代码，改变默认隔离级别的话，再观察日志。 1@Transactional(isolation = Isolation.READ_COMMITTED) 而在事务结束之后，它还会还原此链接的事务隔离级别，又如下图所示。 Spring 事务的实现原理@Transactional 的工作机制主要是利用的 Spring 的 AOP 原理，在加载所有类的时候，容器就会知道某些类需要对应地进行哪些 Interceptor 的处理。 Spring 事务源码分析事务获得连接的关键时机在 TransactionManagementConfigurationSelector 里面设置一个断点 就会知道代理的加载类 ProxyTransactionManagementConfiguration 对事务的处理机制。关键源码如下图所示： 打开 ProxyTransactionManagementConfiguration 的话，就会加载 TransactionInterceptor 的处理类，关键源码如下图： 如果继续加载的话，里面就会加载带有 @Transactional 注解的类或者方法。关键源码如下图所示： 加载期间，通过 @Transactional 注解来确定哪些方法需要进行事务处理。 1o.s.orm.jpa.JpaTransactionManager : Creating new transaction with name 而运行期间通过上面这条日志，就可以找到 JpaTransactionManager 里面通过 getTransaction 方法创建的事务，然后再通过 debug 模式的 IDEA 线程栈进行分析，就能知道创建事务的整个过程。如下图所示： 如上图， createTransactionIfNecessary 是用来判断是否需要创建事务的，如下图所示： 继续往下面 debug 的话，就会找到创建事务的关键代码，它会通过调用 AbstractPlatformTransactionManager 里面的 startTransaction 方法开启事务。请看下图： 然后继续往下断点进行分析，当断点走到最后的时候就是开启事务的时候，必须要从数据源里面获得连接。 看一下断点的栈信息，这里有几个关键的 debug 点。如下图所示。 其中， 第一处：是处理带 @Transactional 的注解的方法，利用 CGLIB 进行事务拦截处理； 第二处：是根据 Spring 的事务传播机制，来判断是用现有的事务，还是创建新的事务； 第七处：是用来判断是否现有连接，如果有直接用，如果没有就从第八处的数据源里面的连接池中获取连接，第七处的关键代码如下： 什么时间释放连接到连接池在 LogicalConnectionManagedImpl 的 releaseConnection 方法中设置一个断点，如下图所示。 在事务执行之后，它会将连接释放到连接池里面。 通过上面的 saveOrUpdate 的详细执行日志，可以观察出来： 事务是在什么时机开启的 数据库连接是什么时机开启的 事务是在什么时机关闭的 数据库连接是在什么时机释放的 Spring 中的事务和连接的关系是，开启事务的同时获取 DB 连接；事务完成的时候释放 DB 连接。 事务和连接池在 JPA 中的注意事项数据源的连接池不能配置过大，否则连接之间切换就会非常耗费应用内部的 CPU 和内存，从而降低应用对外提供 API 的吞吐量。 所以当我们使用事务的时候，需要注意如下几个事项： 事务内的逻辑不能执行时间太长，否则就会导致占用 db 连接的时间过长，会造成数据库连接不够用的情况； 跨应用的操作，如 API 调用等，尽量不要在有事务的方法里面进行； 如果在真实业务场景中有耗时的操作，也需要带事务时（如扣款环节），那么请注意增加数据源配置的连接池数； 通过 MVC 的应用请求连接池数量，也要根据连接池的数量和事务的耗时情况灵活配置；而 tomcat 默认的请求连接池数量是 200 个，可以根据实际情况来增加或者减少请求的连接池数量，从而减少并发处理对事务的依赖。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 处理生产环境多数据源的问题]]></title>
    <url>%2F2020%2F11%2F17%2FSpringDataJpa%E5%A4%84%E7%90%86%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[生产环境多数据源的处理方法有哪些？工作中我们时常会遇到跨数据库操作的情况，这时候就需要配置多数据源： 如何配置？ 常用的方式及其背后的原理？ 常用的配置方式 通过多个 @Configuration 文件 利用 AbstractRoutingDataSource 配置多数据源 第一种方式：多个数据源的 @Configuration 的配置方法 🎯主要思路：不同 Package 下面的实体和 Repository 采用不同的 Data Source 所以改造一下 example 目录结构，来看看不同 Repositories 的数据源是怎么处理的。 第一步：规划 Entity 和 Repository 的目录结构，为了方便配置多数据源。 将 User 和 UserAddress、UserRepository 和 UserAddressRepository 移动到 db1 里面； 将 UserInfo 和 UserInfoRepository 移动到 db2 里面。 如下图所示： 把实体和 Repository 分别放到了 db1 和 db2 两个目录里面，假设数据源 1 是 MySQL，User 表和 UserAddress 表在数据源 1 里面，那么需要配置一个数据源 1 的 Configuration 类，并且在里面配置 DataSource、TransactionManager 和 EntityManager。 第二步：配置 DataSource1Config 类。 目录结构调整完之后，接下来开始配置数据源，完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Configuration@EnableTransactionManagement//开启事务//利用EnableJpaRepositories配置哪些包下面的Repositories，采用哪个EntityManagerFactory和哪个transactionManager@EnableJpaRepositories( basePackages = &#123;"com.example.jpa.example1.db1"&#125;,//数据源1的repository的包路径 entityManagerFactoryRef = "db1EntityManagerFactory",//改变数据源1的EntityManagerFactory的默认值，改为db1EntityManagerFactory transactionManagerRef = "db1TransactionManager"//改变数据源1的transactionManager的默认值，改为db1TransactionManager )public class DataSource1Config &#123; /** * 指定数据源1的dataSource配置 * @return */ @Primary @Bean(name = "db1DataSourceProperties") @ConfigurationProperties("spring.datasource1") //数据源1的db配置前缀采用spring.datasource1 public DataSourceProperties dataSourceProperties() &#123; return new DataSourceProperties(); &#125; /** * 可以选择不同的数据源，这里用HikariDataSource举例，创建数据源1 * @param db1DataSourceProperties * @return */ @Primary @Bean(name = "db1DataSource") @ConfigurationProperties(prefix = "spring.datasource.hikari.db1") //配置数据源1所用的hikari配置key的前缀 public HikariDataSource dataSource(@Qualifier("db1DataSourceProperties") DataSourceProperties db1DataSourceProperties) &#123; HikariDataSource dataSource = db1DataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build(); if (StringUtils.hasText(db1DataSourceProperties.getName())) &#123; dataSource.setPoolName(db1DataSourceProperties.getName()); &#125; return dataSource; &#125; /** * 配置数据源1的entityManagerFactory命名为db1EntityManagerFactory，用来对实体进行一些操作 * @param builder * @param db1DataSource entityManager依赖db1DataSource * @return */ @Primary @Bean(name = "db1EntityManagerFactory") public LocalContainerEntityManagerFactoryBean entityManagerFactory(EntityManagerFactoryBuilder builder, @Qualifier("db1DataSource") DataSource db1DataSource) &#123; return builder.dataSource(db1DataSource) .packages("com.example.jpa.example1.db1") //数据1的实体所在的路径 .persistenceUnit("db1")// persistenceUnit的名字采用db1 .build(); &#125; /** * 配置数据源1的事务管理者，命名为db1TransactionManager依赖db1EntityManagerFactory * @param db1EntityManagerFactory * @return */ @Primary @Bean(name = "db1TransactionManager") public PlatformTransactionManager transactionManager(@Qualifier("db1EntityManagerFactory") EntityManagerFactory db1EntityManagerFactory) &#123; return new JpaTransactionManager(db1EntityManagerFactory); &#125;&#125; 到这里，数据源 1 就配置完了，下面再配置数据源 2。 第三步：配置 DataSource2Config 类，加载数据源 2。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Configuration@EnableTransactionManagement//开启事务//利用EnableJpaRepositories，配置哪些包下面的Repositories，采用哪个EntityManagerFactory和哪个transactionManager@EnableJpaRepositories( basePackages = &#123;"com.example.jpa.example1.db2"&#125;,//数据源2的repository的包路径 entityManagerFactoryRef = "db2EntityManagerFactory",//改变数据源2的EntityManagerFactory的默认值，改为db2EntityManagerFactory transactionManagerRef = "db2TransactionManager"//改变数据源2的transactionManager的默认值，改为db2TransactionManager)public class DataSource2Config &#123; /** * 指定数据源2的dataSource配置 * * @return */ @Bean(name = "db2DataSourceProperties") @ConfigurationProperties("spring.datasource2") //数据源2的db配置前缀采用spring.datasource2 public DataSourceProperties dataSourceProperties() &#123; return new DataSourceProperties(); &#125; /** * 可以选择不同的数据源，这里我用HikariDataSource举例，创建数据源2 * * @param db2DataSourceProperties * @return */ @Bean(name = "db2DataSource") @ConfigurationProperties(prefix = "spring.datasource.hikari.db2") //配置数据源2的hikari配置key的前缀 public HikariDataSource dataSource(@Qualifier("db2DataSourceProperties") DataSourceProperties db2DataSourceProperties) &#123; HikariDataSource dataSource = db2DataSourceProperties.initializeDataSourceBuilder().type(HikariDataSource.class).build(); if (StringUtils.hasText(db2DataSourceProperties.getName())) &#123; dataSource.setPoolName(db2DataSourceProperties.getName()); &#125; return dataSource; &#125; /** * 配置数据源2的entityManagerFactory命名为db2EntityManagerFactory，用来对实体进行一些操作 * * @param builder * @param db2DataSource entityManager依赖db2DataSource * @return */ @Bean(name = "db2EntityManagerFactory") public LocalContainerEntityManagerFactoryBean entityManagerFactory(EntityManagerFactoryBuilder builder, @Qualifier("db2DataSource") DataSource db2DataSource) &#123; return builder.dataSource(db2DataSource) .packages("com.example.jpa.example1.db2") //数据2的实体所在的路径 .persistenceUnit("db2")// persistenceUnit的名字采用db2 .build(); &#125; /** * 配置数据源2的事务管理者，命名为db2TransactionManager依赖db2EntityManagerFactory * * @param db2EntityManagerFactory * @return */ @Bean(name = "db2TransactionManager") public PlatformTransactionManager transactionManager(@Qualifier("db2EntityManagerFactory") EntityManagerFactory db2EntityManagerFactory) &#123; return new JpaTransactionManager(db2EntityManagerFactory); &#125;&#125; 🤣注意：DataSource1Config 和 DataSource2Config 不同的是，1 里面每个 @Bean 都 @Primary，而 2 里面不是的。 第四步：通过 application.properties 配置两个数据源的值，代码如下： 12345678910111213141516171819###########datasource1 采用Mysql数据库spring.datasource1.url=jdbc:mysql://localhost:3306/test2?logger=Slf4JLogger&amp;profileSQL=truespring.datasource1.username=rootspring.datasource1.password=root##数据源1的连接池的名字spring.datasource.hikari.db1.pool-name=jpa-hikari-pool-db1##最长生命周期15分钟够了spring.datasource.hikari.db1.maxLifetime=900000spring.datasource.hikari.db1.maximumPoolSize=8###########datasource2 采用h2内存数据库spring.datasource2.url=jdbc:h2:~/testspring.datasource2.username=saspring.datasource2.password=sa##数据源2的连接池的名字spring.datasource.hikari.db2.pool-name=jpa-hikari-pool-db2##最长生命周期和数据源1区分开，设置成500秒spring.datasource.hikari.db2.maxLifetime=500000##最大连接池大小和数据源1区分开，配置成6个spring.datasource.hikari.db2.maximumPoolSize=6 第五步：写个 Controller 测试一下。 123456789101112131415@RestControllerpublic class UserController &#123; @Autowired private UserRepository userRepository; @Autowired private UserInfoRepository userInfoRepository; //操作user的Repository @PostMapping("/user") public User saveUser(@RequestBody User user) &#123; return userRepository.save(user); &#125; //操作userInfo的Repository @PostMapping("/user/info") public UserInfo saveUserInfo(@RequestBody UserInfo userInfo) &#123; return userInfoRepository.save(userInfo); &#125;&#125; 第六步：直接启动我们的项目，测试一下。 请看这一步的启动日志： 可以看到启动的是两个数据源，其对应的连接池的监控是不一样的：数据源 1 有 8 个，数据源 2 有 6 个。 如果分别请求 Controller 写的两个方法的时候，也会分别插入到不同的数据源里面去。 Datasource 与 TransactionManager、EntityManagerFactory 的关系分析通过一个类的关系图来分析一下： 其中， HikariDataSource 负责实现 DataSource，交给 EntityManager 和 TransactionManager 使用； EntityManager 是利用 DataSource 来操作数据库，而其实现类是 SessionImpl； EntityManagerFactory 是用来管理和生成 EntityManager 的，而 EntityManagerFactory 的实现类是 LocalContainerEntityManagerFactoryBean，通过实现 FactoryBean 接口实现，利用了 FactoryBean 在 Spring 中的 bean 管理机制，所以需要在 Datasource1Config 里面配置 LocalContainerEntityManagerFactoryBean 的 bean 的注入方式； JpaTransactionManager 是用来管理事务的，实现了 TransactionManager 并且通过 EntityManagerFactory 和 Datasource 进行 db 操作，所以要在 DataSourceConfig 里面告诉 JpaTransactionManager 用的 TransactionManager 是 db1EntityManagerFactory。 🏓关系图： DataSourceConfig -&gt; TransactionManager -&gt; EntityManagerFactory -&gt; EntityManager -&gt; DataSource ​ ↓ ↓ ↓ ​ JpaTransactionManager -&gt; LocalContainerEntityManagerFactoryBean -&gt; .. -&gt; HikariDataSource 默认情况下 Datasource 的 EntityManagerFactory 和 TransactionManager 是怎么加载和配置的呢？ 默认的 JpaBaseConfiguration 的加载方式分析可以通过 HibernateJpaConfiguration，找到父类 JpaBaseConfiguration 类，如图所示： 接着打开 JpaBaseConfiguration 就可以看到多数据源的参考原型，如下图所示： 通过上面的代码，可以看到在单个数据源情况下的 EntityManagerFactory 和 TransactionManager 的加载方法， 并且我们在多数据源的配置里面还加载了一个类：EntityManagerFactoryBuilder ，正是从上面的方法加载进去的。 第二种方式：利用 AbstractRoutingDataSource 配置多数据源DataSource 的本质是获得数据库连接，而 AbstractRoutingDataSource 帮我们实现了动态获得数据源的可能性。 第一步：定一个数据源的枚举类，用来标示数据源有哪些。 123456789101112131415/** * 定义一个数据源的枚举类 */public enum RoutingDataSourceEnum &#123; DB1, // 实际工作中枚举的语义可以更加明确一点； DB2; public static RoutingDataSourceEnum findByCode(String dbRouting) &#123; for (RoutingDataSourceEnum e : values()) &#123; if (e.name().equals(dbRouting)) &#123; return e; &#125; &#125; return db1;//没找到的情况下，默认返回数据源1 &#125;&#125; 第二步：新增 DataSourceRoutingHolder，用来存储当前线程需要采用的数据源。 123456789101112131415/** * 利用 ThreadLocal 来存储，当前的线程使用的数据 */public class DataSourceRoutingHolder &#123; private static ThreadLocal&lt;RoutingDataSourceEnum&gt; threadLocal = new ThreadLocal&lt;&gt;(); public static void setBranchContext(RoutingDataSourceEnum dataSourceEnum) &#123; threadLocal.set(dataSourceEnum); &#125; public static RoutingDataSourceEnum getBranchContext() &#123; return threadLocal.get(); &#125; public static void clearBranchContext() &#123; threadLocal.remove(); &#125;&#125; 第三步：配置 RoutingDataSourceConfig，用来指定哪些 Entity 和 Repository 采用动态数据源。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Configuration@EnableTransactionManagement@EnableJpaRepositories( //数据源的 repository 的包路径，这里我们覆盖 db1 和 db2 的包路径 basePackages = &#123;"com.example.jpa.example1"&#125;, entityManagerFactoryRef = "routingEntityManagerFactory", transactionManagerRef = "routingTransactionManager")public class RoutingDataSourceConfig &#123; @Autowired @Qualifier("db1DataSource") private DataSource db1DataSource; @Autowired @Qualifier("db2DataSource") private DataSource db2DataSource; /** * 创建RoutingDataSource，引用我们之前配置的db1DataSource和db2DataSource * * @return */ @Bean(name = "routingDataSource") public DataSource dataSource() &#123; Map&lt;Object, Object&gt; dataSourceMap = Maps.newHashMapWithExpectedSize(2); dataSourceMap.put(RoutingDataSourceEnum.DB1, db1DataSource); dataSourceMap.put(RoutingDataSourceEnum.DB2, db2DataSource); RoutingDataSource routingDataSource = new RoutingDataSource(); //设置RoutingDataSource的默认数据源 routingDataSource.setDefaultTargetDataSource(db1DataSource); //设置RoutingDataSource的数据源列表 routingDataSource.setTargetDataSources(dataSourceMap); return routingDataSource; &#125; /** * 类似db1和db2的配置，唯一不同的是，这里采用routingDataSource * @param builder * @param routingDataSource entityManager依赖routingDataSource * @return */ @Bean(name = "routingEntityManagerFactory") public LocalContainerEntityManagerFactoryBean entityManagerFactory(EntityManagerFactoryBuilder builder, @Qualifier("routingDataSource") DataSource routingDataSource) &#123; return builder.dataSource(routingDataSource) .packages("com.example.jpa.example1") //数据routing的实体所在的路径，这里我们覆盖db1和db2的路径 .persistenceUnit("db-routing")// persistenceUnit的名字采用db-routing .build(); &#125; /** * 配置数据的事务管理者，命名为 routingTransactionManager 依赖 routingEntityManagerFactory * * @param routingEntityManagerFactory * @return */ @Bean(name = "routingTransactionManager") public PlatformTransactionManager transactionManager(@Qualifier("routingEntityManagerFactory") EntityManagerFactory routingEntityManagerFactory) &#123; return new JpaTransactionManager(routingEntityManagerFactory); &#125;&#125; 路由数据源配置与 DataSource1Config 和 DataSource2Config 有相互覆盖关系，这里覆盖 db1 和 db2 的包路径，以便于动态数据源生效。 第四步：写一个 MVC 拦截器，用来指定请求分别采用什么数据源。 新建一个类 DataSourceInterceptor，用来在请求前后指定数据源，请看代码： 123456789101112131415161718192021222324/** * 动态路由的实现逻辑，我们通过请求里面的db-routing，来指定此请求采用什么数据源 */@Componentpublic class DataSourceInterceptor extends HandlerInterceptorAdapter &#123; /** * 请求处理之前更改线程里面的数据源 */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; String dbRouting = request.getHeader("db-routing"); DataSourceRoutingHolder.setBranchContext(RoutingDataSourceEnum.findByCode(dbRouting)); return super.preHandle(request, response, handler); &#125; /** * 请求结束之后清理线程里面的数据源 */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; super.afterCompletion(request, response, handler, ex); DataSourceRoutingHolder.clearBranchContext(); &#125;&#125; 同时需要在实现 WebMvcConfigurer 的配置里面，把我们自定义拦截器 dataSourceInterceptor 加载进去，代码如下： 123456789101112131415/** * 实现WebMvcConfigurer */@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer &#123; @Autowired private DataSourceInterceptor dataSourceInterceptor; //添加自定义拦截器 @Override public void addInterceptors(InterceptorRegistry registry) &#123; registry.addInterceptor(dataSourceInterceptor).addPathPatterns("/**"); WebMvcConfigurer.super.addInterceptors(registry); &#125;......&#125; 此处采用的是 MVC 的拦截器机制动态改变的数据配置，你也可以使用自己的 AOP 任意的拦截器，如事务拦截器、Service 的拦截器等，都可以实现。 🏓注意：要在开启事务之前配置完毕 第五步：启动测试。 在 Http 请求头里面加上 db-routing：DB2，那么本次请求就会采用数据源 2 进行处理，请求代码如下： 1234567POST /user/info HTTP/1.1Host: 127.0.0.1:8089Content-Type: application/jsondb-routing: DB2Cache-Control: no-cachePostman-Token: 56d8dc02-7f3e-7b95-7ff1-572a4bb7d102&#123;"ages":10&#125; 通过上面五个步骤，可以利用 AbstractRoutingDataSource 实现动态数据源，实际工作中可能更复杂，可能需要考虑多线程、线程安全等问题。 多数据源的注意事项 这种方式利用了当前线程事务不变的原理，所以要注意异步线程的处理方式； 这种方式利用了 DataSource 的原理，动态地返回不同的 db 连接，一般需要在开启事务之前使用，需要注意事务的生命周期； 比较适合读写操作分开的业务场景； 多数据的情况下，避免一个事务里面采用不同的数据源，这样会有意想不到的情况发生，比如死锁现象； 学会通过日志检查开启请求的方法和开启的数据源是否正确，可以通过 Debug 断点来观察数据源是否选择的正确，如下图所示： 微服务下的建议在实际工作中，为了便捷省事，更多开发者喜欢配置多个数据源，但是强烈建议不要在对用户直接提供的 API 服务上面配置多数据源，否则将出现令人措手不及的 Bug。 如果是做后台管理界面，供公司内部员工使用的，那么这种 API 可以为了方便而使用多数据源。 微服务的大环境下，服务越小，内聚越高，低耦合服务越健壮，所以一般跨库之间通过 REST 的 API 协议，进行内部服务之间的调用，是最稳妥的方式，原因有如下几点： REST 的 API 协议更容易监控，更容易实现事务的原子性； db 之间解耦，使业务领域代码职责更清晰，更容易各自处理各种问题； 只读和读写的 API 更容易分离和管理。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[系统架构演化]]></title>
    <url>%2F2020%2F11%2F15%2F%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E6%BC%94%E5%8C%96%2F</url>
    <content type="text"><![CDATA[系统架构系统技术挑战与方案互联网系统面临怎样的挑战? 需要面对高并发用户，大流量访问 Google 日均 PV 数 35 亿，日均 IP 访问数 3 亿 微信在线用户数 10 亿 天猫双十一活动一天交易额 3000 亿 高可用 系统 7×24 小时不间断服务，大型互联网站的宕机事件通常会成为新闻焦点 需要存储、管理海量数据 Facebook 每周上传的照片数目接近10亿 百度收录的网页数目有数百亿 Google 有近百万台服务器为全球用户提供服务 用户分布广泛，网络情况复杂 许多大型互联网都是为全球用户提供服务的，用户分布范围广，各地网络情况千差万别。 在国内，有各个运营商网络互通难的问题。而中美光缆的数次故障，也让一些对国外用户依赖较大的网站不得不考虑在海外建立数据中心 安全环境恶劣 由于互联网的开放性，使得互联网站更容易受到攻击，大型网站几乎每天都会遇到黑客攻击情况。2011年国内多个重要网站泄露用户密码，让普通用户也直面一次互联网安全问题 需求快速变更，发布频繁 和传统软件的版本发布频率不同，互联网产品为快速适应市场，满足用户需求，其产品发布频率也是极高的。Office 的产品版本以年为单位发布，而一般大型网站的产品每周都有新版本发布上线，至于中小型网站的发布就更频繁了，有时候一天会发布几十次 渐进式发展 不同于传统软件产品或者企业应用系统，一开始就规划好全部的功能和非功能需求，几 乎所有的大型互联网站都是从一个小网站开始，渐进的发展起来的。 好的互联网产品都是慢慢运营出来的，不是一开始就开发好的。那些刚建立就投入巨资， 有巨大背景的网站，后来发展都很惨淡。 应对高并发挑战的两个技术方向垂直伸缩​ 通过升级硬件和网络吞吐能力可以实现垂直伸缩。由于不需要改变应用架构，所以通常 被认为是最简单的短期伸缩性方案。 通过使用 RAID（独立冗余磁盘阵列）增加 I/O 吞吐能力 通过切换到 SSD（固态硬盘）改善 I/O 访问速度 通过增加内存减少 I/O 操作 通过升级网络接口或者增加网络接口提高网络吞吐能力 更新服务器使用更多处理器或者更多超线程 缺点达到某个程度后，增加计算能力需要的更多的花费。 垂直伸缩有物理极限。 操作系统的设计或者应用程序自身制约着垂直伸缩最多只能达到某个点。 水平伸缩水平伸缩是指通过增加服务器提升计算能力的一类架构方法。 水平伸缩被认为是伸缩性的圣杯，水平伸缩可以克服垂直伸缩带来的单位计算成本随计算 能力增加而迅速飙升的问题。 另外，水平伸缩总是可以增加更多服务器，这样，就不会像垂直伸缩那样遭遇到单台服 务器的极限。 互联网架构演化第零阶段：最简单的互联网应用架构一台服务器里面完成所有的事情 第一阶段：应用数据分离 第二阶段：使用缓存改善系统性能 第三阶段：使用应用服务器集群改善系统的并发处理能力 第四阶段：数据库读写分离 第五阶段：使用反向代理和 CDN 加速网站响应 第六阶段：使用分布式文件系统和分布式数据库系统 第七阶段：使用 NoSQL 和搜索引擎 第八阶段：业务拆分 第九阶段：微服务及中台化 第十阶段 大数据与智能化互联网架构模式每一个模式描述了一个在我们周围不断重复发生的问题以及该问题解决方案的核心。这 样，你就能一次又一次的使用该方案而不必做重复工作。 模式的关键在于模式的可重复性，问题与场景的可重复性带来解决方案的可重复使用。 互联网架构模式就是试图去描述那些为解决互联网系统高性能、高可用、易扩展、可伸 缩、安全等目标，被很多互联网应用重复使用的一些解决方案，这些解决方案是互联网 软件系统的重要组成部分。 分层分层是企业应用系统中最常见的一种架构模式，将系统在横向维度上切分成几个部分， 每个部分负责一部分相对比较单一的职责，然后通过上层对下层依赖和调用组成一个完 整的系统。 分割如果说分层是将软件在横向方面进行切分，那么分割就是在纵向方面对软件进行切分。 系统越大，功能越复杂，服务和数据处理的种类也越多，将这些不同的功能和服务分割开来，包装成高内聚低耦合的模块单元，一方面有助于软件的开发和维护；另一方面， 便于不同模块的分布式部署，提高网站的并发处理能力和功能扩展能力。 分布式对于大型网站，分层和分割的一个主要目的是为了切分后的模块便于分布式部署，就是将不同模块部署在不同的服务器上，通过远程调用协同工作。分布式意味着解决同样的问题可以使用更多的计算机，计算机越多，CPU、内存、存储资源也就越多，能够处理的并发访问和数据量就越大。 分布式应用和服务 分布式静态资源 分布式数据和存储 分布式计算 集群使用分布式虽然已经将分层和分割后的模块独立部署，但是对于用户访问集中的模块， 比如网站的首页，还需要将独立部署的服务器集群化，即多台服务器部署相同应用构成一个集群，通过负载均衡设备共同对外提供服务。 缓存缓存就是将数据存放在距离计算最近的位置以加快处理速度。缓存是改善软件性能的第一手段，现代 CPU 越来越快的一个重要因素就是使用了更多的缓存，在复杂的软件设计中，缓存几乎无处不在。大型网站架构设计在很多方面都使用了缓存设计。 CDN 反向代理 本地缓存 远程缓存 异步计算机软件发展的一个重要目标和驱动力是降低软件耦合性。事物之间越少直接关系， 那么就越少被彼此影响，越可以独立发展。大型网站架构中，系统解耦合的手段除了前面提到的分层、分割、分布等手段，还有一个重要手段是异步，就是将一个业务操作分成多个阶段，每个阶段之间通过共享数据而不是直接调用的方法进行协作。 提高系统可用性 加快网站响应速度 消除并发访问高峰 冗余互联网应用需要 7×24 小时连续运行，但是服务器总有可能会出现故障，特别是服务器规模比较大的时候，服务器宕机是必然事件。要想保证在服务器宕机的情况下网站依然可以继续服务，数据不会丢失，就需要一定程度的服务器冗余运行，数据冗余备份。 自动化在无人值守的情况下网站可以正常运行，一切都可以自动化是网站的理想状态。目前互联网的自动化架构设计主要集中在运维方面。 安全互联网的开放特性使得其从诞生起就面对巨大的安全挑战，网站在安全架构方面也积累了许多模式：通过密码和手机校验码进行身份认证；登录、交易等操作需要对网络通讯进行加密，网站服务器上存储的敏感数据如用户信息等也进行加密处理；为了防止机器人程序滥用网络资源供给网站，网站使用验证码进行识别；对于常见的用于攻击网站的 XSS 攻击，SQL 注入，进行编码转换等相应处理；对于垃圾信息、敏感信息进行过滤； 对转账交易等重要操作根据交易模式和交易信息进行风险控制。 如何衡量一个系统的架构设计高性能性能是互联网的一个重要指标，除非是没得选择，否则用户无法忍受一个响应缓慢的应用。一个打开缓慢应用会导致严重的用户流失，很多时候系统性能问题是系统架构升级优化的触发器。可以说性能是互联网系统架构设计的一个重要方面，任何架构设计方案都必须考虑可能会带来的性能问题。 也正是因为性能问题几乎无处不在，所以优化网站性能的手段也非常多，从用户端到数据库，从代码到机房部署，影响用户请求的所有环节都可以进行性能优化。 高可用因为互联网分布式系统使用的服务器硬件通常是普通的商用服务器，这些服务器的设计目标本身并不保证高可用，也就是说，很有可能会出现服务器硬件故障，也就是俗称的服务器宕机。大型互联网系统通常都会有上万台服务器，每天都必定会有一些服务器宕机，因此系统高可用架构设计的前提是必然会出现服务器宕机，而高可用设计的目标就是当服务器宕机的时候，服务或者应用依然可用。 系统高可用的主要手段是冗余，应用部署在多台服务器上同时提供访问，数据存储在多台服务器上互相备份，任何一台服务器宕机都不会影响应用的整体可用，也不会导致数据丢失。 可伸缩大型互联网应用通过集群的方式将多台服务器组成一个整体共同提供服务。所谓伸缩性是指通过不断向集群中加入服务器的手段来缓解不断上升的用户并发访问压力和不断增长的数据存储需求。 衡量架构伸缩性的主要标准就是 是否可以用多台服务器构建集群 是否容易向集群中添加新的服务器 加入新的服务器后是否可以提供和原来的服务器无差别的服务 集群中可容纳的总的服务器数量是否有限制 可扩展不同于其他架构要素主要关注非功能性需求，扩展性架构直接关注系统的功能需求。互联网应用快速发展，功能不断扩展，如何设计系统的架构使其能够快速响应需求变化， 是系统可扩展架构主要的目的。 衡量系统架构扩展性好坏的主要标准就是在系统增加新的业务产品时 是否可以实现对现有产品透明无影响，不需要任何改动或者很少改动既有业务功能就可以上线新产品。 不同产品之间是否很少耦合，一个产品改动对其他产品无影响，其他产品和功能不需要 受牵连进行改动。 可扩展架构的主要手段是事件驱动架构和分布式服务。 安全互联网是开放的，任何人在任何地方都可以访问系统。系统的安全架构就是保护系统不受恶意访问和攻击，保护网站的重要数据不被窃取。 衡量系统安全架构的标准就是针对现存和潜在的各种攻击与窃密手段，是否有可靠的应对策略。 互联网架构技术 前端架构 App 及 Web 开发技术 浏览器及 HTTP 优化技术 CDN 动静分离 图片服务 反向代理 DNS 网关及应用层架构 网关架构 负载均衡 动态页面静态化 业务拆分 服务层架构 微服务框架 分布式消息队列 分布式缓存 分布式一致性（锁）服务 存储层架构 分布式文件 分布式关系数据库 NoSQL 数据库 后台架构 大数据平台 搜索引擎 推荐引擎 数据仓库 运维与安全 数据采集与展示 数据监控与报警 攻击与防护 数据加密与解密]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 自定义参数解析器]]></title>
    <url>%2F2020%2F11%2F13%2FSpringDataJpa%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[自定义 HandlerMethodArgumentResolver参数解析器 – Method Argument Resolver SpringDataWebConfiguration 类是如何被加载的 PageableHandlerMethodArgumentResolver 是如何生效的 SortHandlerMethodArgumentResolver 是如何生效的 如何定义自己的 HandlerMethodArgumentResolvers 类 还有没有其他 Web 场景需要我们自定义呢 Page 和 Sort 参数原理是 @EnableSpringDataWebSupport 注解将SpringDataWebConfiguration这个类加载进去的，这个类里面把分页和排序的参数加载进去，关键代码如下所示： 其中，@EnableSpringDataWebSupport 注解是 Spring Data JPA 对 Web 支持需要开启的入口，由于我们使用的是 Spring Boot，所以 @EnableSpringDataWebSupport 不需要我们手动去指定。这是由于 Spring Boot 有自动加载的机制，SpringDataWebAutoConfiguration 类里面引用了 @EnableSpringDataWebSupport 的注解，所以也不需要手动去引用了。关键代码如下所示： 而 Spring Boot 的自动加载的核心文件就是 spring.factories 文件，那么我们打开 spring-boot-autoconfigure-2.3.3.jar 包，看一下 spring.factories 文件内容，可以找到 SpringDataWebAutoConfiguration 这个配置类，如下： 结论：只要是 Spring Boot 项目，什么都不需要做，它会天然地让 Spring Data JPA 支持 Web 相关的操作。 因为 PageableHandlerMethodArgumentResolver 和 SortHandlerMethodArgumentResolver 两个类是通过 SpringDataWebConfiguration 加载进去的，所以基本可以知道 Spring Data JPA 的 Page 和 Sort 参数是因为 SpringDataWebConfiguration 里面 @Bean 的注入才生效的。 通过 PageableHandlerMethodArgumentResolver 和 SortHandlerMethodArgumentResolver 这两个类的源码，可以知道它们实现了 org.springframework.web.method.support.HandlerMethodArgumentResolver 这个接口，从而对 Request 里面的 Page 和 Sort 的参数做了处理逻辑和解析逻辑。 在实际工作中，可能存在特殊情况需要对其进行扩展，比如 Page 的参数可能需要支持多种 Key 的情况，那么应该怎么做呢？ HandlerMethodArgumentResolver 用法接口方法详解熟悉 MVC 的人都知道，HandlerMethodArgumentResolvers 在 Spring MVC 中的主要作用是对 Controller 里面的方法参数做解析，即可以把 Request 里面的值映射到方法的参数中。打开此类的源码会发现只有两个方法，如下所示： 12345678public interface HandlerMethodArgumentResolver &#123; //检查方法的参数是否支持处理和转化 boolean supportsParameter(MethodParameter parameter); //根据request上下文，解析方法的参数 @Nullable Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception;&#125; 此接口的应用场景非常广泛，可以看到其子类非常多，如下图所示： 其中几个类的作用如下： PathVariableMapMethodArgumentResolver 专门解析 @PathVariable 里面的值； RequestResponseBodyMethodProcessor 专门解析 @RequestBody 注解的方法参数的值； RequestParamMethodArgumentResolver 专门解析 @RequestParam 的注解参数的值，当方法的参数中没有任何注解的时候，默认是 @RequestParam； PageableHandlerMethodArgumentResolver 专门解析 @PageableDefault Pageable 的值 SortHandlerMethodArgumentResolver 与 HttpMessageConverter 的关系打开 RequestResponseBodyMethodProcessor 就会发现，这个类中主要处理的是，方法里面带 @RequestBody 注解的参数，如下图所示： 而其中的 readWithMessageConverters(webRequest, parameter, parameter.getNestedGenericParameterType()) 方法，如果我们点进去继续观察，发现里面会根据 Http 请求的 MediaType，来选择不同的 HttpMessageConverter 进行转化。 不同的 HttpMessageConverter 都是由 RequestResponseBodyMethodProcessor 进行调用的 HttpMessageConverter 的执行顺序当我们自定义 HandlerMethodArgumentResolver 时，通过下面的方法加载进去。 1234@Overridepublic void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; resolvers) &#123; resolvers.add(myPageableHandlerMethodArgumentResolver);&#125; 在 List&lt;HandlerMethodArgumentResolver&gt; 里面自定义的 resolver 的优先级是最高的，也就是会优先执行 HandlerMethodArgumentResolver 之后，才会按照顺序执行系统里面自带的那一批 HttpMessageConverter，按照 List 的顺序执行。 Spring 里面有个执行效率问题，就是一旦一次执行找到了需要的 HandlerMethodArgumentResolver 的时候，利用 Spring 中的缓存机制，执行过程中就不会再遍历 List&lt;HandlerMethodArgumentResolver&gt; 了，而是直接用上次找到的 HandlerMethodArgumentResolver，这样提升了执行效率。 如果想要了解更多的 Resolver，可以看下图这个类的代码： 如何自定义 HandlerMethodArgumentResolver在实际的工作中，可能会遇到对老项目进行改版的工作，如果要我们把旧的 API 接口改造成 JPA 的技术实现，那么可能会出现需要新、老参数的问题。假设在实际场景中，我们 Page 的参数是 page[number]，而 page size 的参数是 page[size]，看看应该怎么做。 第一步：新建 MyPageableHandlerMethodArgumentResolver 这个类的作用有两个： 用来兼容 ?page[size]=2&amp;page[number]=0 的参数情况； 支持 JPA 新的参数形式 ?size=2&amp;page=0。 通过自定义的 MyPageableHandlerMethodArgumentResolver 来实现这个需求，请看下面这段代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * 通过@Component把此类加载到Spring的容器里面去 */@Componentpublic class MyPageableHandlerMethodArgumentResolver extends PageableHandlerMethodArgumentResolver implements HandlerMethodArgumentResolver &#123; //我们假设sort的参数没有发生变化，采用PageableHandlerMethodArgumentResolver里面的写法 private static final SortHandlerMethodArgumentResolver DEFAULT_SORT_RESOLVER = new SortHandlerMethodArgumentResolver(); //给定两个默认值 private static final Integer DEFAULT_PAGE = 0; private static final Integer DEFAULT_SIZE = 10; //兼容新版，引入JPA的分页参数 private static final String JPA_PAGE_PARAMETER = "page"; private static final String JPA_SIZE_PARAMETER = "size"; //兼容原来老的分页参数 private static final String DEFAULT_PAGE_PARAMETER = "page[number]"; private static final String DEFAULT_SIZE_PARAMETER = "page[size]"; private SortArgumentResolver sortResolver; //模仿 PageableHandlerMethodArgumentResolver 里面的构造方法 public MyPageableHandlerMethodArgumentResolver(@Nullable SortArgumentResolver sortResolver) &#123; this.sortResolver = sortResolver == null ? DEFAULT_SORT_RESOLVER : sortResolver; &#125; @Override public boolean supportsParameter(MethodParameter parameter) &#123;// 假设用我们自己的类 MyPageRequest 接收参数 return MyPageRequest.class.equals(parameter.getParameterType()); //同时我们也可以支持通过Spring Data JPA里面的Pageable参数进行接收，两种效果是一样的// return Pageable.class.equals(parameter.getParameterType()); &#125; /** * 参数封装逻辑page和sort，JPA参数的优先级高于page[number]和page[size]参数 */ //public Pageable resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) &#123; //这种是Pageable的方式 @Override public MyPageRequest resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) &#123; String jpaPageString = webRequest.getParameter(JPA_PAGE_PARAMETER); String jpaSizeString = webRequest.getParameter(JPA_SIZE_PARAMETER); //我们分别取参数里面page、sort和 page[number]、page[size]的值 String pageString = webRequest.getParameter(DEFAULT_PAGE_PARAMETER); String sizeString = webRequest.getParameter(DEFAULT_SIZE_PARAMETER); //当两个都有值时候的优先级，及其默认值的逻辑 Integer page = jpaPageString != null ? Integer.valueOf(jpaPageString) : pageString != null ? Integer.valueOf(pageString) : DEFAULT_PAGE; //在这里同时可以计算 page+1的逻辑;如：page=page+1; Integer size = jpaSizeString != null ? Integer.valueOf(jpaSizeString) : sizeString != null ? Integer.valueOf(sizeString) : DEFAULT_SIZE; //我们假设，sort排序的取值方法先不发生改变 Sort sort = sortResolver.resolveArgument(parameter, mavContainer, webRequest, binderFactory);// 如果使用 Pageable 参数接收值，我们也可以不用自定义 MyPageRequest 对象，直接返回 PageRequest;// return PageRequest.of(page,size,sort); //将 page 和 size 计算出来的记过封装到我们自定义的 MyPageRequest 类里面去 MyPageRequest myPageRequest = new MyPageRequest(page, size,sort); //返回 controller 里面的参数需要的对象； return myPageRequest; &#125;&#125; 代码的逻辑就是取 Request 的 Page 相关的参数，封装到对象中返回给 Controller 的方法参数里面。 其中 MyPageRequest 不是必需的，只是为了演示不同的做法。 第二步：新建 MyPageRequest 12345678/** * 继承父类，可以省掉很多计算 page 和 index 的逻辑 */public class MyPageRequest extends PageRequest &#123; protected MyPageRequest(int page, int size, Sort sort) &#123; super(page, size, sort); &#125;&#125; 此类，我们用来接收 Page 相关的参数值，也不是必需的。 第三步：implements WebMvcConfigurer 加载 myPageableHandlerMethodArgumentResolver 12345678910111213141516/** * 实现 WebMvcConfigurer */@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer &#123; @Autowired private MyPageableHandlerMethodArgumentResolver myPageableHandlerMethodArgumentResolver; /** * 覆盖这个方法，把我们自定义的 myPageableHandlerMethodArgumentResolver 加载到原始的 mvc 的 resolvers 里面去 * @param resolvers */ @Override public void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; resolvers) &#123; resolvers.add(myPageableHandlerMethodArgumentResolver); &#125;&#125; 这里利用 Spring MVC 的机制加载我们自定义的 myPageableHandlerMethodArgumentResolver，由于自定义的优先级是最高的，所以用 MyPageRequest.class 和 Pageable.class 都是可以的 第四步： Controller 里面的写法 12345678910//用 Pageable 这种方式也是可以的@GetMapping("/users")public Page&lt;UserInfo&gt; queryByPage(Pageable pageable, UserInfo userInfo) &#123; return userInfoRepository.findAll(Example.of(userInfo),pageable);&#125;//用 MyPageRequest 进行接收@GetMapping("/users/mypage")public Page&lt;UserInfo&gt; queryByMyPage(MyPageRequest pageable, UserInfo userInfo) &#123; return userInfoRepository.findAll(Example.of(userInfo),pageable);&#125; 这里利用 Pageable 和 MyPageRequest 两种方式都是可以的。 第五步：启动项目测试一下 依次可以测试下面两种情况，发现都是可以正常工作的。 1GET http://localhost:8089/users?page[size]=2&amp;page[number]=0&amp;ages=10&amp;sort=id,desc 1GET http://localhost:8089/users?size=2&amp;page=0&amp;ages=10&amp;sort=id,desc 1GET http://localhost:8089/users/mypage?page[size]=2&amp;page[number]=0&amp;ages=10&amp;sort=id,desc 1GET http://localhost:8089/users/mypage?size=2&amp;page=0&amp;ages=10&amp;sort=id,desc 演示的 Controller 方法里面有多个参数的，每个参数都各司其职，找到自己对应的 HandlerMethodArgumentResolver，这正是 Spring MVC 框架的优雅之处。 实际工作的建议自定义 HandlerMethodArgumentResolver 到底对我们的工作起到哪些作用呢？ 场景一当我们在 Controller 里面处理某些参数时，重复的步骤非常多，那么我们就可以考虑写一下自己的框架，来处理请求里面的参数，而 Controller 里面的代码就会变得非常优雅，不需要关心其他框架代码，只要知道方法的参数有值就可以了。 场景二再举个例子，在实际工作中需要注意的是，默认 JPA 里面的 Page 是从 0 开始，而我们可能有些老的代码也要维护，因为老的代码大多数的 Page 都会从 1 开始。如果我们不自定义 HandlerMethodArgumentResolver，那么在用到分页时，每个 Controller 的方法里面都需要关心这个逻辑。那么这个时候你就应该想到上面列举的自定义 MyPageableHandlerMethodArgumentResolver 的 resolveArgument 方法的实现，使用这种方法我们只需要在里面修改 Page 的计算逻辑即可。 场景三再举个例子，在实际的工作中，还经常会遇到“取当前用户”的应用场景。此时，普通做法是，当使用到当前用户的 UserInfo 时，每次都需要根据请求 header 的 token 取到用户信息，伪代码如下所示： 1234567@PostMapping("user/info")public UserInfo getUserInfo(@RequestHeader String token) &#123; // 伪代码 Long userId = redisTemplate.get(token); UserInfo useInfo = userInfoRepository.getById(userId); return userInfo;&#125; 如果我们使用 HandlerMethodArgumentResolver 接口来实现，代码就会变得优雅许多。伪代码如下： 实现 HandlerMethodArgumentResolver 接口 12345678910111213141516171819202122@Componentpublic class UserInfoArgumentResolver implements HandlerMethodArgumentResolver &#123; private final RedisTemplate redisTemplate;//伪代码，假设我们token是放在redis里面的 private final UserInfoRepository userInfoRepository; public UserInfoArgumentResolver(RedisTemplate redisTemplate, UserInfoRepository userInfoRepository) &#123; this.redisTemplate = redisTemplate;//伪代码，假设我们token是放在redis里面的 this.userInfoRepository = userInfoRepository; &#125; @Override public boolean supportsParameter(MethodParameter parameter) &#123; return UserInfo.class.isAssignableFrom(parameter.getParameterType()); &#125; @Override public Object resolveArgument(MethodParameter parameter, ModelAndViewContainer mavContainer, NativeWebRequest webRequest, WebDataBinderFactory binderFactory) throws Exception &#123; HttpServletRequest nativeRequest = (HttpServletRequest) webRequest.getNativeRequest(); String token = nativeRequest.getHeader("token"); Long userId = (Long) redisTemplate.opsForValue().get(token);//伪代码，假设我们token是放在redis里面的 UserInfo useInfo = userInfoRepository.getOne(userId); return useInfo; &#125;&#125; 只需要在 MyWebMvcConfigurer 里面把 userInfoArgumentResolver 添加进去即可，关键代码如下： 12345678910111213@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer &#123; @Autowired private MyPageableHandlerMethodArgumentResolver myPageableHandlerMethodArgumentResolver; @Autowired private UserInfoArgumentResolver userInfoArgumentResolver; @Override public void addArgumentResolvers(List&lt;HandlerMethodArgumentResolver&gt; resolvers) &#123; resolvers.add(myPageableHandlerMethodArgumentResolver); //我们只需要把userInfoArgumentResolver加入resolvers中即可 resolvers.add(userInfoArgumentResolver); &#125;&#125; 在 Controller 中使用 12345678910111213@RestControllerpublic class UserInfoController &#123; //获得当前用户的信息 @GetMapping("user/info") public UserInfo getUserInfo(UserInfo userInfo) &#123; return userInfo; &#125; //给当前用户 say hello @PostMapping("sayHello") public String sayHello(UserInfo userInfo) &#123; return "hello " + userInfo.getTelephone(); &#125;&#125; 上述代码可以看到，在 Controller 里面可以完全省掉根据 token 从 redis 取当前用户信息的过程，优化了操作流程。 场景四有的时候我们也会更改 Pageable 的默认值和参数的名字，也可以在 application.properties 的文件里面通过如下的 Key 值对自定义进行配置，如下图所示： 思路拓展WebMvcConfigurer 介绍当我们做 Spring 的 MVC 开发的时候，可能会通过实现 WebMvcConfigurer 去做一些公用的业务逻辑，下面我列举几个常见的方法 12345678910111213141516/** 拦截器配置 */void addInterceptors(InterceptorRegistry var1);/** 视图跳转控制器 */void addViewControllers(ViewControllerRegistry registry);/** 静态资源处理 */void addResourceHandlers(ResourceHandlerRegistry registry);/** 默认静态资源处理器 */void configureDefaultServletHandling(DefaultServletHandlerConfigurer configurer);/** 这里配置视图解析器 */void configureViewResolvers(ViewResolverRegistry registry);/** 配置内容裁决的一些选项 */void configureContentNegotiation(ContentNegotiationConfigurer configurer);/** 解决跨域问题 */void addCorsMappings(CorsRegistry registry);/** 添加对 controller 的 Return 的结果的处理 */void addReturnValueHandlers(List&lt;HandlerMethodReturnValueHandler&gt; handlers); 当我们实现 Restful 风格的 API 协议时，会经常看到其对 json 响应结果进行了统一的封装，我们也可以采用 HandlerMethodReturnValueHandler 来实现，再来看一个例子。 用 Result 对 JSON 的返回结果进行统一封装下面通过五个步骤来实现一个通过自定义注解，利用HandlerMethodReturnValueHandler 实现 JSON 结果封装的例子。 第一步：我们自定义一个注解 @WarpWithData，表示此注解包装的返回结果用 Data 进行包装，代码如下： 12345@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documented/** 自定义一个注解对返回结果进行包装 */public @interface WarpWithData &#123;&#125; 第二步：自定义 MyWarpWithDataHandlerMethodReturnValueHandler，并继承 RequestResponseBodyMethodProcessor 来实现 HandlerMethodReturnValueHandler 接口，用来处理 Data 包装的结果，代码如下： 1234567891011121314151617181920//自定义自己的 return 的处理类，我们直接继承 RequestResponseBodyMethodProcessor，这样父类里面的方法我们直接使用就可以了@Componentpublic class MyWarpWithDataHandlerMethodReturnValueHandler extends RequestResponseBodyMethodProcessor implements HandlerMethodReturnValueHandler &#123; //参考父类 RequestResponseBodyMethodProcessor 的做法 public MyWarpWithDataHandlerMethodReturnValueHandler(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; super(converters); &#125; //只处理需要包装的注解的方法 @Override public boolean supportsReturnType(MethodParameter returnType) &#123; return returnType.hasMethodAnnotation(WarpWithData.class); &#125; //将返回结果包装一层 Data @Override public void handleReturnValue(Object returnValue, MethodParameter methodParameter, ModelAndViewContainer modelAndViewContainer, NativeWebRequest nativeWebRequest) throws IOException, HttpMediaTypeNotAcceptableException &#123; Map&lt;String,Object&gt; res = new HashMap&lt;&gt;(); res.put("data",returnValue); super.handleReturnValue(res,methodParameter,modelAndViewContainer,nativeWebRequest); &#125;&#125; 第三步：在 MyWebMvcConfigurer 里面直接把 myWarpWithDataHandlerMethodReturnValueHandler 加入 handlers 里面即可，也是通过覆盖父类 WebMvcConfigurer 里面的 addReturnValueHandlers 方法完成的，关键代码如下： 1234567891011121314151617181920@Configurationpublic class MyWebMvcConfigurer implements WebMvcConfigurer &#123; @Autowired private MyWarpWithDataHandlerMethodReturnValueHandler myWarpWithDataHandlerMethodReturnValueHandler; //把我们自定义的 myWarpWithDataHandlerMethodReturnValueHandler 加入 handlers 里面即可 @Override public void addReturnValueHandlers(List&lt;HandlerMethodReturnValueHandler&gt; handlers) &#123; handlers.add(myWarpWithDataHandlerMethodReturnValueHandler); &#125; @Autowired private RequestMappingHandlerAdapter requestMappingHandlerAdapter; //由于 HandlerMethodReturnValueHandler 处理的优先级问题，通过如下方法，把我们自定义的 myWarpWithDataHandlerMethodReturnValueHandler 放到第一个； @PostConstruct public void init() &#123; List&lt;HandlerMethodReturnValueHandler&gt; returnValueHandlers = Lists.newArrayList(myWarpWithDataHandlerMethodReturnValueHandler); // 取出原始列表，重新覆盖进去； returnValueHandlers.addAll(requestMappingHandlerAdapter.getReturnValueHandlers()); requestMappingHandlerAdapter.setReturnValueHandlers(returnValueHandlers); &#125;&#125; 这里需要注意的是，我们利用 @PostConstruct 调整了一下 HandlerMethodReturnValueHandler 加载的优先级，使其生效。 第四步：Controller 方法中直接加上 @WarpWithData 注解，关键代码如下： 12345@GetMapping("/user/&#123;id&#125;")@WarpWithDatapublic UserInfo getUserInfoFromPath(@PathVariable("id") Long id) &#123; return userInfoRepository.getOne(id);&#125; 第五步：我们测试一下。 1GET http://localhost:8089/user/1 就会得到如下结果，返回的 JSON 结果多了一个 Data 包装。 12345678910111213&#123; "data": &#123; "id": 1, "version": 0, "createUserId": null, "createTime": "2020-10-23T00:23:10.185Z", "lastModifiedUserId": null, "lastModifiedTime": "2020-10-23T00:23:10.185Z", "ages": 10, "telephone": null, "hibernateLazyInitializer": &#123;&#125; &#125;&#125;]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 对 WebMVC 的支持]]></title>
    <url>%2F2020%2F11%2F11%2FSpringDataJpa%E5%AF%B9WebMVC%E7%9A%84%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa 对 WebMVC 的支持Spring Data 对 Spring MVC 做了很好的支持，体现在以下几个方面： 支持在 Controller 层直接返回实体，而不使用其显式的调用方法； 对 MVC 层支持标准的分页和排序功能； 扩展的插件支持 Querydsl，可以帮忙进行一些通用的查询逻辑。 正常情况下，我们开启 Spring Data 对 Spring Web MVC 支持需要在 @Configuration 的配置文件里面添加 @EnableSpringDataWebSupport 这一注解，如下面这种形式： 12345@Configuration@EnableWebMvc//开启支持Spring Data Web的支持@EnableSpringDataWebSupportpublic class WebConfiguration &#123; &#125; 由于我们用了 Spring Boot，其有自动加载机制，会自动加载 SpringDataWebAutoConfiguration 类，发生如下变化： 1234567@EnableSpringDataWebSupport@ConditionalOnWebApplication(type = Type.SERVLET)@ConditionalOnClass(&#123; PageableHandlerMethodArgumentResolver.class, WebMvcConfigurer.class &#125;)@ConditionalOnMissingBean(PageableHandlerMethodArgumentResolver.class)@EnableConfigurationProperties(SpringDataWebProperties.class)@AutoConfigureAfter(RepositoryRestMvcAutoConfiguration.class)public class SpringDataWebAutoConfiguration &#123; ... &#125; 从类上面可以看出来，@EnableSpringDataWebSupport 会自动开启，所以当我们用 Spring Boot + JPA + MVC 的时候，什么都不需要做，因为 Spring Boot 利用 Spring Data 对 Spring MVC 做了很多 Web 开发的天然支持。支持的组件有 DomainConverter、Page、Sort、DataBinding、Dynamic Param 等。 DomainClassConverter 组件这个组件的主要作用是帮我们把 Path 中 ID 的变量，或 Request 参数中的变量 ID 的参数值，直接转化成实体对象注册到 Controller 方法的参数里面。怎么理解呢？我们看个例子，就很好懂了。 举例首先，写一个 MVC 的 Controller，分别从 Path 和 Param 变量里面，根据 ID 转化成实体，代码如下： 123456789101112131415161718192021@RestControllerpublic class UserInfoController &#123; /** * 从path变量里面获得参数ID的值，然后直接转化成UserInfo实体 * @param userInfo * @return */ @GetMapping("/user/&#123;id&#125;") public UserInfo getUserInfoFromPath(@PathVariable("id") UserInfo userInfo) &#123; return userInfo; &#125; /** * 将request的param中的ID变量值，转化成UserInfo实体 * @param userInfo * @return */ @GetMapping("/user") public UserInfo getUserInfoFromRequestParam(@RequestParam("id") UserInfo userInfo) &#123; return userInfo; &#125;&#125; 然后，我们运行起来，看一下结果： 123456789GET http://localhost:8089/user/1Content-Type: application/json&#123; "id": 1, "version": 0, "ages": 10, "telephone": "123456789"&#125; 从结果来看，Controller 里面的 getUserInfoFromRequestParam 方法会自动根据 ID 查询实体对象 UserInfo，然后注入方法的参数里面。那它是怎么实现的呢？ 源码分析DomainClassConverter 类里面有个 ToEntityConverter 的内部转化类的 Matches 方法，它会判断参数的类型是不是实体，并且有没有对应的实体 Repository 存在。如果不存在，就会直接报错说找不到合适的参数转化器。 DomainClassConverter 里面的关键代码如下： 123456789101112131415161718192021222324public class DomainClassConverter&lt;T extends ConversionService &amp; ConverterRegistry&gt; implements ConditionalGenericConverter, ApplicationContextAware &#123; @Override public boolean matches(TypeDescriptor sourceType, TypeDescriptor targetType) &#123; //判断参数的类型是不是实体 if (sourceType.isAssignableTo(targetType)) &#123; return false; &#125; Class&lt;?&gt; domainType = targetType.getType(); //有没有对应的实体的 Repository 存在 if (!repositories.hasRepositoryFor(domainType)) &#123; return false; &#125; Optional&lt;RepositoryInformation&gt; repositoryInformation = repositories.getRepositoryInformationFor(domainType); return repositoryInformation.map(it -&gt; &#123; Class&lt;?&gt; rawIdType = it.getIdType(); return sourceType.equals(TypeDescriptor.valueOf(rawIdType)) || conversionService.canConvert(sourceType.getType(), rawIdType); &#125;).orElseThrow( () -&gt; new IllegalStateException(String.format("Couldn't find RepositoryInformation for %s!", domainType))); &#125;&#125;......&#125; 所以，我们上面的例子其实是需要有 UserInfoRepository 的，否则会失败。 通过源码我们也可以看到，如果 matches=true，那么就会执行下面的 convert 方法，最终调用 findById 的方法帮我们执行查询动作，如下图所示： 而 DomainClassConverter 是 Spring MVC 自定义 Formatter 的一种机制，加载进去，可以看到如下界面： 而 SpringDataWebConfiguration 是因为实现了 WebMvcConfigurer 的 addFormatters 所有加载了自定义参数转化器的功能，所以才有了 DomainClassConverter 组件的支持。关键代码如下： 1234@Configurationpublic class SpringDataWebConfiguration implements WebMvcConfigurer, BeanClassLoaderAware &#123;......&#125; 从源码上我们也可以看到，DomainClassConverter 只会根据 ID 来查询实体，很有局限性，没有更加灵活的参数转化功能，不过你也可以根据源码自己进行扩展。 Page 和 Sort 的参数支持举例首先，新建一个 UserInfoController，里面添加如下两个方法，分别测试分页和排序。 12345678@GetMapping("/users")public Page&lt;UserInfo&gt; queryByPage(Pageable pageable, UserInfo userInfo) &#123; return userInfoRepository.findAll(Example.of(userInfo),pageable);&#125;@GetMapping("/users/sort")public HttpEntity&lt;List&lt;UserInfo&gt;&gt; queryBySort(Sort sort) &#123; return new HttpEntity&lt;&gt;(userInfoRepository.findAll(sort));&#125; 其中，queryByPage 方法中，两个参数可以分别接收分页参数和查询条件，我们请求一下，看看效果： 1GET http://localhost:8089/users?size=2&amp;page=0&amp;ages=10&amp;sort=id,desc 参数里面可以支持分页大小为 2、页码 0、排序（按照 ID 倒序） ages=10 所有结果，如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041&#123; "content": [ &#123; "id": 4, "version": 0, "ages": 10, "telephone": "123456789" &#125;, &#123; "id": 3, "version": 0, "ages": 10, "telephone": "123456789" &#125; ], "pageable": &#123; "sort": &#123; "sorted": true, "unsorted": false, "empty": false &#125;, "offset": 0, "pageNumber": 0, "pageSize": 2, "unpaged": false, "paged": true &#125;, "totalPages": 2, "totalElements": 4, "last": false, "size": 2, "number": 0, "numberOfElements": 2, "sort": &#123; "sorted": true, "unsorted": false, "empty": false &#125;, "first": true, "empty": false&#125; 因此，可以得出结论：Pageable 既支持分页参数，也支持排序参数。并且从下面这行代码可以看出其也可以单独调用 Sort 参数。 1GET http://localhost:8089/users/sort?ages=10&amp;sort=id,desc 原理分析和 DomainClassConverter 组件的支持是一样的，由于 SpringDataWebConfiguration 实现了 WebMvcConfigurer 接口，通过 addArgumentResolvers 方法，扩展了 Controller 方法的参数 HandlerMethodArgumentResolver ，从下面图片中你就可以看出来。 我们通过箭头的地方分析一下 SortHandlerMethodArgumentResolver 的类，会看到如下界面： 这个类里面最关键的就是下面两个方法： supportsParameter：表示只处理类型为 Sort.class 的参数； resolveArgument：可以把请求里面参数的值，转换成该方法里面的参数 Sort 对象。 这里还要提到的是另外一个类：PageableHandlerMethodArgumentResolver 类。 这个类里面也有两个最关键的方法： supportsParameter：表示我只处理类型是 Pageable.class 的参数； resolveArgument：把请求里面参数的值，转换成该方法里面的参数 Pageable 的实现类 PageRequest。 Web DataBinding SupportSpring Data JPA 里面，也可以通过 @ProjectedPayload 和 @JsonPath 对接口进行注解支持，不过要注意这与 Jackson 注解的区别在于，此时用的是接口。 举例第一步：如果要支持 Projection，必须要在 gradle 里面引入 jsonpath 依赖才可以： 1implementation &apos;com.jayway.jsonpath:json-path&apos; 第二步：新建一个 UserInfoInterface 接口类，用来接收接口传递的 json 对象。 123456789101112package com.example.jpa.example1;import org.springframework.data.web.JsonPath;import org.springframework.data.web.ProjectedPayload;@ProjectedPayloadpublic interface UserInfoInterface &#123; @JsonPath("$.ages") // 第一级参数/JSON里面找ages字段// @JsonPath("$..ages") $..代表任意层级找ages字段 Integer getAges(); @JsonPath("$.telephone") //第一级找参数/JSON里面的telephone字段// @JsonPath(&#123; "$.telephone", "$.user.telephone" &#125;) //第一级或者user下面的telephone都可以 String getTelephone();&#125; 第三步：在 Controller 里面新建一个 post 方法，通过接口获得 RequestBody 参数对象里面的值。 1234@PostMapping("/users/projected")public UserInfoInterface saveUserInfo(@RequestBody UserInfoInterface userInfoInterface) &#123; return userInfoInterface;&#125; 第四步：我们发送一个 post 请求，代码如下： 1234567POST http://localhost:8089/users/projectedContent-Type: application/json&#123; "ages":10, "telephone":"123456789"&#125; 此时可以正常得到如下结果： 1234&#123; "ages": 10, "telephone": "123456789"&#125; 这个响应结果说明了接口可以正常映射。 原理分析 Spring 里面是如何通过 HttpMessageConverter 对 Projected 进行的支持 查看SpringDataWebConfiguration类，其中实现的 WebMvcConfigurer 接口里面有个 extendMessageConverters 方法，方法里面加了一个 ProjectingJackson2HttpMessageConverter 的类，这个类会把带 ProjectedPayload.class 注解的接口进行 Converter。 其中主要的两个方法： 加载 ProjectingJackson2HttpMessageConverter，用来做 Projecting 的接口转化。通过源码看一下是在哪里被加载进去的，如下： 而 ProjectingJackson2HttpMessageConverter 主要是继承了 MappingJackson2HttpMessageConverter，并且实现了 HttpMessageConverter 的接口里面的两个重要方法，如下图所示： 其中， canRead 通过判断参数的实体类型里面是否有接口，以及是否有 ProjectedPayload.class 注解后，才进行解析； read 方法负责把 HttpInputMessage 转化成 Projected 的映射代理对象。 QueryDSL Web Support实际工作中，经常有人会用 Querydsl 做一些复杂查询，方便生成 Rest 的 API 接口，那么这种方法有什么好处，又会暴露什么缺点呢？ 举例第一步：需要 gradle 引入 querydsl 的依赖。 12345678implementation &apos;com.querydsl:querydsl-apt&apos;implementation &apos;com.querydsl:querydsl-jpa&apos;annotationProcessor(&quot;com.querydsl:querydsl-apt:4.3.1:jpa&quot;, &quot;org.hibernate.javax.persistence:hibernate-jpa-2.1-api:1.0.2.Final&quot;, &quot;javax.annotation:javax.annotation-api:1.3.2&quot;, &quot;org.projectlombok:lombok&quot;)annotationProcessor(&quot;org.springframework.boot:spring-boot-starter-data-jpa&quot;)annotationProcessor &apos;org.projectlombok:lombok&apos; 第二步：UserInfoRepository 继承 QuerydslPredicateExecutor 接口，就可以实现 QueryDSL 的查询方法了，代码如下： 1public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt;, QuerydslPredicateExecutor&lt;UserInfo&gt; &#123;&#125; 第三步：Controller 里面直接利用 @QuerydslPredicate 注解接收 Predicate 参数。 12345@GetMapping(value = "user/dsl")Page&lt;UserInfo&gt; queryByDsl(@QuerydslPredicate(root = UserInfo.class) com.querydsl.core.types.Predicate predicate, Pageable pageable) &#123; //这里面用的是 userInfoRepository 里面的 QuerydslPredicateExecutor 里面的方法 return userInfoRepository.findAll(predicate, pageable);&#125; 第四步：直接请求我们的 user / dsl 即可，这里利用 queryDsl 的语法 ，使 &amp;ages=10 作为我们的请求参数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546GET http://localhost:8089/user/dsl?size=2&amp;page=0&amp;ages=10&amp;sort=id%2Cdesc&amp;ages=10Content-Type: application/json&#123; "content": [ &#123; "id": 2, "version": 0, "ages": 10, "telephone": "123456789" &#125;, &#123; "id": 1, "version": 0, "ages": 10, "telephone": "123456789" &#125; ], "pageable": &#123; "sort": &#123; "sorted": true, "unsorted": false, "empty": false &#125;, "offset": 0, "pageNumber": 0, "pageSize": 2, "unpaged": false, "paged": true &#125;, "totalPages": 1, "totalElements": 2, "last": true, "size": 2, "number": 0, "sort": &#123; "sorted": true, "unsorted": false, "empty": false &#125;, "numberOfElements": 2, "first": true, "empty": false&#125;Response code: 200; Time: 721ms; Content length: 425 bytes 结论: QueryDSL 可以帮我们省去创建 Predicate 的过程，简化了操作流程。但是它依然存在一些局限性，比如多了一些模糊查询、范围查询、大小查询，它对这些方面的支持不是特别友好。可能未来会更新、优化 原理分析QueryDSL 也是主要利用自定义 Spring MVC 的 HandlerMethodArgumentResolver 实现类，根据请求的参数字段，转化成 Controller 里面所需要的参数，请看一下源码。 12345public class QuerydslPredicateArgumentResolver implements HandlerMethodArgumentResolver &#123;....public Object resolveArgument(MethodParameter parameter, @Nullable ModelAndViewContainer mavContainer, NativeWebRequest webRequest, @Nullable WebDataBinderFactory binderFactory) throws Exception &#123; ..... 在实际开发中，关于 insert 和 update 的接口是“逃不掉”的，但不是每次的字段都会全部传递过来，那这个时候我们应该怎么做呢？这就涉及了上述实例里面的两个注解 @DynamicUpdate 和 @DynamicInsert，下面来详细介绍一下。 @DynamicUpdate &amp; @DynamicInsert 详解@DynamicInsert：这个注解表示 insert 的时候，会动态生产 insert SQL 语句 其生成 SQL 的规则是：只有非空的字段才能生成 SQL。代码如下： 123456@Target( TYPE )@Retention( RUNTIME )public @interface DynamicInsert &#123; //默认是true，如果设置成false，就表示空的字段也会生成sql语句； boolean value() default true;&#125; 这个注解主要是用在 @Entity 的实体中，如果加上这个注解，就表示生成的 insert SQL 的 Columns 只包含非空的字段；如果实体中不加这个注解，默认的情况是空的，字段也会作为 insert 语句里面的 Columns。 @DynamicUpdate：和 insert 是一个意思，只不过这个注解指的是在 update 的时候，会动态产生 update SQL 语句 生成 SQL 的规则是：只有非空的字段才生成到 update SQL 里面。代码如下： 123456@Target( TYPE )@Retention( RUNTIME )public @interface DynamicUpdate &#123; //和insert里面一个意思，默认true; boolean value() default true;&#125; 和上一个注解的原理类似，这个注解也是用在 @Entity 的实体中，如果加上这个注解，就表示生成的 update SQL 的 Columns 只包含非空的字段；如果不加这个注解，默认的情况是空的字段也会作为 update 语句里面的 Columns。 举例第一步：为了方便测试，我们修改一下 User 实体：加上 @DynamicInsert 和 @DynamicUpdate 注解。 123456789@DynamicInsert@DynamicUpdatepublic class User extends BaseEntity &#123; private String name; private String email; @Enumerated(EnumType.STRING) private SexEnum sex; private Integer age;......&#125;//其他不变的信息省略 第二步：UserInfo 实体还保持不变，即没有加上 @DynamicInsert 和 @DynamicUpdate 注解。 1234567891011@Entity@Data@AllArgsConstructor@NoArgsConstructorpublic class UserInfo extends BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private Integer ages; private String telephone;&#125; 第三步：我们在 UserController 里面添加如下方法，用来测试新增和更新 User。 1234@PostMapping("/user")public User saveUser(@RequestBody User user) &#123; return userRepository.save(user);&#125; 第四步：在 UserInfoController 里面添加如下方法，用来测试新增和更新 UserInfo。 1234@PostMapping("/user/info")public UserInfo saveUserInfo(@RequestBody UserInfo userInfo) &#123; return userInfoRepository.save(userInfo);&#125; 第五步：测试一下 UserController的 post 的 user 情况，看一下 insert 的情况。 1234567#### 通过post测试insertPOST /user HTTP/1.1Host: 127.0.0.1:8089Content-Type: application/jsonCache-Control: no-cachePostman-Token: 56d8dc02-7f3e-7b95-7ff1-572a4bb7d102&#123;"ages":10, "name":"jack"&#125; 这时，发送一个 post 请求，只带 ages 和 name 字段，而并没有带上 User 实体里面的其他字段，生成的 SQL 如下： 1Hibernate: insert into user (create_time, last_modified_time, version, name, id) values (?, ?, ?, ?, ?) 除了 BaseEntity 里面的一些基础字段，而其他字段并没有生成到 insert 语句里面。 第六步：我们再测试一下 user 的 update 情况。 1234567#### 还是发生post请求，带上ID和version执行update操作POST /user HTTP/1.1Host: 127.0.0.1:8089Content-Type: application/jsonCache-Control: no-cachePostman-Token: 56d8dc02-7f3e-7b95-7ff1-572a4bb7d102&#123;"ages":10,"name":"jack1","id":1,"version":0&#125; 此时看到，update 和 insert 的唯一区别就是，当 Entity 里面有 version 字段的时候，我们再带上 version 和 id 就会显示为 update，再看一下调用完之后的 sql：用一条 select 查询一下实体是否存在，代码如下： 1Hibernate: select user0_.id as id1_1_0_, user0_.create_time as create_t2_1_0_, user0_.create_user_id as create_u3_1_0_, user0_.last_modified_time as last_mod4_1_0_, user0_.last_modified_user_id as last_mod5_1_0_, user0_.version as version6_1_0_, user0_.age as age7_1_0_, user0_.deleted as deleted8_1_0_, user0_.email as email9_1_0_, user0_.name as name10_1_0_, user0_.sex as sex11_1_0_ from user user0_ where user0_.id=? 其中一条 update 动态更新了我们传递的那些值，而不更新 null 的字段，并且只更新有变化的字段，操作如下： 1Hibernate: update user set last_modified_time=?, version=?, name=? where id=? and version=? 第七步：那么我们再看一下 UserInfo 的 insert 方法。 1234567#### insertPOST /user/info HTTP/1.1Host: 127.0.0.1:8089Content-Type: application/jsonCache-Control: no-cachePostman-Token: 56d8dc02-7f3e-7b95-7ff1-572a4bb7d102&#123;"ages":10&#125; 发送一个 post 的 insert 操作，我们看一下 SQL： 1Hibernate: insert into user_info (create_time, create_user_id, last_modified_time, last_modified_user_id, version, ages, telephone, id) values (?, ?, ?, ?, ?, ?, ?, ?) 发现无论有没有传递值，每个字段都做了 insert，没有传递的话会用 null 代替。第八步：我们再看一下 UserInfo 的 update 方法。 1234567#### updatePOST /user/info HTTP/1.1Host: 127.0.0.1:8089Content-Type: application/jsonCache-Control: no-cachePostman-Token: 56d8dc02-7f3e-7b95-7ff1-572a4bb7d102&#123;"ages":10,"id":1,"version":0&#125; 1Hibernate: update user_info set create_time=?, create_user_id=?, last_modified_time=?, last_modified_user_id=?, version=?, ages=?, telephone=? where id=? and version=? 通过 update 的 SQL 可以看出，即使只传递了 ages 的值，它也会把我们没有传递的 telephone 更新成 null。 通过上面的两个例子应该能弄清楚 @DynamicInsert 和 @DynamicUpdate 注解作用，在写 API 的时候就要考虑一下是否需要对 null 的字段进行操作。 Spring Data 对系统监控做了哪些支持？我们既然做了 MVC，一定也免不了要对系统进行监控，那么怎么看监控指标呢？ 对数据层面的系统进行监控，主要有两个方法。 方法一：/actuator/health 的支持，里面会检查 DB 的状态。 方法二：/actuator/prometheus 里面会包含一些 Hibernate 和 Datasource 的 metric。 这个方法在我们做 grafana 图表的时候会很有用，不过需要注意的是： 开启 prometheus 需要 gradle 额外引入下面这个包： 1implementation &apos;io.micrometer:micrometer-registry-prometheus&apos; 开启 Hibernate 的 statistics 需要配置如下操作： 123spring.jpa.properties.hibernate.generate_statistics=truemanagement.endpoint.prometheus.enabled=truemanagement.metrics.export.prometheus.enabled=true]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 乐观锁与重试机制]]></title>
    <url>%2F2020%2F11%2F10%2FSpringDataJpa%E4%B9%90%E8%A7%82%E9%94%81%E4%B8%8E%E9%87%8D%E8%AF%95%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa 乐观锁与重试机制什么是乐观锁乐观锁在实际开发过程中很常用，它没有加锁、没有阻塞，在多线程环境以及高并发的情况下 CPU 的利用率是最高的，吞吐量也是最大的。 而 Java Persistence API 协议也对乐观锁的操作做了规定：通过指定 @Version 字段对数据增加版本号控制，进而在更新的时候判断版本号是否有变化。如果没有变化就直接更新；如果有变化，就会更新失败并抛出“OptimisticLockException”异常。我们用 SQL 表示一下乐观锁的做法，代码如下： 12select uid, name, version from user where id=1;update user set name='jack', version=version+1 where id=1 and version=1 假设本次查询的 version=1，在更新操作时，加上这次查出来的 Version，这样和我们上一个版本相同，就会更新成功，并且不会出现互相覆盖的问题，保证了数据的原子性。 这就是乐观锁在数据库里面的应用。 乐观锁的实现方法JPA 协议规定，想要实现乐观锁可以通过 @Version 注解标注在某个字段上面，并且可以持久化到 DB 即可。其支持的类型有如下四种： Integer Short Long java.sql.Timestamp 这样就可以完成乐观锁的操作。我比较推荐使用 Integer 类型的字段，因为这样语义比较清晰、简单。 注意：Spring Data JPA 里面有两个 @Version 注解 请使用 @javax.persistence.Version 而不是 @org.springframework.data.annotation.Version @Version 的用法1. 实体里面添加带 @Version 注解的持久化字段 直接在这个 BaseEntity 基类里面添加 @Version 即可，当然也可以把这个字段放在 sub-class-entity 里面。推荐放在基类里面，因为这段逻辑是公共的字段。改动完之后我们看看会发生什么变化，如下所示： 12345678910@Data@MappedSuperclasspublic class BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; @Version private Integer version; //......当然也可以用上一课时讲解的 auditing 字段，这里我们先省略&#125; 2. 用 UserInfo 实体继承 BaseEntity，就可以实现 @Version 的效果，代码如下： 12345678910111213@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructor@ToString(callSuper = true)public class UserInfo extends BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private Integer ages; private String telephone;&#125; 3. 创建 UserInfoRepository，方便进行 DB 操作。 1public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt; &#123;&#125; 4. 创建 UserInfoService 和 UserInfoServiceImpl，用来模拟 Service 的复杂业务逻辑。 1234567891011121314151617181920212223242526272829public interface UserInfoService &#123; /** * 根据 UserId 产生的一些业务计算逻辑 */ UserInfo calculate(Long userId);&#125;@Componentpublic class UserInfoServiceImpl implements UserInfoService &#123; @Autowired private UserInfoRepository userInfoRepository; /** * 根据 UserId 产生的一些业务计算逻辑 * @param userId * @return */ @Override @Transactional public UserInfo calculate(Long userId) &#123; UserInfo userInfo = userInfoRepository.getOne(userId); try &#123; //模拟复杂的业务计算逻辑耗时操作； Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; userInfo.setAges(userInfo.getAges()+1); return userInfoRepository.saveAndFlush(userInfo); &#125;&#125; 其中，我们通过 @Transactional 开启事务，并且在查询方法后面模拟复杂业务逻辑，用来呈现多线程的并发问题。 5. 测试用例 1234567891011121314151617181920212223242526272829303132333435363738394041424344@ExtendWith(SpringExtension.class)@DataJpaTest@ComponentScan(basePackageClasses=UserInfoServiceImpl.class)public class UserInfoServiceTest &#123; @Autowired private UserInfoService userInfoService; @Autowired private UserInfoRepository userInfoRepository; @Test public void testVersion() &#123; //加一条数据 UserInfo userInfo = userInfoRepository.save(UserInfo.builder().ages(20).telephone("1233456").build()); //验证一下数据库里面的值 Assertions.assertEquals(0, userInfo.getVersion()); Assertions.assertEquals(20, userInfo.getAges()); userInfoService.calculate(1L); //验证一下更新成功的值 UserInfo u2 = userInfoRepository.getOne(1L); Assertions.assertEquals(1, u2.getVersion()); Assertions.assertEquals(21, u2.getAges()); &#125; @Test @Rollback(false) @Transactional(propagation = Propagation.NEVER) public void testVersionException() &#123; //加一条数据 userInfoRepository.saveAndFlush(UserInfo.builder().ages(20).telephone("1233456").build()); //模拟多线程执行两次 new Thread(() -&gt; userInfoService.calculate(1L)).start(); try &#123; Thread.sleep(10L);// &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //如果两个线程同时执行会发生乐观锁异常； Exception exception = Assertions.assertThrows(ObjectOptimisticLockingFailureException.class, () -&gt; &#123; userInfoService.calculate(1L); //模拟多线程执行两次 &#125;); System.out.println(exception); &#125;&#125; 从上面的测试得到的结果中，我们执行 testVersion()，会发现在 save 的时候， Version 会自动 +1，第一次初始化为 0；update 的时候也会附带 Version 条件。通过下图的 SQL，也可以看到 Version 的变化。 而当面我们调用 testVersionException() 测试方法的时候，利用多线程模拟两个并发情况，会发现两个线程同时取到了历史数据，并在稍后都对历史数据进行了更新。 由此你会发现，第二次测试的结果是乐观锁异常，更新不成功。请看一下测试的日志。 通过日志又会发现，两个 SQL 同时更新的时候，Version 是一样的，是它导致了乐观锁异常。 📌注意：乐观锁异常不仅仅是同一个方法多线程才会出现的问题，我们只是为了方便测试而采用同一个方法；不同的方法、不同的项目，都有可能导致乐观锁异常。乐观锁的本质是 SQL 层面发生的，和使用的框架、技术没有关系。 @Version 对 Save 方法的影响通过上面的实例发现，@Version 底层实现逻辑和 @EntityListeners 一点关系没有，底层是通过 Hibernate 判断实体里面是否有 @Version 的持久化字段，利用乐观锁机制来创建和使用 Version 的值。 因此，还是那句话：Java Persistence API 负责制定协议，Hibernate 负责实现逻辑，Spring Data JPA 负责封装和使用。那么我们来看下 Save 对象的时候，如何判断是新增的还是 merge 的逻辑呢？ isNew 判断的逻辑通过断点，我们可以进入 SimpleJpaRepository.class 的 Save 方法中，看到如下图显示的界面： 然后，我们进入 JpaMetamodelEntityInformation.class 的 isNew 方法中，又会看到下图显示的界面： 其中，第一段逻辑，判断其中是否有 @Version 标注的属性，并且该属性是否为基础类型。如果不满足条件，调用 super.isNew(entity) 方法，而 super.isNew 里面只判断了 ID 字段是否有值。 第二段逻辑，如果有 @Version 字段，那么看看这个字段是否有值，如果没有就返回 true，如果有值则返回 false。 🎯 结论：如果有 @Version 注解的字段，就以 @Version 字段来判断 save / update；如果没有，就以 @ID 字段是否有值来判断 save / update。 注意：虽然看到的是 merge 方法，但是不一定会执行 update 操作，merge 方法会判断对象是否为游离状态，以及有无 ID 值。它会先触发一条 select 语句，并根据 ID 查一下这条记录是否存在，如果不存在，虽然 ID 和 Version 字段都有值，但也只是执行 insert 语句；如果本条 ID 记录存在，才会执行 update 的 sql。至于这个具体的 insert 和 update 的 sql、传递的参数是什么。 总之，如果使用纯粹的 saveOrUpdate 方法，那么完全不需要自己写这一段逻辑，只要保证 ID 和 Version 存在该有的值就可以了，JPA 会帮我们实现剩下的逻辑。 实际工作中，特别是分布式更新的时候，很容易碰到乐观锁，这时候还要结合重试机制才能完美解决我们的问题。 先了解一下 Spring 支持的重试机制是什么样的。 重试机制详解Spring 全家桶里面提供了@Retryable 的注解，会帮我们进行重试。下面看一个 @Retryable 的例子。 第一步：利用 gradle 引入 spring-retry 的依赖 jar，如下所示： 1implementation &apos;org.springframework.retry:spring-retry&apos; 第二步：在 UserInfoServiceImpl 的方法中添加 @Retryable 注解，就可以实现重试的机制了，代码如下： 第三步：新增一个 RetryConfiguration 并添加 @EnableRetry 注解，是为了开启重试机制，使 @Retryable 生效。 1234@EnableRetry@Configurationpublic class RetryConfiguration &#123;&#125; 第四步：测试用例测试一下。 1234567891011121314151617181920212223242526272829@ExtendWith(SpringExtension.class)@DataJpaTest@ComponentScan(basePackageClasses=UserInfoServiceImpl.class)@Import(RetryConfiguration.class)public class UserInfoServiceRetryTest &#123; @Autowired private UserInfoService userInfoService; @Autowired private UserInfoRepository userInfoRepository; @Test @Rollback(false) @Transactional(propagation = Propagation.NEVER) public void testRetryable() &#123; //加一条数据 userInfoRepository.saveAndFlush(UserInfo.builder().ages(20).telephone("1233456").build()); //模拟多线程执行两次 new Thread(() -&gt; userInfoService.calculate(1L)).start(); try &#123; Thread.sleep(10L);// &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; //模拟多线程执行两次，由于加了@EnableRetry，所以这次也会成功 UserInfo userInfo = userInfoService.calculate(1L); //经过了两次计算，年龄变成了 22 Assertions.assertEquals(22,userInfo.getAges()); Assertions.assertEquals(2,userInfo.getVersion()); &#125;&#125; 在测试用例里面执行 @Import(RetryConfiguration.class)，这样就开启了重试机制，然后继续在里面模拟了两次线程调用，发现第二次发生了乐观锁异常之后依然成功了。为什么呢？我们通过日志可以看到，它是失败了一次之后又进行了重试，所以第二次成功了。 通过案例发现 Retry 的逻辑其实很简单，只需要利用 @Retryable 注解即可 @Retryable 详细用法其源码里面提供了很多方法，看下面这个图片。 下面对常用的 @Retryable 注解中的参数做一下说明： maxAttempts：最大重试次数，默认为 3，如果要设置的重试次数为 3，可以不写； value：抛出指定异常才会重试； include：和 value 一样，默认为空，当 exclude 也为空时，默认异常； exclude：指定不处理的异常； backoff：重试等待策略，默认使用 @Backoff 的 value，默认为 1s，请看下面这个图。 其中： value=delay：隔多少毫秒后重试，默认为 1000L，单位是毫秒； multiplier（指定延迟倍数）默认为 0，表示固定暂停 1 秒后进行重试，如果把 multiplier 设置为 1.5，则第一次重试为 1.5 秒，第二次为 3 秒，第三次为 4.5 秒。 下面是一个关于 @Retryable 扩展的使用例子，具体看一下代码： 123456@Servicepublic interface MyService &#123; @Retryable( value = SQLException.class, maxAttempts = 2, backoff = @Backoff(delay = 100)) void retryServiceWithCustomization(String sql) throws SQLException;&#125; 可以看到，这里明确指定 SQLException.class 异常的时候需要重试两次，每次中间间隔 100 毫秒。 123456@Service public interface MyService &#123; @Retryable( value = SQLException.class, maxAttemptsExpression = "$&#123;retry.maxAttempts&#125;", backoff = @Backoff(delayExpression = "$&#123;retry.maxDelay&#125;")) void retryServiceWithExternalizedConfiguration(String sql) throws SQLException; &#125; 此外，你也可以利用 SpEL 表达式读取配置文件里面的值。 如果你遇到更复杂的场景，可以到 GitHub 中看一下官方的 Retryable文档。 乐观锁+重试机制的最佳实践建议使用如下配置： 12@Retryable(value = ObjectOptimisticLockingFailureException.class, backoff = @Backoff(multiplier = 1.5, random = true)) 这里明确指定 ObjectOptimisticLockingFailureException.class 等乐观锁异常要进行重试，如果引起其他异常的话，重试也会失败，没有意义；而 backoff 采用随机 +1.5 倍的系数，这样基本很少会出现连续 3 次乐观锁异常的情况，并且也很难发生重试风暴而引起系统重试崩溃的问题。 悲观锁的类型怎么实现？(不推荐使用)Java Persistence API 2.0 协议里面有一个 LockModeType 枚举值，里面包含了所有它支持的乐观锁和悲观锁的值。 12345678910111213141516171819public enum LockModeType&#123; //等同于OPTIMISTIC，默认，用来兼容2.0之前的协议 READ, //等同于OPTIMISTIC_FORCE_INCREMENT，用来兼容2.0之前的协议 WRITE, //乐观锁，默认，2.0协议新增 OPTIMISTIC, //乐观写锁，强制version加1，2.0协议新增 OPTIMISTIC_FORCE_INCREMENT, //悲观读锁 2.0协议新增 PESSIMISTIC_READ, //悲观写锁，version不变，2.0协议新增 PESSIMISTIC_WRITE, //悲观写锁，version会新增，2.0协议新增 PESSIMISTIC_FORCE_INCREMENT, //2.0协议新增无锁状态 NONE&#125; 只需要在自己的 Repository 里面覆盖父类的 Repository 方法，然后添加 @Lock 注解并指定 LockModeType 即可支持悲观锁，请看如下代码： 1234public interface UserInfoRepository extends JpaRepository&lt;UserInfo, Long&gt; &#123; @Lock(LockModeType.PESSIMISTIC_WRITE) Optional&lt;UserInfo&gt; findById(Long userId);&#125; UserInfoRepository 里面覆盖了父类的 findById 方法，并指定锁的类型为悲观锁。如果将 service 改调用为悲观锁的方法，会发生什么变化呢？如下图所示： 然后再执行上面测试中 testRetryable 的方法，跑完测试用例的结果依然是通过的，我们看下日志。 刚才的串行操作完全变成了并行操作。所以少了一次 Retry 的过程，结果还是一样的。 在生产环境中要慎用悲观锁，因为它是阻塞的，一旦发生服务异常，可能会造成死锁的现象。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>乐观锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JavaScript for循环写法]]></title>
    <url>%2F2020%2F11%2F10%2FJavaScript%20for%E5%BE%AA%E7%8E%AF%E5%86%99%E6%B3%95%2F</url>
    <content type="text"><![CDATA[JavaScript for循环写法简单 for 循环下面先来看看大家最常见的一种写法： 1234const arr = [1, 2, 3];for(let i = 0; i &lt; arr.length; i++) &#123; console.log(arr[i]);&#125; 当数组长度在循环过程中不会改变时，我们应将数组长度用变量存储起来，这样会获得更好的效率，下面是改进的写法： 1234const arr = [1, 2, 3];for(let i = 0, len = arr.length; i &lt; len; i++) &#123; console.log(arr[i]);&#125; for-in通常情况下，我们可以用 for-in 来遍历一遍数组的内容，代码如下： 12345const arr = [1, 2, 3];let index;for(index in arr) &#123; console.log("arr[" + index + "] = " + arr[index]);&#125; 一般情况下，运行结果如下： 123arr[0] = 1arr[1] = 2arr[2] = 3 但这么做往往会出现问题。 for-in 的真相 for-in 循环遍历的是对象的属性，而不是数组的索引。因此， for-in 遍历的对象便不局限于数组，还可以遍历对象。例子如下： 123456789const person = &#123; fname: "san", lname: "zhang", age: 99&#125;;let info;for(info in person) &#123; console.log("person[" + info + "] = " + person[info]);&#125; 结果如下： 123person[fname] = sanperson[lname] = zhangperson[age] = 99 需要注意的是， for-in 遍历属性的顺序并不确定，即输出的结果顺序与属性在对象中的顺序无关，也与属性的字母顺序无关，与其他任何顺序也无关。 Array 的真相 Array 在 JavaScript 中是一个对象， Array 的索引是属性名。事实上， JavaScript 中的 “array” 有些误导性， JavaScript 中的 Array 并不像大部分其他语言的数组。首先， JavaScript 中的 Array 在内存上并不连续，其次， Array 的索引并不是指偏移量。实际上， Array 的索引也不是 Number 类型，而是 String 类型的。我们可以正确使用如 arr[0] 的写法的原因是语言可以自动将 Number 类型的 0 转换成 String 类型的 “0″ 。所以，在 JavaScript 中从来就没有 Array 的索引，而只有类似 “0″ 、 “1″ 等等的属性。有趣的是，每个 Array 对象都有一个 length 的属性，导致其表现地更像其他语言的数组。但为什么在遍历 Array 对象的时候没有输出 length 这一条属性呢？那是因为 for-in 只能遍历“可枚举的属性”， length 属于不可枚举属性，实际上， Array 对象还有许多其他不可枚举的属性。 现在，我们再回过头来看看用 for-in 来循环数组的例子，我们修改一下前面遍历数组的例子： 123456const arr = [1, 2, 3];arr.name = "Hello world";let index;for(index in arr) &#123; console.log("arr[" + index + "] = " + arr[index]);&#125; 运行结果是： 1234arr[0] = 1arr[1] = 2arr[2] = 3arr[name] = Hello world 我们看到 for-in 循环访问了我们新增的 “name” 属性，因为 for-in 遍历了对象的所有属性，而不仅仅是“索引”。同时需要注意的是，此处输出的索引值，即 “0″、 “1″、 “2″不是 Number 类型的，而是 String 类型的，因为其就是作为属性输出，而不是索引。那是不是说不在我们的 Array 对象中添加新的属性，我们就可以只输出数组中的内容了呢？答案是否定的。因为 for-in 不仅仅遍历 array 自身的属性，其还遍历 array 原型链上的所有可枚举的属性。下面我们看个例子： 1234567Array.prototype.fatherName = "Father";const arr = [1, 2, 3];arr.name = "Hello world";let index;for(index in arr) &#123; console.log("arr[" + index + "] = " + arr[index]);&#125; 运行结果是： 12345arr[0] = 1arr[1] = 2arr[2] = 3arr[name] = Hello worldarr[fatherName] = Father 写到这里，我们可以发现 for-in 并不适合用来遍历 Array 中的元素，其更适合遍历对象中的属性，这也是其被创造出来的初衷。却有一种情况例外，就是稀疏数组。考虑下面的例子： 12345678910111213let key;const arr = [];arr[0] = "a";arr[100] = "b";arr[10000] = "c";for(key in arr) &#123; if(arr.hasOwnProperty(key) &amp;&amp; /^0$|^[1-9]\d*$/.test(key) &amp;&amp; key &lt;= 4294967294 ) &#123; console.log(arr[key]); &#125;&#125; for-in 只会遍历存在的实体，上面的例子中， for-in 遍历了3次（遍历属性分别为”0″、 “100″、 “10000″的元素，普通 for 循环则会遍历 10001 次）。所以，只要处理得当， for-in 在遍历 Array 中元素也能发挥巨大作用。 为了避免重复劳动，我们可以包装一下上面的代码： 12345function arrayHasOwnIndex(array, prop) &#123; return array.hasOwnProperty(prop) &amp;&amp; /^0$|^[1-9]\d*$/.test(prop) &amp;&amp; prop &lt;= 4294967294; // 2^32 - 2&#125; 使用示例如下： 12345for (let key in arr) &#123; if (arrayHasOwnIndex(arr, key)) &#123; console.log(arr[key]); &#125;&#125; for-in 性能 正如上面所说，每次迭代操作会同时搜索实例或者原型属性， for-in 循环的每次迭代都会产生更多开销，因此要比其他循环类型慢，一般速度为其他类型循环的 1/7。因此，除非明确需要迭代一个属性数量未知的对象，否则应避免使用 for-in 循环。如果需要遍历一个数量有限的已知属性列表，使用其他循环会更快，比如下面的例子： 123456789const obj = &#123; "prop1": "value1", "prop2": "value2"&#125;;const props = ["prop1", "prop2"];for(let i = 0; i &lt; props.length; i++) &#123; console.log(obj[props[i]]);&#125; 上面代码中，将对象的属性都存入一个数组中，相对于 for-in 查找每一个属性，该代码只关注给定的属性，节省了循环的开销和时间。 forEach在 ES5 中，引入了新的循环，即 forEach 循环。 1234const arr = [1, 2, 3];arr.forEach((data) =&gt; &#123; console.log(data);&#125;); 运行结果： 123123 forEach 方法为数组中含有有效值的每一项执行一次 callback 函数，那些已删除（使用 delete 方法等情况）或者从未赋值的项将被跳过（不包括那些值为 undefined 或 null 的项）。 callback 函数会被依次传入三个参数： 数组当前项的值； 数组当前项的索引； 数组对象本身； 需要注意的是，forEach 遍历的范围在第一次调用 callback 前就会确定。调用forEach 后添加到数组中的项不会被 callback 访问到。如果已经存在的值被改变，则传递给 callback 的值是 forEach 遍历到他们那一刻的值。已删除的项不会被遍历到。 12345678const arr = [];arr[0] = "a";arr[3] = "b";arr[10] = "c";arr.name = "Hello world";arr.forEach((data, index, array) =&gt; &#123; console.log(data, index, array);&#125;); 运行结果： 123a 0 ["a", 3: "b", 10: "c", name: "Hello world"]b 3 ["a", 3: "b", 10: "c", name: "Hello world"]c 10 ["a", 3: "b", 10: "c", name: "Hello world"] 这里的 index 是 Number 类型，并且也不会像 for-in 一样遍历原型链上的属性。 所以，使用 forEach 时，我们不需要专门地声明 index 和遍历的元素，因为这些都作为回调函数的参数。 另外，forEach 将会遍历数组中的所有元素，但是 ES5 定义了一些其他有用的方法，下面是一部分： every: 循环在第一次 return false 后返回 some: 循环在第一次 return true 后返回 filter: 返回一个新的数组，该数组内的元素满足回调函数 map: 将原数组中的元素处理后再返回 reduce: 对数组中的元素依次处理，将上次处理结果作为下次处理的输入，最后得到最终结果。 forEach 性能 在不同浏览器下测试的结果都是 forEach 的速度不如 for。如果大家把测试代码放在控制台的话，可能会得到不一样的结果，主要原因是控制台的执行环境与真实的代码执行环境有所区别。 for-of先来看个例子： 1234const arr = ['a', 'b', 'c'];for(let data of arr) &#123; console.log(data);&#125; 运行结果是： 123abc 为什么要引进 for-of？ 要回答这个问题，我们先来看看ES6之前的 3 种 for 循环有什么缺陷： forEach 不能 break 和 return； for-in 缺点更加明显，它不仅遍历数组中的元素，还会遍历自定义的属性，甚至原型链上的属性都被访问到。而且，遍历数组元素的顺序可能是随机的。 所以，鉴于以上种种缺陷，我们需要改进原先的 for 循环。但 ES6 不会破坏你已经写好的 JS 代码。目前，成千上万的 Web 网站依赖 for-in 循环，其中一些网站甚至将其用于数组遍历。如果想通过修正 for-in 循环增加数组遍历支持会让这一切变得更加混乱，因此，标准委员会在 ES6 中增加了一种新的循环语法来解决目前的问题，即 for-of 。 那 for-of 到底可以干什么呢？ 跟 forEach 相比，可以正确响应 break, continue, return。 for-of 循环不仅支持数组，还支持大多数类数组对象，例如 DOM nodelist 对象。 for-of 循环也支持字符串遍历，它将字符串视为一系列 Unicode 字符来进行遍历。 for-of 也支持 Map 和 Set （两者均为 ES6 中新增的类型）对象遍历。 总结一下，for-of 循环有以下几个特征： 这是最简洁、最直接的遍历数组元素的语法。 这个方法避开了 for-in 循环的所有缺陷。 与 forEach 不同的是，它可以正确响应 break、continue 和 return 语句。 其不仅可以遍历数组，还可以遍历类数组对象和其他可迭代对象。 但需要注意的是，for-of循环不支持普通对象，但如果你想迭代一个对象的属性，你可以用 for-in 循环（这也是它的本职工作）。 最后要说的是，ES6 引进的另一个方式也能实现遍历数组的值，那就是 Iterator。上个例子： 1234567const arr = ['a', 'b', 'c'];const iter = arr[Symbol.iterator]();iter.next() // &#123; value: 'a', done: false &#125;iter.next() // &#123; value: 'b', done: false &#125;iter.next() // &#123; value: 'c', done: false &#125;iter.next() // &#123; value: undefined, done: true &#125;]]></content>
      <categories>
        <category>前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECMAScript]]></title>
    <url>%2F2020%2F11%2F09%2FECMAScript%2F</url>
    <content type="text"><![CDATA[ECMAScript ES 全称 ECMAScript，ECMAScript 是 ECMA 制定的标准化脚本语言 ES6新特性（2015）ES6中的特性比较多。 列举几个常用的： 类 模块化 箭头函数 函数参数默认值 模板字符串 解构赋值 延展操作符 对象属性简写 Promise Let 与 Const 类（class）1234567891011121314151617181920212223242526272829303132333435class Animal &#123; // 构造函数，实例化的时候将会被调用，如果不指定，那么会有一个不带参数的默认构造函数. constructor(name, color) &#123; this.name = name; this.color = color; &#125; // toString 是原型对象上的属性 toString() &#123; console.log('name:' + this.name + ',color:' + this.color); &#125; &#125;var animal = new Animal('dog','white'); //实例化 Animalanimal.toString();console.log(animal.hasOwnProperty('name')); // trueconsole.log(animal.hasOwnProperty('toString')); // falseconsole.log(animal.__proto__.hasOwnProperty('toString')); // trueclass Cat extends Animal &#123; constructor(action) &#123; // 子类必须要在constructor中指定 super 函数，否则在新建实例的时候会报错. // 如果没有置顶consructor, 默认带super函数的constructor将会被添加 super('cat','white'); this.action = action; &#125; toString() &#123; console.log(super.toString()); &#125;&#125;var cat = new Cat('catch')cat.toString();// 实例cat 是 Cat 和 Animal 的实例，和 Es5 完全一致。console.log(cat instanceof Cat); // trueconsole.log(cat instanceof Animal); // true 模块化(Module)ES5 不支持原生的模块化，在 ES6 中模块作为重要的组成部分被添加进来。模块的功能主要由 export 和 import 组成。每一个模块都有自己单独的作用域，模块之间的相互调用关系是通过 export 来规定模块对外暴露的接口，通过 import 来引用其它模块提供的接口。同时还为模块创造了命名空间，防止函数的命名冲突。 导出(export)ES6 允许在一个模块中使用 export 来导出多个变量或函数。 导出变量12//test.jsexport var name = 'Rainbow' ES6不仅支持变量的导出，也支持常量的导出。 export const sqrt = Math.sqrt // 导出常量 ES6 将一个文件视为一个模块，上面的模块通过 export 向外输出了一个变量。一个模块也可以同时往外面输出多个变量。 1234//test.js var name = 'Rainbow'; var age = '24'; export &#123;name, age&#125;; 导出函数1234// myModule.jsexport function myModule(someArg) &#123; return someArg;&#125; 导入(import)定义好模块的输出以后就可以在另外一个模块通过 import 引用。 12import &#123;myModule&#125; from 'myModule' // main.jsimport &#123;name,age&#125; from 'test' // test.js 一条 import 语句可以同时导入默认函数和其它变量。import defaultMethod, { otherMethod } from &#39;xxx.js&#39; 箭头（Arrow）函数=&gt;不只是关键字 function 的简写，它还带来了其它好处。箭头函数与包围它的代码共享同一个this，能帮你很好的解决 this 的指向问题。有经验的 JavaScript 开发者都熟悉诸如var self = this或var that = this这种引用外围 this 的模式。但借助=&gt;，就不需要这种模式了。 箭头函数的结构箭头函数的箭头=&gt;之前是一个空括号、单个的参数名、或用括号括起的多个参数名，而箭头之后可以是一个表达式（作为函数的返回值），或者是用花括号括起的函数体（需要自行通过 return 来返回值，否则返回的是 undefined）。 12345678910111213// 箭头函数的例子()=&gt;1v=&gt;v+1(a,b)=&gt;a+b()=&gt;&#123; alert("foo");&#125;e=&gt;&#123; if (e == 0)&#123; return 0; &#125; return 1000/e;&#125; 不论是箭头函数还是 bind，每次被执行都返回的是一个新的函数引用，因此如果你还需要函数的引用去做一些别的事情（譬如卸载监听器），那么你必须自己保存这个引用。 卸载监听器时的陷阱错误的做法 12345678910class PauseMenu extends React.Component&#123; componentWillMount()&#123; AppStateIOS.addEventListener('change', this.onAppPaused.bind(this)); &#125; componentWillUnmount()&#123; AppStateIOS.removeEventListener('change', this.onAppPaused.bind(this)); &#125; onAppPaused(event)&#123; &#125;&#125; 正确的做法 1234567891011121314class PauseMenu extends React.Component&#123; constructor(props)&#123; super(props); this._onAppPaused = this.onAppPaused.bind(this); &#125; componentWillMount()&#123; AppStateIOS.addEventListener('change', this._onAppPaused); &#125; componentWillUnmount()&#123; AppStateIOS.removeEventListener('change', this._onAppPaused); &#125; onAppPaused(event)&#123; &#125;&#125; 除上述的做法外，还可以这样做： 1234567891011class PauseMenu extends React.Component&#123; componentWillMount()&#123; AppStateIOS.addEventListener('change', this.onAppPaused); &#125; componentWillUnmount()&#123; AppStateIOS.removeEventListener('change', this.onAppPaused); &#125; onAppPaused = (event) =&gt; &#123; //把函数直接作为一个arrow function的属性来定义，初始化的时候就绑定好了this指针 &#125;&#125; 函数参数默认值ES6支持在定义函数的时候为其设置默认值： 1234function foo(height = 50, color = 'red')&#123; // ...&#125; 不使用默认值： 123456function foo(height, color)&#123; var height = height || 50; var color = color || 'red'; //...&#125; 这样写一般没问题，但当参数的布尔值为 false 时，就会有问题了。 比如，我们这样调用foo函数： 1foo(0, "") 因为 0 的布尔值为 false，这样 height 的取值将是50。同理 color 的取值为 ‘red’。 所以说，函数参数默认值不仅能是代码变得更加简洁而且能规避一些问题。 模板字符串ES6 支持模板字符串，使得字符串的拼接更加的简洁、直观。 不使用模板字符串： 1var name = 'Your name is ' + first + ' ' + last + '.' 使用模板字符串： 1var name = `Your name is $&#123;first&#125; $&#123;last&#125;.` 在 ES6 中通过${}就可以完成字符串的拼接，只需要将变量放在大括号之中。 解构赋值解构赋值语法是 JavaScript 的一种表达式，可以方便的从数组或者对象中快速提取值赋给定义的变量。 获取数组中的值从数组中获取值并赋值到变量中，变量的顺序与数组中对象顺序对应。 123456789101112131415161718var foo = ["one", "two", "three", "four"];var [a, b, c] = foo;console.log(a); // "one"console.log(b); // "two"console.log(c); // "three"//如果你要忽略某些值，你可以按照下面的写法获取你想要的值var [first, , , last] = foo;console.log(first); // "one"console.log(last); // "four"//你也可以这样写var a, b; //先声明变量[a, b] = [1, 2];console.log(a); // 1console.log(b); // 2 如果没有从数组中的获取到值，你可以为变量设置一个默认值。 12345var a, b;[a=5, b=7] = [1];console.log(a); // 1console.log(b); // 7 通过解构赋值可以方便的交换两个变量的值。 123456var a = 1;var b = 3;[a, b] = [b, a];console.log(a); // 3console.log(b); // 1 获取对象中的值12345678910const student = &#123; name:'Ming', age:'18', city:'Shanghai' &#125;;const &#123;name, age, city&#125; = student;console.log(name); // "Ming"console.log(age); // "18"console.log(city); // "Shanghai" 延展操作符( Spread operator )延展操作符...可以在函数调用/数组构造时，将数组表达式或者 string 在语法层面展开；还可以在构造对象时，将对象表达式按 key-value 的方式展开。 语法函数调用 1myFunction(...iterableObj); 数组构造或字符串 1[...iterableObj, '4', ...'hello', 6]; 构造对象时，进行克隆或者属性拷贝（ECMAScript 2018规范新增特性） 1let objClone = &#123; ...obj &#125;; 应用场景在函数调用时使用延展操作符 12345678910function sum(x, y, z) &#123; return x + y + z;&#125;const numbers = [1, 2, 3];//不使用延展操作符console.log(sum.apply(null, numbers));//使用延展操作符console.log(sum(...numbers));// 6 构造数组 没有展开语法的时候，只能组合使用 push、splice、concat 等方法，来将已有数组元素变成新数组的一部分。有了展开语法，构造新数组会变得更简单、更优雅： 123const stuendts = ['Jine', 'Tom']; const persons = ['Tony', ...stuendts, 'Aaron', 'Anna'];conslog.log(persions) // ["Tony", "Jine", "Tom", "Aaron", "Anna"] 和参数列表的展开类似，... 在构造字数组时，可以在任意位置多次使用。 数组拷贝 1234var arr = [1, 2, 3];var arr2 = [...arr]; // 等同于 arr.slice()arr2.push(4); console.log(arr2) //[1, 2, 3, 4] 展开语法和 Object.assign() 行为一致, 执行的都是浅拷贝(只遍历一层)。 连接多个数组 12345var arr1 = [0, 1, 2];var arr2 = [3, 4, 5];var arr3 = [...arr1, ...arr2]; // 将 arr2 中所有元素附加到 arr1 后面并返回// 等同于var arr4 = arr1.concat(arr2); 在ECMAScript 2018中延展操作符增加了对对象的支持 12345678var obj1 = &#123; foo: 'bar', x: 42 &#125;;var obj2 = &#123; foo: 'baz', y: 13 &#125;;var clonedObj = &#123; ...obj1 &#125;;// 克隆后的对象: &#123; foo: "bar", x: 42 &#125;var mergedObj = &#123; ...obj1, ...obj2 &#125;;// 合并后的对象: &#123; foo: "baz", x: 42, y: 13 &#125; 在React中的应用通常我们在封装一个组件时，会对外公开一些 props 用于实现功能。大部分情况下在外部使用都应显示的传递 props 。但是当传递大量的props时，会非常繁琐，这时我们可以使用 ...(延展操作符，用于取出参数对象的所有可遍历属性) 来进行传递。 一般情况下我们应该这样写： 1&lt;CustomComponent name ='Jine' age =&#123;21&#125; /&gt; 使用 ... ，等同于上面的写法 12345const params = &#123; name: 'Jine', age: 21&#125;&lt;CustomComponent &#123;...params&#125; /&gt; 配合解构赋值避免传入一些不需要的参数 1234567891011var params = &#123; name: '123', title: '456', type: 'aaa'&#125;var &#123; type, ...other &#125; = params;&lt;CustomComponent type='normal' number=&#123;2&#125; &#123;...other&#125; /&gt;//等同于&lt;CustomComponent type='normal' number=&#123;2&#125; name='123' title='456' /&gt; 对象属性简写在ES6中允许我们在设置一个对象的属性的时候不指定属性名。 不使用ES6 12345678const name='Ming', age='18', city='Shanghai';const student = &#123; name:name, age:age, city:city&#125;;console.log(student); //&#123;name: "Ming", age: "18", city: "Shanghai"&#125; 对象中必须包含属性和值，显得非常冗余。 使用ES6 12345678const name='Ming', age='18', city='Shanghai';const student = &#123; name, age, city&#125;;console.log(student); //&#123;name: "Ming", age: "18", city: "Shanghai"&#125; 对象中直接写变量，非常简洁。 PromisePromise 是异步编程的一种解决方案，比传统的解决方案callback更加的优雅。它最早由社区提出和实现的，ES6 将其写进了语言标准，统一了用法，原生提供了Promise对象。 不使用ES6 嵌套两个 setTimeout 回调函数： 12345678setTimeout(function()&#123; console.log('Hello'); // 1秒后输出 "Hello" setTimeout(function() &#123; console.log('Hi'); // 2秒后输出 "Hi" &#125;, 1000);&#125;, 1000); 使用ES6 123456789101112131415var waitSecond = new Promise(function(resolve, reject)&#123; setTimeout(resolve, 1000);&#125;);waitSecond .then(function() &#123; console.log("Hello"); // 1秒后输出"Hello" return waitSecond; &#125;) .then(function() &#123; console.log("Hi"); // 2秒后输出"Hi" &#125;); 上面的的代码使用两个 then 来进行异步编程串行化，避免了回调地狱： 支持 let 与 const在之前 JS 是没有块级作用域的，const 与 let 填补了这方便的空白，const 与 let 都是块级作用域。 使用 var 定义的变量为函数级作用域 12345&#123; var a = 10;&#125;console.log(a); // 输出10 使用 let 与 const 定义的变量为块级作用域 12345&#123; let a = 10;&#125;console.log(a); //-1 or Error“ReferenceError: a is not defined” ES7新特性（2016）ES2016添加了两个小的特性来说明标准化过程： 数组 includes() 方法，用来判断一个数组是否包含一个指定的值，根据情况，如果包含则返回 true，否则返回 false。 a ** b 指数运算符，它与 Math.pow(a, b) 相同。 Array.prototype.includes()includes() 函数用来判断一个数组是否包含一个指定的值，如果包含则返回 true，否则返回false。 includes 函数与 indexOf 函数很相似，下面两个表达式是等价的： 12arr.includes(x)arr.indexOf(x) &gt;= 0 在 ES7 之前的做法 使用indexOf()验证数组中是否存在某个元素，这时需要根据返回值是否为-1来判断： 123456let arr = ['react', 'angular', 'vue'];if (arr.indexOf('react') !== -1)&#123; console.log('react存在');&#125; 使用 ES7 的 includes() 使用 includes() 验证数组中是否存在某个元素，这样更加直观简单 123456let arr = ['react', 'angular', 'vue'];if (arr.includes('react'))&#123; console.log('react存在');&#125; 指数操作符在 ES7 中引入了指数运算符**，**具有与Math.pow(..)等效的计算结果。 不使用指数操作符 使用自定义的递归函数 calculateExponent 或者 Math.pow() 进行指数运算： 1234567891011121314function calculateExponent(base, exponent)&#123; if (exponent === 1) &#123; return base; &#125; else &#123; return base * calculateExponent(base, exponent - 1); &#125;&#125;console.log(calculateExponent(2, 10)); // 输出1024console.log(Math.pow(2, 10)); // 输出1024 使用指数操作符 使用指数运算符**，就像+、-等操作符一样： 1console.log(2**10);// 输出1024 ES8新特性（2017） async/await Object.values() Object.entries() String padding: padStart()和padEnd()，填充字符串达到当前长度 函数参数列表结尾允许逗号 Object.getOwnPropertyDescriptors() ShareArrayBuffer和Atomics对象，用于从共享内存位置读取和写入 async/await ES6 提供的 Promise 方法和 ES7 提供的 Async/Await 语法糖可以更好解决多层回调问题 Promise 对象用于表示一个异步操作的最终状态（完成或失败），以及其返回的值。 async 表示这是一个 async 函数，即异步函数。 await 操作符用于等待一个 Promise 对象。它只能在异步函数 async function 中使用，Promise 返回结果后，再继续往下执行。 await 后面跟着的应该是一个 Promise 对象（当然，其他返回值也没关系，只是会立即执行，不过那样就没有意义了…） await 等待的虽然是 Promise 对象，但不必写.then(..)，直接可以得到返回值。 Object.values()Object.values()是一个与Object.keys()类似的新函数，但返回的是Object自身属性的所有值，不包括继承的值。 假设我们要遍历如下对象obj的所有值： 1const obj = &#123;a: 1, b: 2, c: 3&#125;; 不使用Object.values() :ES7 12const vals=Object.keys(obj).map(key=&gt;obj[key]);console.log(vals);//[1, 2, 3] 使用Object.values() :ES8 12const values=Object.values(obj1);console.log(values);//[1, 2, 3] 从上述代码中可以看出Object.values()为我们省去了遍历key，并根据这些key获取value的步骤。 Object.entries()Object.entries()函数返回一个给定对象自身可枚举属性的键值对的数组。 接下来我们来遍历上文中的obj对象的所有属性的key和value： 不使用Object.entries() :ES7 123456Object.keys(obj).forEach(key=&gt;&#123; console.log('key:'+key+' value:'+obj[key]);&#125;)//key:a value:1//key:b value:2//key:c value:3 使用Object.entries() :ES8 123456for(let [key,value] of Object.entries(obj1))&#123; console.log(`key: $&#123;key&#125; value:$&#123;value&#125;`)&#125;//key:a value:1//key:b value:2//key:c value:3 String padding在 ES8 中 String 新增了两个实例函数String.prototype.padStart和String.prototype.padEnd，允许将空字符串或其他字符串添加到原始字符串的开头或结尾。 String.padStart(targetLength,[padString]) targetLength：当前字符串需要填充到的目标长度。如果这个数值小于当前字符串的长度，则返回当前字符串本身。 padString(可选)：填充字符串。如果字符串太长，使填充后的字符串长度超过了目标长度，则只保留最左侧的部分，其他部分会被截断，此参数的缺省值为 “ “。 12console.log('0.0'.padStart(4,'10')) //10.0console.log('0.0'.padStart(20)) // 0.00 String.padEnd(targetLength,padString]) targetLength：当前字符串需要填充到的目标长度。如果这个数值小于当前字符串的长度，则返回当前字符串本身。 padString(可选)：填充字符串。如果字符串太长，使填充后的字符串长度超过了目标长度，则只保留最左侧的部分，其他部分会被截断，此参数的缺省值为 “ “； 12console.log('0.0'.padEnd(4,'0')) //0.00 console.log('0.0'.padEnd(10,'0')) // 0.00000000 函数参数列表结尾允许逗号主要作用是方便使用 git 进行多人协作开发时修改同一个函数减少不必要的行变更。 Object.getOwnPropertyDescriptors()Object.getOwnPropertyDescriptors()函数用来获取一个对象的所有自身属性的描述符，如果没有任何自身属性，则返回空对象。 函数原型： 1Object.getOwnPropertyDescriptors(obj) 返回obj对象的所有自身属性的描述符，如果没有任何自身属性，则返回空对象。 12345678910111213141516171819const obj2 = &#123; name: 'Jine', get age() &#123; return '18' &#125;&#125;;Object.getOwnPropertyDescriptors(obj2)// &#123;// age: &#123;// configurable: true,// enumerable: true,// get: function age()&#123;&#125;, //the getter function// set: undefined// &#125;,// name: &#123;// configurable: true,// enumerable: true,// value:"Jine",// writable:true// &#125;// &#125; SharedArrayBuffer对象SharedArrayBuffer 对象用来表示一个通用的，固定长度的原始二进制数据缓冲区，类似于 ArrayBuffer 对象，它们都可以用来在共享内存（shared memory）上创建视图。与 ArrayBuffer 不同的是，SharedArrayBuffer 不能被分离。 123456/** * * @param &#123;*&#125; length 所创建的数组缓冲区的大小，以字节(byte)为单位。 * @returns &#123;SharedArrayBuffer&#125; 一个大小指定的新 SharedArrayBuffer 对象。其内容被初始化为 0。 */new SharedArrayBuffer(length) Atomics对象Atomics 对象提供了一组静态方法用来对 SharedArrayBuffer 对象进行原子操作。 这些原子操作属于 Atomics 模块。与一般的全局对象不同，Atomics 不是构造函数，因此不能使用 new 操作符调用，也不能将其当作函数直接调用。Atomics 的所有属性和方法都是静态的（与 Math 对象一样）。 多个共享内存的线程能够同时读写同一位置上的数据。原子操作会确保正在读或写的数据的值是符合预期的，即下一个原子操作一定会在上一个原子操作结束后才会开始，其操作过程不会中断。 Atomics.add() 将指定位置上的数组元素与给定的值相加，并返回相加前该元素的值。 Atomics.and() 将指定位置上的数组元素与给定的值相与，并返回与操作前该元素的值。 Atomics.compareExchange() 如果数组中指定的元素与给定的值不等，则将其更新为新的值，并返回该元素原先的值。 Atomics.exchange() 将数组中指定的元素更新为给定的值，并返回该元素更新前的值。 Atomics.load() 返回数组中指定元素的值。 Atomics.or() 将指定位置上的数组元素与给定的值相或，并返回或操作前该元素的值。 Atomics.store() 将数组中指定的元素设置为给定的值，并返回该值。 Atomics.sub() 将指定位置上的数组元素与给定的值相减，并返回相减前该元素的值。 Atomics.xor() 将指定位置上的数组元素与给定的值相异或，并返回异或操作前该元素的值。 wait() 和 wake() 方法采用的是 Linux 上的 futexes 模型（fast user-space mutex，快速用户空间互斥量），可以让进程一直等待直到某个特定的条件为真，主要用于实现阻塞。 Atomics.wait() 检测数组中某个指定位置上的值是否仍然是给定值，是则保持挂起直到被唤醒或超时。返回值为 “ok”、”not-equal” 或 “time-out”。调用时，如果当前线程不允许阻塞，则会抛出异常（大多数浏览器都不允许在主线程中调用 wait()）。 Atomics.wake() 唤醒等待队列中正在数组指定位置的元素上等待的线程。返回值为成功唤醒的线程数量。 Atomics.isLockFree(size) 可以用来检测当前系统是否支持硬件级的原子操作。对于指定大小的数组，如果当前系统支持硬件级的原子操作，则返回 true；否则就意味着对于该数组，Atomics 对象中的各原子操作都只能用锁来实现。此函数面向的是技术专家。–&gt; ES9新特性（2018） 异步迭代 Promise.finally() Rest/Spread 属性 正则表达式命名捕获组（Regular Expression Named Capture Groups） 正则表达式反向断言（lookbehind） 正则表达式dotAll模式 正则表达式 Unicode 转义 非转义序列的模板字符串 异步迭代在async/await的某些时刻，你可能尝试在同步循环中调用异步函数。例如： 12345async function process(array) &#123; for (let i of array) &#123; await doSomething(i); &#125;&#125; 这段代码不会正常运行，下面这段同样也不会： 12345async function process(array) &#123; array.forEach(async i =&gt; &#123; await doSomething(i); &#125;);&#125; 这段代码中，循环本身依旧保持同步，并在在内部异步函数之前全部调用完成。 ES2018 引入异步迭代器（asynchronous iterators），这就像常规迭代器，除了next()方法返回一个Promise。因此await可以和for...of循环一起使用，以串行的方式运行异步操作。例如： 12345async function process(array) &#123; for await (let i of array) &#123; doSomething(i); &#125;&#125; Promise.finally()一个 Promise 调用链要么成功到达最后一个.then()，要么失败触发.catch()。在某些情况下，你想要在无论Promise运行成功还是失败，运行相同的代码，例如清除，删除对话，关闭数据库连接等。 .finally()允许你指定最终的逻辑： 1234567891011function doSomething() &#123; doSomething1() .then(doSomething2) .then(doSomething3) .catch(err =&gt; &#123; console.log(err); &#125;) .finally(() =&gt; &#123; // finish here! &#125;);&#125; Rest/Spread 属性ES2015 引入了Rest参数和扩展运算符。三个点（…）仅用于数组。Rest参数语法允许我们将一个不定数量的参数表示为一个数组。 1234567restParam(1, 2, 3, 4, 5);function restParam(p1, p2, ...p3) &#123; // p1 = 1 // p2 = 2 // p3 = [3, 4, 5]&#125; 展开操作符以相反的方式工作，将数组转换成可传递给函数的单独参数。例如Math.max()返回给定数字中的最大值： 12const values = [99, 100, -1, 48, 16];console.log( Math.max(...values) ); // 100 ES2018 为对象解构提供了和数组一样的Rest参数（）和展开操作符，一个简单的例子： 123456789const myObject = &#123; a: 1, b: 2, c: 3&#125;;const &#123; a, ...x &#125; = myObject;// a = 1// x = &#123; b: 2, c: 3 &#125; 或者你可以使用它给函数传递参数： 12345678910restParam(&#123; a: 1, b: 2, c: 3&#125;);function restParam(&#123; a, ...x &#125;) &#123; // a = 1 // x = &#123; b: 2, c: 3 &#125;&#125; 跟数组一样，Rest参数只能在声明的结尾处使用。此外，它只适用于每个对象的顶层，如果对象中嵌套对象则无法适用。 扩展运算符可以在其他对象内使用，例如： 123const obj1 = &#123; a: 1, b: 2, c: 3 &#125;;const obj2 = &#123; ...obj1, z: 26 &#125;;// obj2 is &#123; a: 1, b: 2, c: 3, z: 26 &#125; 可以使用扩展运算符拷贝一个对象，像是这样obj2 = {...obj1}，但是 这只是一个对象的浅拷贝。另外，如果一个对象A的属性是对象B，那么在克隆后的对象 cloneB 中，该属性指向对象B。 正则表达式命名捕获组JavaScript 正则表达式可以返回一个匹配的对象——一个包含匹配字符串的类数组，例如：以YYYY-MM-DD的格式解析日期： 123456const reDate = /([0-9]&#123;4&#125;)-([0-9]&#123;2&#125;)-([0-9]&#123;2&#125;)/, match = reDate.exec('2018-04-30'), year = match[1], // 2018 month = match[2], // 04 day = match[3]; // 30 这样的代码很难读懂，并且改变正则表达式的结构有可能改变匹配对象的索引。 ES2018 允许命名捕获组使用符号?&lt;name&gt;，在打开捕获括号(后立即命名，示例如下： 123456const reDate = /(?&lt;year&gt;[0-9]&#123;4&#125;)-(?&lt;month&gt;[0-9]&#123;2&#125;)-(?&lt;day&gt;[0-9]&#123;2&#125;)/, match = reDate.exec('2018-04-30'), year = match.groups.year, // 2018 month = match.groups.month, // 04 day = match.groups.day; // 30 任何匹配失败的命名组都将返回undefined。 命名捕获也可以使用在replace()方法中。例如将日期转换为美国的 MM-DD-YYYY 格式： 1234const reDate = /(?&lt;year&gt;[0-9]&#123;4&#125;)-(?&lt;month&gt;[0-9]&#123;2&#125;)-(?&lt;day&gt;[0-9]&#123;2&#125;)/, d = '2018-04-30', usDate = d.replace(reDate, '$&lt;month&gt;-$&lt;day&gt;-$&lt;year&gt;'); 正则表达式反向断言目前 JavaScript 在正则表达式中支持先行断言（lookahead）。这意味着匹配会发生，但不会有任何捕获，并且断言没有包含在整个匹配字段中。例如从价格中捕获货币符号： 12345const reLookahead = /\D(?=\d+)/, match = reLookahead.exec('$123.89');console.log( match[0] ); // $ ES2018 引入以相同方式工作但是匹配前面的反向断言（lookbehind），这样我就可以忽略货币符号，单纯的捕获价格的数字： 12345const reLookbehind = /(?&lt;=\D)\d+/, match = reLookbehind.exec('$123.89');console.log( match[0] ); // 123.89 以上是 肯定反向断言，非数字\D必须存在。同样的，还存在 否定反向断言，表示一个值必须不存在，例如： 12345const reLookbehindNeg = /(?&lt;!\D)\d+/, match = reLookbehind.exec('$123.89');console.log( match[0] ); // null 正则表达式dotAll模式正则表达式中点.匹配除回车外的任何单字符，标记s改变这种行为，允许行终止符的出现，例如： 12/hello.world/.test('hello\nworld'); // false/hello.world/s.test('hello\nworld'); // true 正则表达式 Unicode 转义到目前为止，在正则表达式中本地访问 Unicode 字符属性是不被允许的。ES2018添加了 Unicode 属性转义——形式为\p{...}和\P{...}，在正则表达式中使用标记 u (unicode) 设置，在\p块儿内，可以以键值对的方式设置需要匹配的属性而非具体内容。例如： 12const reGreekSymbol = /\p&#123;Script=Greek&#125;/u;reGreekSymbol.test('π'); // true 此特性可以避免使用特定 Unicode 区间来进行内容类型判断，提升可读性和可维护性。 非转义序列的模板字符串之前，\u开始一个 unicode 转义，\x开始一个十六进制转义，\后跟一个数字开始一个八进制转义。这使得创建特定的字符串变得不可能，例如Windows文件路径 C:\uuu\xxx\111。更多细节参考模板字符串。 ES10新特性（2019） 行分隔符（U + 2028）和段分隔符（U + 2029）符号现在允许在字符串文字中，与JSON匹配 更加友好的 JSON.stringify 新增了Array的flat()方法和flatMap()方法 新增了String的trimStart()方法和trimEnd()方法 Object.fromEntries() Symbol.prototype.description String.prototype.matchAll Function.prototype.toString()现在返回精确字符，包括空格和注释 简化try {} catch {},修改 catch 绑定 新的基本数据类型BigInt globalThis import() Legacy RegEx 私有的实例方法和访问器 行分隔符（U + 2028）和段分隔符（U + 2029）符号现在允许在字符串文字中，与 JSON 匹配以前，这些符号在字符串文字中被视为行终止符，因此使用它们会导致 SyntaxError 异常。 更加友好的 JSON.stringify如果输入 Unicode 格式但是超出范围的字符，在原先 JSON.stringify 返回格式错误的Unicode 字符串。现在实现了一个改变 JSON.stringify 的第3阶段提案，因此它为其输出转义序列，使其成为有效Unicode（并以UTF-8表示） 新增了 Array 的 flat() 方法和 flatMap() 方法flat() 和 flatMap()本质上就是是归纳（reduce） 与 合并（concat）的操作。 Array.prototype.flat()flat() 方法会按照一个可指定的深度递归遍历数组，并将所有元素与遍历到的子数组中的元素合并为一个新数组返回。 flat()方法最基本的作用就是数组降维 123456789101112131415var arr1 = [1, 2, [3, 4]];arr1.flat(); // [1, 2, 3, 4]var arr2 = [1, 2, [3, 4, [5, 6]]];arr2.flat();// [1, 2, 3, 4, [5, 6]]var arr3 = [1, 2, [3, 4, [5, 6]]];arr3.flat(2);// [1, 2, 3, 4, 5, 6]//使用 Infinity 作为深度，展开任意深度的嵌套数组arr3.flat(Infinity); // [1, 2, 3, 4, 5, 6] 其次，还可以利用flat()方法的特性来去除数组的空项 123var arr4 = [1, 2, , 4, 5];arr4.flat();// [1, 2, 4, 5] Array.prototype.flatMap()flatMap() 方法首先使用映射函数映射每个元素，然后将结果压缩成一个新数组。它与 map 和 深度值1 的 flat 几乎相同，但 flatMap 通常在合并成一种方法的效率稍微高一些。 这里我们拿map方法与flatMap方法做一个比较。 1234567891011var arr1 = [1, 2, 3, 4];arr1.map(x =&gt; [x * 2]); // [[2], [4], [6], [8]]arr1.flatMap(x =&gt; [x * 2]);// [2, 4, 6, 8]// 只会将 flatMap 中的函数返回的数组 “压平” 一层arr1.flatMap(x =&gt; [[x * 2]]);// [[2], [4], [6], [8]] 新增了String的 trimStart() 方法和 trimEnd() 方法新增的这两个方法很好理解，分别去除字符串首尾空白字符。 Object.fromEntries()Object.entries()方法的作用是返回一个给定对象自身可枚举属性的键值对数组，其排列与使用 for…in 循环遍历该对象时返回的顺序一致（区别在于 for-in 循环也枚举原型链中的属性）。 而Object.fromEntries() 则是 Object.entries() 的反转。 Object.fromEntries() 函数传入一个键值对的列表，并返回一个带有这些键值对的新对象。这个迭代参数应该是一个能够实现@iterator方法的的对象，返回一个迭代器对象。它生成一个具有两个元素的类似数组的对象，第一个元素是将用作属性键的值，第二个元素是与该属性键关联的值。 通过 Object.fromEntries， 可以将 Map 转化为 Object: 123const map = new Map([ ['foo', 'bar'], ['baz', 42] ]);const obj = Object.fromEntries(map);console.log(obj); // &#123; foo: "bar", baz: 42 &#125; 通过 Object.fromEntries， 可以将 Array 转化为 Object: 1234const arr = [ ['0', 'a'], ['1', 'b'], ['2', 'c'] ];const obj = Object.fromEntries(arr);console.log(obj); // &#123; 0: "a", 1: "b", 2: "c" &#125;复制代码 Symbol.prototype.description通过工厂函数 Symbol() 创建符号时，您可以选择通过参数提供字符串作为描述： 1const sym = Symbol('The description'); 以前，访问描述的唯一方法是将符号转换为字符串： 1assert.equal(String(sym), 'Symbol(The description)'); 现在引入了getter Symbol.prototype.description 以直接访问描述： 1assert.equal(sym.description, 'The description'); String.prototype.matchAllmatchAll() 方法返回一个包含所有匹配正则表达式及分组捕获结果的迭代器。 在 matchAll 出现之前，通过在循环中调用 regexp.exec 来获取所有匹配项信息（ regexp 需使用 /g 标志： 12345678const regexp = RegExp('foo*','g');const str = 'table football, foosball';while ((matches = regexp.exec(str)) !== null) &#123; console.log(`Found $&#123;matches[0]&#125;. Next starts at $&#123;regexp.lastIndex&#125;.`); // expected output: "Found foo. Next starts at 9." // expected output: "Found foo. Next starts at 19."&#125; 如果使用 matchAll ，就可以不必使用 while 循环加 exec 方式（且正则表达式需使用／g标志）。使用 matchAll 会得到一个迭代器的返回值，配合 for…of，array spread，or Array.from() 可以更方便实现功能： 12345678910111213141516const regexp = RegExp('foo*','g'); const str = 'table football, foosball';let matches = str.matchAll(regexp);for (const match of matches) &#123; console.log(match);&#125;// Array [ "foo" ]// Array [ "foo" ]// matches iterator is exhausted after the for..of iteration// Call matchAll again to create a new iteratormatches = str.matchAll(regexp);Array.from(matches, m =&gt; m[0]);// Array [ "foo", "foo" ] matchAll 可以更好的用于分组1234567891011var regexp = /t(e)(st(\d?))/g;var str = 'test1test2';str.match(regexp); // Array ['test1', 'test2']let array = [...str.matchAll(regexp)];array[0];// ['test1', 'e', 'st1', '1', index: 0, input: 'test1test2', length: 4]array[1];// ['test2', 'e', 'st2', '2', index: 5, input: 'test1test2', length: 4] Function.prototype.toString() 现在返回精确字符，包括空格和注释123456789101112function /* comment */ foo /* another comment */() &#123;&#125;// 之前不会打印注释部分console.log(foo.toString()); // function foo()&#123;&#125;// ES2019 会把注释一同打印console.log(foo.toString()); // function /* comment */ foo /* another comment */ ()&#123;&#125;// 箭头函数const bar /* comment */ = /* another comment */ () =&gt; &#123;&#125;;console.log(bar.toString()); // () =&gt; &#123;&#125; 修改 catch 绑定在 ES10 之前，我们必须通过语法为 catch 子句绑定异常变量，无论是否有必要。很多时候 catch 块是多余的。 ES10 提案使我们能够简单的把变量省略掉。 不算大的改动。 之前是 1try &#123;&#125; catch(e) &#123;&#125; 现在是 1try &#123;&#125; catch &#123;&#125; 新的基本数据类型 BigInt现在的基本数据类型（值类型）不止5种（ES6之后是六种），加上 BigInt 一共有七种基本数据类型，分别是： String、Number、Boolean、Null、Undefined、Symbol、BigInt]]></content>
      <categories>
        <category>前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>ECMAScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue created 与 mounted]]></title>
    <url>%2F2020%2F11%2F09%2FVue%20created%20%E4%B8%8E%20mounted%2F</url>
    <content type="text"><![CDATA[Vue created 与 mounted 区别 生命周期 是否获取dom节点 是否可以获取data 是否获取methods beforeCreate 否 否 否 created 否 是 是 beforeMount 否 是 是 mounted 是 是 是 created：在模板渲染成 html 前调用，即通常初始化某些属性值，然后再渲染成视图。 mounted：在模板渲染成 html 后调用，通常是初始化页面完成后，再对 html 的dom 节点进行一些需要的操作。 如果要类似 let ctx = document.getElementById(ID)获取 dom 元素，只能等这个html 渲染完后才可以进行，那么就用 mounted ，如果不需要 created 即可。]]></content>
      <categories>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>Vue</tag>
        <tag>Mixin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa @Entity的回调方法]]></title>
    <url>%2F2020%2F11%2F09%2FSpringDataJpa%E7%9A%84%40Entity%E5%9B%9E%E8%B0%83%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[@Entity 的回调方法JPA 的 Callbacks 有哪些JPA 协议里面规定，可以通过一些注解，为其监听回调事件、指定回调方法。 回调事件注解表： Annotation Description @PrePersist EntityManager.persist 方法调用之前的回调注解，即新增之前调用 @PostPersist EntityManager.persist 方法之后调用的回调注解，EntityManager.flush 或 EntityManager.commit 方法之后调用的此方法，即保存到数据库之后进行调用 @PreRemove EntityManager.remove 之前调用的回调注解，即删除之前调用 @PostRemove EntityManager.remove 之后调用的回调注解，即删除之后调用 @PreUpdate 在实体更新之前调用，所谓的更新起始是在merge之后，实体发生变化，这一注解可以在变化储存到数据库之前调用 @PostUpdate 在实体更新之后调用，即实体的字段的值变化之后，在调用EntityManager.flush 或 EntityManager.commit 方法之后调用的此方法 @postLoad 在实体从DB加载到程序里面之后回调 注意： 回调函数都是和 EntityManager.flush 或 EntityManager.commit 在同一个线程里面执行的，只不过调用方法有先后之分，都是同步调用，所以当任何一个回调方法里面发生异常，都会触发事务进行回滚，而不会触发事务提交。 Callbacks 注解可以放在实体里面，可以放在 super-class 里面，也可以定义在 Entity 的 listener 里面，但需要注意的是：放在实体（或者 super-class）里面的方法，签名格式为“void ()”，即没有参数，方法里面操作的是 this 对象自己；放在实体的 Entity Listener 里面的方法签名格式为“void (Object)”，也就是方法可以有参数，参数是代表用来接收回调方法的实体。 上述注解生效的回调方法可以是 public、private、protected、friendly 类型的，但是不能是 static 和 final 类型的方法。 JPA Callbacks 的使用方法在实体和 super-class 中使用第一步：修改 BaseEntity，在里面新增回调函数和注解，代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.example.jpa.example1.base;import lombok.Data;import org.springframework.data.annotation.*;import org.springframework.data.jpa.domain.support.AuditingEntityListener;import javax.persistence.*;import java.time.Instant;@Data@MappedSuperclass@EntityListeners(AuditingEntityListener.class)public class BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id;// @CreatedBy 这个可能会被 AuditingEntityListener 覆盖，为了方便测试，先注释掉 private Integer createUserId; @CreatedDate private Instant createTime; @LastModifiedBy private Integer lastModifiedUserId; @LastModifiedDate private Instant lastModifiedTime;// @Version 由于本身有乐观锁机制，这个测试的时候先注释掉，改用手动设置的值； private Integer version; @PreUpdate public void preUpdate() &#123; System.out.println("preUpdate::"+this.toString()); this.setCreateUserId(200); &#125; @PostUpdate public void postUpdate() &#123; System.out.println("postUpdate::"+this.toString()); &#125; @PreRemove public void preRemove() &#123; System.out.println("preRemove::"+this.toString()); &#125; @PostRemove public void postRemove() &#123; System.out.println("postRemove::"+this.toString()); &#125; @PostLoad public void postLoad() &#123; System.out.println("postLoad::"+this.toString()); &#125;&#125; 上述代码中，使用了@PreUpdate、@PostUpdate、@PreRemove、@PostRemove、@PostLoad 几个注解，并在相应的回调方法里面加了相应的日志。并且在 @PreUpdate 方法里面修改了 create_user_id 的值为 200，这样做是为了方便我们后续测试。 第二步：修改一下 User 类，也新增两个回调函数，并且和 BaseEntity 做法一样 123456789101112131415161718192021222324252627282930313233package com.example.jpa.example1;import com.example.jpa.example1.base.BaseEntity;import com.fasterxml.jackson.annotation.JsonIgnore;import lombok.*;import javax.persistence.*;import java.util.List;@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "addresses",callSuper = true)@EqualsAndHashCode(callSuper=false)public class User extends BaseEntity &#123;// implements Auditable&lt;Integer,Long, Instant&gt; &#123; private String name; private String email; @Enumerated(EnumType.STRING) private SexEnum sex; private Integer age; @OneToMany(mappedBy = "user") @JsonIgnore private List&lt;UserAddress&gt; addresses; private Boolean deleted; @PrePersist private void prePersist() &#123; System.out.println("prePersist::"+this.toString()); this.setVersion(1); &#125; @PostPersist public void postPersist() &#123; System.out.println("postPersist::"+this.toString()); &#125;&#125; 使用了 @PrePersist、@PostPersist 回调事件，为了方便测试，在 @PrePersist 里面将 version 修改为 1。 第三步：写一个测试用例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364@DataJpaTest@TestInstance(TestInstance.Lifecycle.PER_CLASS)@Import(JpaConfiguration.class)public class UserRepositoryTest &#123; @Autowired private UserRepository userRepository; @MockBean MyAuditorAware myAuditorAware; /** * 为了和测试方法的事务分开，我们在 init 里面初始化数据做新增操作 */ @BeforeAll @Rollback(false) @Transactional public void init() &#123; //由于测试用例模拟 web context 环境，这里利用 @MockBean，mock掉方法，期待返回13这个用户ID Mockito.when(myAuditorAware.getCurrentAuditor()).thenReturn(Optional.of(13)); User u1 = User.builder() .name("jack") .email("123456@126.com") .sex(SexEnum.BOY) .age(20) .build(); //没有save之前 version是null Assertions.assertNull(u1.getVersion()); userRepository.save(u1); //这里面触发保存方法，这个时候我们将version设置成了1，然后验证一下 Assertions.assertEquals(1,u1.getVersion()); &#125; /** * 测试一下更新和查询 */ @Test @Rollback(false) @Transactional public void testCallBackUpdate() &#123; //此时会触发@PostLoad事件 User u1 = userRepository.getOne(1L); //从db里面重新查询出来，验证一下version是不是1 Assertions.assertEquals(1,u1.getVersion()); u1.setSex(SexEnum.GIRL); //此时会触发@PreUpdate事件 userRepository.save(u1); List&lt;User&gt; u3 = userRepository.findAll(); u3.stream().forEach(u-&gt;&#123; //从db查询出来，验证一下CreateUserId是否为我们刚才修改的200 Assertions.assertEquals(200,u.getCreateUserId()); &#125;); &#125; /** * 测试一下删除事件 */ @Test @Rollback(false) @Transactional public void testCallBackDelete() &#123; //此时会触发@PostLoad事件 User u1 = userRepository.getOne(1L); Assertions.assertEquals(200,u1.getCreateUserId()); userRepository.delete(u1); //此时会触发@PreRemove、@PostRemove事件 System.out.println("delete_after::"); &#125;&#125; 通过测试用例验证了回调函数的事件后，看一下输出的 SQL 和日志： 通过上图的日志也可以看到响应的回调函数被触发了，并且可以看到执行顺序为： 保存：PrePersist -&gt; Insert -&gt; PostPersist 查询：Select -&gt; PostLoad 更新：PreUpdate -&gt; Update -&gt; PostUpdate 删除：PreRemove -&gt; Remove -&gt; PostRemove 若回调函数里面发生异常，数据会回滚，如在@PostPersist的方法里抛异常 12345@PostPersistpublic void postPersist() &#123; System.out.println("postPersist::"+this.toString()); throw new RuntimeException("jack test exception transactional roll back");&#125; 再跑测试用例就会发现，其中发生了 RollbackException 异常，这样的话数据是不会提交到 DB 里面的，也就会导致数据进行回滚，后面的业务流程无法执行下去。 12Could not commit JPA transaction; nested exception is javax.persistence.RollbackException: Error while committing the transactionorg.springframework.transaction.TransactionSystemException: Could not commit JPA transaction; nested exception is javax.persistence.RollbackException: Error while committing the transaction 所以在使用此方法时，你要注意考虑异常情况，避免不必要的麻烦。 自定义 EntityListener第一步：自定义一个 EntityLoggingListener 用来记录操作日志，通过 listener 的方式配置回调函数注解 12345678910111213141516171819202122232425262728293031323334353637383940package com.example.jpa.example1.base;import com.example.jpa.example1.User;import lombok.extern.log4j.Log4j2;import javax.persistence.*;@Log4j2public class EntityLoggingListener &#123; @PrePersist private void prePersist(BaseEntity entity) &#123; //entity.setVersion(1); 如果注释了，测试用例这个地方的验证也需要去掉 log.info("prePersist::&#123;&#125;",entity.toString()); &#125; @PostPersist public void postPersist(Object entity) &#123; log.info("postPersist::&#123;&#125;",entity.toString()); &#125; @PreUpdate public void preUpdate(BaseEntity entity) &#123; //entity.setCreateUserId(200); 如果注释了，测试用例这个地方的验证也需要去掉 log.info("preUpdate::&#123;&#125;",entity.toString()); &#125; @PostUpdate public void postUpdate(Object entity) &#123; log.info("postUpdate::&#123;&#125;",entity.toString()); &#125; @PreRemove public void preRemove(Object entity) &#123; log.info("preRemove::&#123;&#125;",entity.toString()); &#125; @PostRemove public void postRemove(Object entity) &#123; log.info("postRemove::&#123;&#125;",entity.toString()); &#125; @PostLoad public void postLoad(Object entity) &#123; //查询方法里面可以对一些敏感信息做一些日志 if (User.class.isInstance(entity)) &#123; log.info("postLoad::&#123;&#125;",entity.toString()); &#125; &#125;&#125; 在这一步骤中需要注意的是： 上面注释的代码，也可以改变 entity 里面的值，但是在这个 Listener 的里面我们不做修改，所以把 setVersion 和 setCreateUserId 注释掉了，要注意测试用例里面这两处也需要修改。 如果在 @PostLoad 里面记录日志，不一定每个实体、每次查询都需要记录日志，只需要对一些敏感的实体或者字段做日志记录即可。 回调函数时可以加上参数，这个参数可以是父类 Object，可以是 BaseEntity，也可以是具体的某一个实体；推荐用 BaseEntity，因为这样的方法是类型安全的，它可以约定一些框架逻辑，比如 getCreateUserId、getLastModifiedUserId 等。 第二步：写一个测试用例。 这次执行 testCallBackDelete()，看看会得到什么样的效果。 123452020-10-05 13:55:19.332 INFO 62541 --- [ Test worker] c.e.j.e.base.EntityLoggingListener : prePersist::User(super=BaseEntity(id=null, createUserId=13, createTime=2020-10-05T05:55:19.246Z, lastModifiedUserId=13, lastModifiedTime=2020-10-05T05:55:19.246Z, version=null), name=jack, email=123456@126.com, sex=BOY, age=20, deleted=null)2020-10-05 13:55:19.449 INFO 62541 --- [ Test worker] c.e.j.e.base.EntityLoggingListener : postPersist::User(super=BaseEntity(id=1, createUserId=13, createTime=2020-10-05T05:55:19.246Z, lastModifiedUserId=13, lastModifiedTime=2020-10-05T05:55:19.246Z, version=0), name=jack, email=123456@126.com, sex=BOY, age=20, deleted=null)2020-10-05 13:55:19.698 INFO 62541 --- [ Test worker] c.e.j.e.base.EntityLoggingListener : postLoad::User(super=BaseEntity(id=1, createUserId=13, createTime=2020-10-05T05:55:19.246Z, lastModifiedUserId=13, lastModifiedTime=2020-10-05T05:55:19.246Z, version=0), name=jack, email=123456@126.com, sex=BOY, age=20, deleted=null)2020-10-05 13:55:19.719 INFO 62541 --- [ Test worker] c.e.j.e.base.EntityLoggingListener : preRemove::User(super=BaseEntity(id=1, createUserId=13, createTime=2020-10-05T05:55:19.246Z, lastModifiedUserId=13, lastModifiedTime=2020-10-05T05:55:19.246Z, version=0), name=jack, email=123456@126.com, sex=BOY, age=20, deleted=null)2020-10-05 13:55:19.798 INFO 62541 --- [ Test worker] c.e.j.e.base.EntityLoggingListener : postRemove::User(super=BaseEntity(id=1, createUserId=13, createTime=2020-10-05T05:55:19.246Z, lastModifiedUserId=13, lastModifiedTime=2020-10-05T05:55:19.246Z, version=0), name=jack, email=123456@126.com, sex=BOY, age=20, deleted=null) 通过日志我们可以很清晰地看到 callback 注解标注的方法的执行过程，及其实体参数的值。你就会发现，原来自定义 EntityListener 回调函数的方法也是如此简单。 细心的你这个时候可能也会发现，我们上面其实应用了两个 EntityListener，所以这个时候 @EntityListeners 有个加载顺序的问题，你需要重点注意一下。 关于 @EntityListeners 加载顺序的说明 默认如果子类和父类都有 EntityListeners，那么 listeners 会按照加载的顺序执行所有 EntityListeners； EntityListeners 和实体里面的回调函数注解可以同时使用，但需要注意顺序问题； 如果我们不想加载 super-class 里面的EntityListeners，那么我们可以通过注解 @ExcludeSuperclassListeners，排除所有父类里面的实体监听者，需要用到的时候，我们再在子类实体里面重新引入即可，代码如下： 1234@ExcludeSuperclassListenerspublic class User extends BaseEntity &#123;......&#125; JPA Callbacks 的最佳实践个人经验总结了几个最佳实践 回调函数里面应尽量避免直接操作业务代码，最好用一些具有框架性的公用代码，如 Auditing 或 实体操作日志 等； 注意回调函数方法要在同一个事务中进行，异常要可预期，非可预期的异常要进行捕获，以免出现意想不到的线上 Bug； 回调函数方法是同步的，如果一些计算量大的和一些耗时的操作，可以通过发消息等机制异步处理，以免阻塞主流程，影响接口的性能。比如上面说的日志，如果我们要将其记录到数据库里面，可以在回调方法里面发个消息，改进之后将变成如下格式： 123456789101112131415161718192021222324252627282930public class AuditLoggingListener &#123; @PostLoad private void postLoad(Object entity) &#123; this.notice(entity, OperateType.load); &#125; @PostPersist private void postPersist(Object entity) &#123; this.notice(entity, OperateType.create); &#125; @PostRemove private void PostRemove(Object entity) &#123; this.notice(entity, OperateType.remove); &#125; @PostUpdate private void PostUpdate(Object entity) &#123; this.notice(entity, OperateType.update); &#125; private void notice(Object entity, OperateType type) &#123; //我们通过active mq 异步发出消息处理事件 ActiveMqEventManager.notice(new ActiveMqEvent(type, entity)); &#125; @Getter enum OperateType &#123; create("创建"), remove("删除"),update("修改"),load("查询"); private final String description; OperateType(String description) &#123; this.description=description; &#125; &#125;&#125; 在回调函数里面，尽量不要直接在操作 EntityManager 后再做 session 的整个生命周期的其他持久化操作，以免破坏事务的处理流程；也不要进行其他额外的关联关系更新动作，业务性的代码一定要放在 service 层面，否则太过复杂，时间长了代码很难维护； 回调函数里面比较适合用一些计算型的transient方法，如下面这个操作： 1234567public class UserListener &#123; @PrePersist public void prePersist(User user) &#123; //通过一些逻辑计算年龄； user.calculationAge(); &#125;&#125; JPA 官方比较建议放一些默认值，但是我不是特别赞同，因为觉得那样不够直观，直接用字段初始化就可以了，没必要在回调函数里面放置默认值。 除了日志，其他公用的场景不多。当遇到其他场景，可以根据不同的实体实际情况制定自己独有的 EntityListener 方法，如下： 12345678910@Entity@EntityListeners(UserListener.class)public class User extends BaseEntity &#123;// implements Auditable&lt;Integer,Long, Instant&gt; &#123; @Transient public void calculationAge() &#123; //通过一些逻辑计算年龄； this.age=10; &#125; ......//其他不重要的省略&#125; 例如，User 中我们有个计算年龄的逻辑要独立调用，就可以在持久化之前调用此方法，新建一个自己的 UserListener 即可，代码如下： 1234567public class UserListener &#123; @PrePersist public void prePersist(User user) &#123; //通过一些逻辑计算年龄； user.calculationAge(); &#125;&#125; JPA Callbacks 的实现原理，事件机制Java Persistence API 规定：JPA 的实现方需要实现功能，需要支持回调事件注解；而 Hibernate 内部负责实现，Hibernate 内部维护了一套实体的 EventType，其内部包含了各种回调事件，下面列举一下： 123456789public static final EventType&lt;PreLoadEventListener&gt; PRE_LOAD = create( "pre-load", PreLoadEventListener.class );public static final EventType&lt;PreDeleteEventListener&gt; PRE_DELETE = create( "pre-delete", PreDeleteEventListener.class );public static final EventType&lt;PreUpdateEventListener&gt; PRE_UPDATE = create( "pre-update", PreUpdateEventListener.class );public static final EventType&lt;PreInsertEventListener&gt; PRE_INSERT = create( "pre-insert", PreInsertEventListener.class );public static final EventType&lt;PostLoadEventListener&gt; POST_LOAD = create( "post-load", PostLoadEventListener.class );public static final EventType&lt;PostDeleteEventListener&gt; POST_DELETE = create( "post-delete", PostDeleteEventListener.class );public static final EventType&lt;PostUpdateEventListener&gt; POST_UPDATE = create( "post-update", PostUpdateEventListener.class );public static final EventType&lt;PostInsertEventListener&gt; POST_INSERT = create( "post-insert", PostInsertEventListener.class );更多的事件类型，你可以通过查看 org.hibernate.event.spi.EventType 类，了解更多；在 session factory 构建的时候，EventListenerRegistryImpl 负责注册这些事件，我们看一下 debug 的关键节点： 这部分原理不常用，知道有这么回事即可]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F11%2F09%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[简单工厂模式又叫做静态工厂方法（Static Factory Method） 简单工厂模式的实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类。 Spring 中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的标识来获得bean对象，但是否是在传入参数后创建还是传入参数前创建这个要根据具体情况来定。如下配置，就是在 HelloItxxz 类中创建一个 itxxzBean。 123456789101112&lt;beans&gt; &lt;bean id="singletonBean" class="com.itxxz.HelloItxxz"&gt; &lt;constructor-arg&gt; &lt;value&gt;Hello! 这是singletonBean&lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id="itxxzBean" class="com.itxxz.HelloItxxz" singleton="false"&gt; &lt;constructor-arg&gt; &lt;value&gt;Hello! 这是itxxzBean! &lt;/value&gt; &lt;/constructor-arg&gt; &lt;/bean&gt;&lt;/beans&gt; 单例模式懒汉模式线程不安全实现 123456789101112public class Singleton &#123; private static Singleton instance; private Singleton() &#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 双重检查模式实现 12345678910111213141516public class Singleton &#123; private static volatile Singleton singleton; private Singleton() &#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125;&#125; 饿汉单例模式123456789public class Singleton &#123; private static final Singleton instance = new Singleton(); private Singleton() &#123;&#125; public static Singleton getInstance() &#123; return instance; &#125;&#125; 枚举单例模式123public enum Singleton &#123; INSTANCE;&#125; 策略模式 ( Strategy Pattern )定义该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。 优点 使用策略模式可以避免使用多重条件语句 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离 缺点 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类 策略模式造成很多的策略类 实现继承实现定义一个抽象类，强制子类实现该方法 这种实现，对父类对子类不一定是透明的，如果父类提供了默认的实现，子类需要了解实现的细节，再决定是否重写 123456789101112131415161718public abstract class Action &#123; // 也可以将该方法设置成抽象方法， 强迫子类来实现该方法 public void execute() &#123; // 提供一个默认的实现 &#125;&#125;public class ActionModelA extends Action &#123; public void execute() &#123; aStyleBrake();// A 风格的行为 &#125;&#125;public class ActionModelB extends Action &#123; public void execute() &#123; bStyleBrake(); // B 风格的行为 &#125;&#125; 组合实现这种实现是透明的，只需要改变对象的引用就可以改变其行为。 123456789101112131415161718192021222324252627public interface IBrakeBehavior &#123; void execute();&#125;public class AStyleBrake implements IBrakeBehavior &#123; public void execute() &#123; aStyleBrake(); // A 风格的行为 &#125;&#125;public class BStyleBrake implements IBrakeBehavior &#123; public void execute() &#123; bStyleBrake(); // B 风格的行为 &#125;&#125;public class Action &#123; protected IBrakeBehavior brakeBehavior; public void execute() &#123; brakeBehavior.execute(); &#125; public void setBrakeBehavior(final IBrakeBehavior brakeType) &#123; this.brakeBehavior = brakeType; &#125;&#125; 最后通过工厂类，根据运行的参数选择出对应的实现 1Action action = ActionFactory.createAction(String parameters); action.execute(); 只有在策略选择里有条件选择语句，其他地方不出现。 如上述的createAction()方法 适配器模式 ( Adapter Pattern )定义将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。分为两种模式： 类适配器模式：适配器与适配者之间是继承（或实现）关系 对象适配器模式：适配器与适配者之间是关联关系 前者的耦合度比后者高，且要求开发了解现有组件库中的相关组件的内部结构，应用相对较少些。 优点 将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，而无须修改原有代码。 增加了类的透明性和复用性，将具体的实现封装在适配者类中，对于客户端类来说是透明的，而且提高了适配者的复用性。 灵活性和扩展性都非常好，通过使用配置文件，可以很方便地更换适配器，也可以在不修改原有代码的基础上增加新的适配器类，完全符合“开闭原则”。 类适配器模式优点： 由于适配器类是适配者类的子类，因此可以在适配器类中置换一些适配者的方法，使得适配器的灵活性更强。 对象适配器模式优点： 同一个适配器可以把适配者类和它的子类都适配到目标接口。 缺点类适配器模式缺点： 对于 Java 等不支持多重继承的语言，一次最多只能适配一个适配者类，而且目标抽象类只能为抽象类，不能为具体类，其使用有一定的局限性，不能将一个适配者类和它的子类都适配到目标接口。 对象适配器模式缺点： 与类适配器模式相比，要想置换适配者类的方法就不容易。如果一定要置换掉适配者类的一个或多个方法，就只好先做一个适配者类的子类，将适配者类的方法置换掉，然后再把适配者类的子类当做真正的适配者进行适配，实现过程较为复杂。 实现角色 Target（目标抽象类）：目标抽象类定义客户所需接口，可以是一个抽象类或接口，也可以是具体类。 Adapter（适配器类）：适配器可以调用另一个接口，作为一个转换器，对 Adaptee 和 Target 进行适配，适配器类是适配器模式的核心，在对象适配器中，它通过继承 Target 并关联一个 Adaptee 对象使二者产生联系。 Adaptee（适配者类）：适配者即被适配的角色，它定义了一个已经存在的接口，这个接口需要适配，适配者类一般是一个具体类，包含了客户希望使用的业务方法，在某些情况下可能没有适配者类的源代码。 类适配器首先有一个已存在的将被适配的类： 12345public class Adaptee &#123; public void adapteeRequest() &#123; System.out.println("被适配者的方法"); &#125;&#125; 定义一个目标接口： 123public interface Target &#123; void request();&#125; 如果通过一个适配器类，实现 Target 接口，同时继承了 Adaptee 类，然后在实现的 request() 方法中调用父类的 adapteeRequest() 即可实现 12345678public class Adapter extends Adaptee implements Target&#123; @Override public void request() &#123; //...一些操作... super.adapteeRequest(); //...一些操作... &#125;&#125; 对象适配器对象适配器与类适配器不同之处在于，类适配器通过继承来完成适配，对象适配器则是通过关联来完成，这里稍微修改一下 Adapter 类即可将转变为对象适配器 1234567891011public class Adapter implements Target&#123; // 适配者是对象适配器的一个属性 private Adaptee adaptee = new Adaptee(); @Override public void request() &#123; //... adaptee.adapteeRequest(); //... &#125;&#125; 装饰器模式在我们的项目中遇到这样一个问题：我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。我们以往在spring和hibernate框架中总是配置一个数据源，因而sessionFactory的dataSource属性总是指向这个数据源并且恒定不变，所有DAO在使用sessionFactory的时候都是通过这个数据源访问数据库。 但是现在，由于项目的需要，我们的DAO在访问sessionFactory的时候都不得不在多个数据源中不断切换，问题就出现了：如何让sessionFactory在执行数据持久化的时候，根据客户的需求能够动态切换不同的数据源？我们能不能在spring的框架下通过少量修改得到解决？是否有什么设计模式可以利用呢？ 首先想到在spring的applicationContext中配置所有的dataSource。这些dataSource可能是各种不同类型的，比如不同的数据库：Oracle、SQL Server、MySQL等，也可能是不同的数据源：比如apache 提供的org.apache.commons.dbcp.BasicDataSource、spring提供的org.springframework.jndi.JndiObjectFactoryBean等。然后sessionFactory根据客户的每次请求，将dataSource属性设置成不同的数据源，以到达切换数据源的目的。 spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator。 基本上都是动态地给一个对象添加一些额外的职责。 模板方法模式定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。Template Method使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。 Template Method模式一般是需要继承的。这里想要探讨另一种对Template Method的理解。spring中的JdbcTemplate，在用这个类时并不想去继承这个类，因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。怎么办？那我们就用回调对象吧。 在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。这可能是Template Method不需要继承的另一种实现方式。 以下是一个具体的例子： JdbcTemplate中的execute方法 ![](F:/CloudNode/后台/2-Java EE/Spring/image/6.png) 组合模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa Persistence Context 所表达的核心概念]]></title>
    <url>%2F2020%2F11%2F05%2FSpringDataJpa%E7%9A%84PersistenceContext%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Persistence Context 所表达的核心概念EntityManagerFactory 和 Persistence Unit 是什么按照 JPA 协议里面的定义：persistence unit 是一些持久化配置的集合，里面包含了数据源的配置、EntityManagerFactory 的配置，Spring 3.1 之前主要是通过 persistence.xml 的方式来配置一个 persistence unit，而 Spring 3.1 之后已经不再推荐这种方式了，但是还保留了 persistence unit 的概念，只需要在配置 LocalContainerEntityManagerFactory 的时候，指定 persistence unit 的名字即可，如： 12345678@Bean(name = "db2EntityManagerFactory")public LocalContainerEntityManagerFactoryBean entityManagerFactory(EntityManagerFactoryBuilder builder, @Qualifier("db2DataSource") DataSource db2DataSource) &#123; return builder.dataSource(db2DataSource) .packages("com.example.jpa.example1.db2") //数据2的实体所在的路径 .persistenceUnit("db2")// persistenceUnit的名字采用db2 .build();&#125; EntityManagerFactory 的用途就比较明显了，即根据不同的数据源，来管理 Entity 和创建 EntityManger，在整个 application 的生命周期中是单例状态。所以在 spring 的 application 里面获得 EntityManagerFactory 有两种方式。 第一种：通过 Spring 的 Bean 的方式注入。 123@Autowired@Qualifier(value="db2EntityManagerFactory") private EntityManagerFactory entityManagerFactory; 这种方式是比较推荐的，它利用了 Spring 自身的 Bean 的管理机制。 第二种：利用 java.persistence.PersistenceUnit 注解的方式获取。 12@PersistenceUnit("db2")private EntityManagerFactory entityManagerFactory; EntityManager 和 PersistenceContext 是什么按照 JPA 协议的规范，PersistenceContext 是用来管理会话里面的 Entity 状态的一个上下文环境，使 Entity 的实例有了不同的状态，也就是常说的实体实例的生命周期。 而这些实体在 PersistenceContext 中的不同状态都是通过 EntityManager 提供的一些方法进行管理的，也就是说： PersistenceContext 是持久化上下文，是 JPA 协议定义的，而 Hibernate 的实现是通过 Session 创建和销毁的，也就是说一个 Session 有且仅有一个 PersistenceContext； PersistenceContext 是持久化上下文，里面管理的是 Entity 的状态； EntityManager 是通过 PersistenceContext 创建的，用来管理 PersistenceContext 中 Entity 状态的方法，离开 PersistenceContext 持久化上下文，EntityManager 没有意义； EntityManger 是操作对象的唯一入口，一个请求里面可能会有多个 EntityManger 对象。 看一下 PersistenceContext 是怎么创建的。直接打开 SessionImpl 的构造方法，就可以知道 PersistenceContext 是和 Session 的生命周期绑定的，关键代码如下： 12345678910111213//session实例初始化的入口public SessionImpl(SessionFactoryImpl factory, SessionCreationOptions options) &#123; super( factory, options ); //Session里面创建了persistenceContext，每次session都是新对象 this.persistenceContext = createPersistenceContext();// ......省略一些不重要的代码 protected StatefulPersistenceContext createPersistenceContext() &#123; return new StatefulPersistenceContext( this );&#125;//StatefulPersistenceContext就是PersistenceContext的实现类public class StatefulPersistenceContext implements PersistenceContext &#123;// ......&#125; EntityManger 需要通过 @PersistenceContext 的方式进行获取，代码如下： 12@PersistenceContextprivate EntityManager em; 而其中 @PersistenceContext 的属性配置有如下这些。 1234567891011public @interface PersistenceContext &#123; String name() default ""; //PersistenceContextUnit的名字，多数据源的时候有用 String unitName() default ""; //是指创建的EntityManager的生命周期是存在事务内还是可以跨事务，默认为生命周期和事务一样； PersistenceContextType type() default PersistenceContextType.TRANSACTION; //同步的类型：只有SYNCHRONIZED和 UNSYNCHRONIZED 两个值用来表示，但开启事务的时候是否自动加入已开启的事务里面，默认SYNCHRONIZED表示自动加入，不创建新的事务。而UNSYNCHRONIZED表示，不自动加入上下文已经有的事务，自动开启新的事务；这里你使用的时候需要注意看一下事务的日志； SynchronizationType synchronization() default SynchronizationType.SYNCHRONIZED; //持久化的配置属性，这里指hibernate中AvailableSettings里面的值 PersistenceProperty[] properties() default &#123;&#125;;&#125; 一般情况下保持默认即可，你也可以根据实际情况自由组合，举个复杂点的例子。 12345678910111213@PersistenceContext( unitName = "db2",//采用数据源2的 //可以跨事务的EntityManager type = PersistenceContextType.EXTENDED, properties = &#123; //通过properties改变一下自动flush的机制 @PersistenceProperty( name="org.hibernate.flushMode", value= "MANUAL"//改成手动刷新方式 ) &#125;)private EntityManager entityManager; 实体对象的生命周期既然 PersistenceContext 是存储 Entity 的，那么 Entity 在 PersistenceContext 里面肯定有不同的状态。对此，JPA 协议定义了四种状态：new、manager、detached、removed。如下图所示： 第一种：New 状态的对象当使用关键字 new 的时候创建的实体对象，称为 new 状态的 Entity 对象。它需要同时满足两个条件： new 状态的实体 Id 和 Version 字段都是 null； new 状态的实体没有在 PersistenceContext 中出现过。 那么如果要把 new 状态的 Entity 放到 PersistenceContext 里面，有两种方法： 执行 entityManager.persist(entity) 方法； 通过关联关系的实体关系配置 cascade=PERSIST or cascade=ALL 这种类型，并且关联关系的一方，也执行了 entityManager.persist(entity) 方法。 1234567891011@Testpublic void testPersist() &#123; UserInfo userInfo = UserInfo.builder().lastName("jack").build(); //通过 contains 方法可以验证对象是否在 PersistenceContext 里面，此时不在 Assertions.assertFalse(entityManager.contains(userInfo)); //通过 persist 方法把对象放到 PersistenceContext 里面 entityManager.persist(userInfo); //通过 contains 方法可以验证对象是否在 PersistenceContext 里面，此时在 Assertions.assertTrue(entityManager.contains(userInfo)); Assertions.assertNotNull(userInfo.getId());&#125; 第二种：Detached（游离）的实体对象Detached 状态的对象表示和 PersistenceContext 脱离关系的 Entity 对象。它和 new 状态的对象的不同点在于： Detached 有持久化的 ID 变成持久化对象需要进行 merger 操作，merger 操作会 copy 一个新的实体对象，然后把新的实体对象变成 Manager 状态 而 Detached 和 new 状态的对象相同点也有两个方面： 都和 PersistenceContext 脱离了关系； 当执行 flush 操作或者 commit 操作的时候，不会进行数据库同步。 如果想让 Manager(persist) 状态的对象从 PersistenceContext 里面游离出来变成 Detached 的状态，可以通过 EntityManager 的 Detach 方法实现：entityManager.detach(entity); 当执行完 entityManager.clear()、entityManager.close()，或者事务 commit()、事务 rollback() 之后，所有曾经在 PersistenceContext 里面的实体都会变成 Detached 状态。 而游离状态的对象想回到 PersistenceContext 里面变成 manager 状态的话，只能执行 entityManager 的 merge 方法：entityManager.merge(entity); 游离状态的实体执行 EntityManager 中 persist 方法的时候就会报异常： 12345678910111213@Testpublic void testMergeException() &#123; //通过new的方式构建一个游离状态的对象 UserInfo userInfo = UserInfo.builder().id(1L).lastName("jack").version(1).build(); //验证是否存在于persistence context 里面，new的肯定不存在 Assertions.assertFalse(entityManager.contains(userInfo)); //当执行persist方法的时候就会报异常 Assertions.assertThrows(PersistentObjectException.class,()-&gt;entityManager.persist(userInfo)); //detached状态的实体通过merge的方式保存在了persistence context里面 UserInfo user2 = entityManager.merge(userInfo); //验证一下存在于持久化上下文里面 Assertions.assertTrue(entityManager.contains(user2));&#125; 第三种：Manager（persist） 状态的实体Manager 状态的实体，是指在 PersistenceContext 里面管理的实体，而此种状态的实体当我们执行事务的 commit()，或者 entityManager 的 flush 方法的时候，就会进行数据库的同步操作。可以说是和数据库的数据有映射关系。 New 状态如果要变成 Manager 的状态，需要执行 persist 方法；而 Detached 状态的实体如果想变成 Manager 的状态，则需要执行 merge 方法。在 session 的生命周期中，任何从数据库里面查询到的 Entity 都会自动成为 Manager 的状态，如 entityManager.findById(id)、entityManager.getReference 等方法。 而 Manager 状态的 Entity 要同步到数据库里面，必须执行 EntityManager 里面的 flush 方法。也就是说我们对 Entity 对象做的任何增删改查，必须通过 entityManager.flush() 执行之后才会变成 SQL 同步到 DB 里面。 12345678910@Test@Rollback(value = false)public void testManagerException() &#123; UserInfo userInfo = UserInfo.builder().lastName("jack").build(); entityManager.persist(userInfo); System.out.println("没有执行 flush()方法，没有产生insert sql"); entityManager.flush(); System.out.println("执行了flush()方法，产生了insert sql"); Assertions.assertTrue(entityManager.contains(userInfo));&#125; 123没有执行 flush()方法，没有产生insert sqlHibernate: insert into user_info (create_time, create_user_id, last_modified_time, last_modified_user_id, version, ages, email_address, last_name, telephone, id) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)执行了flush()方法，产生了insert sql 第四种：Removed 的实体状态Removed 的状态，是指删除了的实体，但是此实体还在 PersistenceContext 里面，只是在其中表示为 Removed 的状态，它和 Detached 状态的实体最主要的区别就是在不在 PersistenceContext 里面，但都有 ID 属性。 而 Removed 状态的实体，当我们执行 entityManager.flush() 方法的时候，就会生成一条 delete 语句到数据库里面。Removed 状态的实体，在执行 flush() 方法之前，执行 entityManger.persist(removedEntity) 方法时候，就会去掉删除的表示，变成 Managed 的状态实例。 12345678910@Testpublic void testDelete() &#123; UserInfo userInfo = UserInfo.builder().lastName("jack").build(); entityManager.persist(userInfo); entityManager.flush(); System.out.println("执行了flush()方法，产生了insert sql"); entityManager.remove(userInfo); entityManager.flush(); System.out.println("执行了flush()方法之后，又产生了delete sql");&#125; 1234Hibernate: insert into user_info (create_time, create_user_id, last_modified_time, last_modified_user_id, version, ages, email_address, last_name, telephone, id) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)执行了flush()方法，产生了insert sqlHibernate: delete from user_info where id=? and version=?执行了flush()方法之后，又产生了delete sql MyBatis 是对数据库的操作所见即所得的模式；而使用 JPA，你的任何操作都不会产生 DB 的sql EntityManager 的 flush() 方法flush 方法的用法很简单，就是在需要 DB 同步 sql 执行的时候，执行 entityManager.flush() 即可 Flush 的作用flush 重要的、唯一的作用，就是将 Persistence Context 中变化的实体转化成 sql 语句，同步执行到数据库里面。换句话来说，如果我们不执行 flush() 方法的话，通过 EntityManager 操作的任何 Entity 过程都不会同步到数据库里面。 而 flush() 方法很多时候不需要我们手动操作，这里我直接通过 entityManager 操作 flush() 方法，仅仅是为了演示执行过程。实际工作中很少会这样操作，而是会直接利用 JPA 和 Hibernate 底层框架帮我们实现的自动 flush 的机制。 Flush 的机制JPA 协议规定了 EntityManager 可以通过如下方法修改 FlushMode。 123456789//entity manager 里面提供的修改FlushMode的方法public void setFlushMode(FlushModeType flushMode);//FlushModeType只有两个值，自动和事务提交之前public enum FlushModeType &#123; //事务commit之前 COMMIT, //自动规则，默认 AUTO&#125; 而 Hibernate 还提供了一种手动触发的机制，可以通过如下代码的方式进行修改。 12345@PersistenceContext(properties = &#123;@PersistenceProperty( name = "org.hibernate.flushMode", value = "MANUAL"//手动flush)&#125;)private EntityManager entityManager; 手动和 commit 的时候很好理解，就是手动执行 flush 方法；事务就是代码在执行事务 commit 的时候，必须要执行 flush() 方法 Flush 的自动机制默认情况下，JPA 和 Hibernate 都是采用的 AUTO 的 Flush 机制，自动触发的规则如下： 事务 commit 之前，即指执行 transactionManager.commit() 之前都会触发，这个很好理解； 执行任何的 JPQL 或者 native SQL（代替直接操作 Entity 的方法）都会触发 flush。 12345678910111213141516 @Test public void testPersist() &#123; UserInfo userInfo = UserInfo.builder().lastName("jack").build(); //通过contains方法可以验证对象是否在PersistenceContext里面，此时不在 Assertions.assertFalse(entityManager.contains(userInfo)); //通过persist方法把对象放到PersistenceContext里面 entityManager.persist(userInfo);//是直接操作Entity的，不会触发flush操作 //entityManager.remove(userInfo);//是直接操作Entity的，不会触发flush操作 System.out.println("没有执行 flush()方法，没有产生insert sql"); UserInfo userInfo2 = entityManager.find(UserInfo.class,2L);//是直接操作Entity的，这个就不会触发flush操作// userInfoRepository.queryByFlushTest();//是操作JPQL的，这个就会先触发flush操作； System.out.println("flush()方法，产生insert sql"); //通过contains方法可以验证对象是否在 PersistenceContext 里面，此时在 Assertions.assertTrue(entityManager.contains(userInfo)); Assertions.assertNotNull(userInfo.getId()); &#125; 而只有执行类似 .queryByFlushTest() 这个方法的时候，才会触发 flush，因为它是用的 JPQL 的机制执行的。 上面的方法触发了 flush 的日志，会输出如下格式，你可以看到这里多了一个 insert 语句。 1234没有执行 flush()方法，没有产生insert sqlHibernate: insert into user_info (create_time, create_user_id, last_modified_time, last_modified_user_id, version, ages, email_address, last_name, telephone, id) values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)Hibernate: select userinfo0_.id as id1_0_, userinfo0_.create_time as create_t2_0_, userinfo0_.create_user_id as create_u3_0_, userinfo0_.last_modified_time as last_mod4_0_, userinfo0_.last_modified_user_id as last_mod5_0_, userinfo0_.version as version6_0_, userinfo0_.ages as ages7_0_, userinfo0_.email_address as email_ad8_0_, userinfo0_.last_name as last_nam9_0_, userinfo0_.telephone as telepho10_0_ from user_info userinfo0_ where userinfo0_.id=2flush()方法，产生insert sql 没有触发 flush 的日志输出的是如下格式，其中没有 insert 语句。 123没有执行 flush()方法，没有产生insert sqlHibernate: select userinfo0_.id as id1_0_0_, userinfo0_.create_time as create_t2_0_0_, userinfo0_.create_user_id as create_u3_0_0_, userinfo0_.last_modified_time as last_mod4_0_0_, userinfo0_.last_modified_user_id as last_mod5_0_0_, userinfo0_.version as version6_0_0_, userinfo0_.ages as ages7_0_0_, userinfo0_.email_address as email_ad8_0_0_, userinfo0_.last_name as last_nam9_0_0_, userinfo0_.telephone as telepho10_0_0_ from user_info userinfo0_ where userinfo0_.id=?flush()方法，产生insert sql Flush 会改变 SQL 的执行顺序flush() 方法调用之后，同一个事务内，sql 的执行顺序会变成如下模式： insert 的先执行 delete 的第二个执行 update 的第三个执行 123entityManager.remove(u3);UserInfo userInfo = UserInfo.builder().lastName("jack").build();entityManager.persist(userInfo); 看一下执行的 sql 会变成如下模样，即先 insert 后 delete。 12Hibernate: insert into user_info 。。。。。。Hibernate: delete from user_info where id=? and version=? 这种会改变顺序的现象，主要是由 persistence context 的实体状态机制导致的，所以在 Hibernate 的环境中，顺序会变成如下的 ActionQueue 的模式： OrphanRemovalAction EntityInsertAction EntityIdentityInsertAction EntityUpdateAction CollectionRemoveAction CollectionUpdateAction CollectionRecreateAction EntityDeleteAction Flush 与事务 Commit 的关系大概有以下几点： 在当前的事务执行 commit 的时候，会触发 flush 方法； 在当前的事务执行完 commit 的时候，如果隔离级别是可重复读的话，flush 之后执行的 update、insert、delete 的操作，会被其他的新事务看到最新结果； 假设当前的事务是可重复读的，当我们手动执行 flush 方法之后，没有执行事务 commit 方法，那么其他事务是看不到最新值变化的，但是最新值变化对当前没有 commit 的事务是有效的； 如果执行了 flush 之后，当前事务发生了 rollback 操作，那么数据将会被回滚（数据库的机制）。 saveAndFlush 和 save 的区别**Repository 里面有一个 saveAndFlush(entity); 的方法，源码如下： 12345678@Transactional@Overridepublic &lt;S extends T&gt; S saveAndFlush(S entity) &#123; //执行了save方法之后，调用了flush()方法 S result = save(entity); flush(); return result;&#125; 而另一个 **Repository 里面的 save 的方法，源码如下： 1234567891011// 没有做 flush 操作，只是，执行了 persist 或者 merge 的操作@Transactional@Overridepublic &lt;S extends T&gt; S save(S entity) &#123; if (entityInformation.isNew(entity)) &#123; em.persist(entity); return entity; &#125; else &#123; return em.merge(entity); &#125;&#125; 区别有如下几点： saveAndFlush 执行完save，再执行 flush，会刷新整个 PersistenceContext 里面的实体并进入到数据库里面，那么当我们频繁调用 saveAndFlush 就失去了 cache 的意义，这个时候就和执行 mybatis 的 saveOrUpdate 是一样的效果； 当多次调用相同的 save 方法的时候，最终 flush 执行只会产生一条 sql，在性能上会比 saveAndFlush 高一点； 不管是 saveAndFlush 还是 save，都受当前事务控制，事务在没有 commit 之前，都只会影响当前事务的操作； 综上，两种本质的区别就是 flush 执行的时机不一样而已，对数据库中数据的事务一致性没有任何影响。然而有的时候，即使我们调用了 flush 的方法也是一条 sql 都没有，为什么呢？再来了解一个概念：Dirty。 Entity 的 Dirty 判断逻辑及其作用在 PersistenceContext 里面还有一个重要概念，就是当实体不是 Dirty 状态，也就是没有任何变化的时候，是不会进行任何 db 操作的。所以即使我们执行 flush 和 commit，实体没有变化，就没有必要执行，这也能大大减少数据库的压力。Dirty 的效果的例子： 123456789//我们假设数据库里面存在一条id=1的数据，我们不做任何改变执行save或者saveAndFlush，除了select之外，不会产生任何sql语句；@Test@Transactional@Rollback(value = false)public void testDirty() &#123; UserInfo userInfo = userInfoRepository.findById(1L).get(); userInfoRepository.saveAndFlush(userInfo); userInfoRepository.save(userInfo);&#125; 那么当我们尝试改变一下 userInfo 里面的值，当执行如下方法的时候就会产生 update 的 sql。 12345678@Test@Transactional@Rollback(value = false)public void testDirty() &#123; UserInfo userInfo = userInfoRepository.findById(1L).get(); userInfo.setLastName("jack_test_dirty"); userInfoRepository.saveAndFlush(userInfo);&#125; Entity 判断 Dirty 的过程DefaultFlushEntityEventListener 的源码里面 isUpdateNecessary 的关键方法如下所示： 看 dirtyCheck 的实现，可以看发现如下关键点，从而找出发生变化的 properties。 再仔细看 persister.findDirty（values, loadedState, entity, session），可以看出来源码里面是通过一个字段一个字段比较的，所以可以知道 PersistenceContext 中的前后两个 Entity 的哪些字段发生了变化。因此当我们执行完 save 之后，没有产生任何 sql（因为没有变化）。 总结：在 flush 的时候，Hibernate 会一个个判断实体的前后对象中哪个属性发生变化了，如果没有发生变化，则不产生 update 的 sql 语句；只有变化才会才生 update sql，并且可以做到同一个事务里面的多次 update 合并，从而在一定程度上可以减轻 DB 的压力。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 的审计功能]]></title>
    <url>%2F2020%2F11%2F05%2FSpringDataJpa%E7%9A%84%E5%AE%A1%E8%AE%A1%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[JPA 的审计功能 JPA 的审计功能，即 Auditing Auditing 指的是什么Auditing 是帮我们做审计用的，当我们操作一条记录的时候，需要知道这是谁创建的、什么时间创建的、最后修改人是谁、最后修改时间是什么时候，甚至需要修改记录……这些都是 Spring Data JPA 里面的 Auditing 支持的，它为我们提供了四个注解来完成上面说的一系列事情，如下： @CreatedBy 是哪个用户创建的。 @CreatedDate 创建的时间。 @LastModifiedBy 最后修改实体的用户。 @LastModifiedDate 最后一次修改的时间。 这就是 Auditing 了 Auditing 如何实现利用上面的四个注解实现方法，一共有三种方式实现 Auditing。 方式一：直接在实例里面添加注解在 User 实体添加四个字段，分别记录创建人、创建时间、最后修改人、最后修改时间。 第一步：在 User 里面添加四个注解，并且新增 @EntityListeners(AuditingEntityListener.class) 注解。 添加完之后，User 的实体代码如下： 1234567891011121314151617181920212223242526272829@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "addresses")@EntityListeners(AuditingEntityListener.class)public class User implements Serializable &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; @Enumerated(EnumType.STRING) private SexEnum sex; private Integer age; @OneToMany(mappedBy = "user") @JsonIgnore private List&lt;UserAddress&gt; addresses; private Boolean deleted; @CreatedBy private Integer createUserId; @CreatedDate private Date createTime; @LastModifiedBy private Integer lastModifiedUserId; @LastModifiedDate private Date lastModifiedTime;&#125; 在 @Entity 实体中我们需要做两点操作： 其中最主要的四个字段分别记录创建人、创建时间、最后修改人、最后修改时间，代码如下： 12345678@CreatedByprivate Integer createUserId;@CreatedDateprivate Date createTime;@LastModifiedByprivate Integer lastModifiedUserId;@LastModifiedDateprivate Date lastModifiedTime; 其中 AuditingEntityListener 不能少，必须通过这段代码： 1@EntityListeners(AuditingEntityListener.class) 在 Entity 的实体上面进行注解。 第二步：实现 AuditorAware 接口，告诉 JPA 当前的用户是谁。 需要实现 AuditorAware 接口，以及 getCurrentAuditor 方法，并返回一个 Integer 的 user ID。这里可以自定义返回修改用户的信息，也可以是用户名 12345678910public class MyAuditorAware implements AuditorAware&lt;Integer&gt; &#123; //需要实现AuditorAware接口，返回当前的用户ID @Override public Optional&lt;Integer&gt; getCurrentAuditor() &#123; ServletRequestAttributes servletRequestAttributes = (ServletRequestAttributes) RequestContextHolder.getRequestAttributes(); Integer userId = (Integer) servletRequestAttributes.getRequest().getSession().getAttribute("userId"); return Optional.ofNullable(userId); &#125;&#125; 这里关键的一步，是实现 AuditorAware 接口的方法，如下所示： 123public interface AuditorAware&lt;T&gt; &#123; T getCurrentAuditor();&#125; 🎯注意：这里获得用户 ID 的方法不止这一种，实际工作中，我们可能将当前的 user 信息放在 Session 中，可能把当前信息放在 Redis 中，也可能放在 Spring 的 security 里面管理。 此外，这里的实现会有略微差异，我们以 security 为例： 12345Authentication authentication = SecurityContextHolder.getContext().getAuthentication();if (authentication == null || !authentication.isAuthenticated()) &#123; return null;&#125;Integer userId = ((LoginUserInfo) authentication.getPrincipal()).getUser().getId(); 这时获取 userId 的代码可能会变成上面这样子。 第三步：通过 @EnableJpaAuditing 注解开启 JPA 的 Auditing 功能。 第三步是最重要的一步，如果想使上面的配置生效，我们需要开启 JPA 的 Auditing 功能（默认没开启）。这里需要用到的注解是 @EnableJpaAuditing，代码如下： 123456789101112131415@Inherited@Documented@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Import(JpaAuditingRegistrar.class)public @interface EnableJpaAuditing &#123;//auditor用户的获取方法，默认是找AuditorAware的实现类；String auditorAwareRef() default "";//是否在创建修改的时候设置时间，默认是trueboolean setDates() default true;//在创建的时候是否同时作为修改，默认是trueboolean modifyOnCreate() default true;//时间的生成方法，默认是取当前时间(为什么提供这个功能呢？因为测试的时候有可能希望时间保持不变，它提供了一种自定义的方法)；String dateTimeProviderRef() default "";&#125; 在了解了@EnableJpaAuditing注解之后，我们需要创建一个Configuration 文件，添加 @EnableJpaAuditing 注解，并且把我们的 MyAuditorAware 加载进去即可，如下所示： 123456789@Configuration@EnableJpaAuditingpublic class JpaConfiguration &#123; @Bean @ConditionalOnMissingBean(name = "myAuditorAware") MyAuditorAware myAuditorAware() &#123; return new MyAuditorAware(); &#125;&#125; 经验之谈 这里说一个 Configuration 的最佳实践的写法。我们为什么要单独写一个Configuration 的配置文件，而不是把@EnableJpaAuditing 放在 Application 的类里面呢？因为这样的话 Configuration 文件可以单独加载、单独测试，如果都放在 Application 类里面的话，岂不是每次测试都要启动整个应用吗？ MyAuditorAware 也可以通过 @Component 注解进行加载，我为什么推荐 @Bean 的方式呢？因为这种方式可以让使用的人直接通过我们的配置文件知道我们自定义了哪些组件，不会让用的人产生不必要的惊讶，这是一点写 framework 的经验，供你参考。 第四步：我们写个测试用例测试一下。 12345678910111213141516171819202122232425262728@DataJpaTest@TestInstance(TestInstance.Lifecycle.PER_CLASS)@Import(JpaConfiguration.class)public class UserRepositoryTest &#123; @Autowired private UserRepository userRepository; @MockBean MyAuditorAware myAuditorAware; @Test public void testAuditing() &#123; //由于测试用例模拟web context环境不是我们的重点，我们这里利用@MockBean，mock掉我们的方法，期待返回13这个用户ID Mockito.when(myAuditorAware.getCurrentAuditor()).thenReturn(Optional.of(13)); //我们没有显式的指定更新时间、创建时间、更新人、创建人 User user = User.builder() .name("jack") .email("123456@126.com") .sex(SexEnum.BOY) .age(20) .build(); userRepository.save(user); //验证是否有创建时间、更新时间，UserID是否正确； List&lt;User&gt; users = userRepository.findAll(); Assertions.assertEquals(13,users.get(0).getCreateUserId()); Assertions.assertNotNull(users.get(0).getLastModifiedTime()); System.out.println(users.get(0)); &#125;&#125;需要注意的是： 我们利用 @MockBean 模拟 MyAuditorAware 返回结果 13 这个 User ID； 我们测试并验证 create_user_id 是否是我们预期的。 测试结果如下： 1User(id=1, name=jack, email=123456@126.com, sex=BOY, age=20, deleted=null, createUserId=13, createTime=Sat Oct 03 21:19:57 CST 2020, lastModifiedUserId=13, lastModifiedTime=Sat Oct 03 21:19:57 CST 2020) 结果完全符合我们的预期。 方式二：实体里面实现Auditable 接口我们改一下上面的 User 实体对象，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "addresses")@EntityListeners(AuditingEntityListener.class)public class User implements Auditable&lt;Integer,Long, Instant&gt; &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; @Enumerated(EnumType.STRING) private SexEnum sex; private Integer age; @OneToMany(mappedBy = "user") @JsonIgnore private List&lt;UserAddress&gt; addresses; private Boolean deleted; private Integer createUserId; private Instant createTime; private Integer lastModifiedUserId; private Instant lastModifiedTime; @Override public Optional&lt;Integer&gt; getCreatedBy() &#123; return Optional.ofNullable(this.createUserId); &#125; @Override public void setCreatedBy(Integer createdBy) &#123; this.createUserId = createdBy; &#125; @Override public Optional&lt;Instant&gt; getCreatedDate() &#123; return Optional.ofNullable(this.createTime); &#125; @Override public void setCreatedDate(Instant creationDate) &#123; this.createTime = creationDate; &#125; @Override public Optional&lt;Integer&gt; getLastModifiedBy() &#123; return Optional.ofNullable(this.lastModifiedUserId); &#125; @Override public void setLastModifiedBy(Integer lastModifiedBy) &#123; this.lastModifiedUserId = lastModifiedBy; &#125; @Override public void setLastModifiedDate(Instant lastModifiedDate) &#123; this.lastModifiedTime = lastModifiedDate; &#125; @Override public Optional&lt;Instant&gt; getLastModifiedDate() &#123; return Optional.ofNullable(this.lastModifiedTime); &#125; @Override public boolean isNew() &#123; return id==null; &#125;&#125; 与第一种方式的差异是，这里我们要去掉上面说的四个注解，并且要实现接口 Auditable 的方法，代码会变得很冗余和啰唆。 而其他都不变，我们再跑一次刚才的测试用例，发现效果是一样的。从代码的复杂程度来看，这种方式不推荐使用。 方式三：利用 @MappedSuperclass 注解它主要是用来解决公共 BaseEntity 的问题，而且其代表的是继承它的每一个类都是一个独立的表。 我们先看一下 @MappedSuperclass 的语法。 12345@Documented@Target(&#123;TYPE&#125;)@Retention(RUNTIME)public @interface MappedSuperclass &#123;&#125; 它注解里面什么都没有，其实就是代表了抽象关系，即所有子类的公共字段而已。那么接下来我们看一下实例。 第一步：创建一个 BaseEntity，里面放一些实体的公共字段和注解。 1234567891011121314151617package com.example.jpa.example1.base;import org.springframework.data.annotation.*;import javax.persistence.MappedSuperclass;import java.time.Instant;@Data@MappedSuperclass@EntityListeners(AuditingEntityListener.class)public class BaseEntity &#123; @CreatedBy private Integer createUserId; @CreatedDate private Instant createTime; @LastModifiedBy private Integer lastModifiedUserId; @LastModifiedDate private Instant lastModifiedTime;&#125; 注意： BaseEntity 里面需要用上面提到的四个注解，并且加上@EntityListeners(AuditingEntityListener.class)，这样所有的子类就不需要加了。 第二步：实体直接继承 BaseEntity 即可。 我们修改一下上面的 User 实例继承 BaseEntity，代码如下： 1234567891011121314151617181920@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructor@ToString(exclude = "addresses")public class User extends BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; @Enumerated(EnumType.STRING) private SexEnum sex; private Integer age; @OneToMany(mappedBy = "user") @JsonIgnore private List&lt;UserAddress&gt; addresses; private Boolean deleted;&#125; 这样的话，User 实体就不需要关心太多，我们只关注自己需要的逻辑即可，如下： 去掉了 @EntityListeners(AuditingEntityListener.class)； 去掉了 @CreatedBy、@CreatedDate、@LastModifiedBy、@LastModifiedDate 四个注解的公共字段。 接着我们再跑一下上面的测试用例，发现效果还是一样的。 这种方式，是我最推荐的，也是实际工作中使用最多的一种方式。它的好处显而易见就是公用性强，代码简单，需要关心的少。 通过上面的实际案例，我们其实也能很容易发现 Auditing 帮我们解决了什么问题，下面总结一下。 JPA 的审计功能解决了哪些问题？ 可以很容易地让我们写自己的 BaseEntity，把一些公共的字段放在里面，不需要我们关心太多和业务无关的字段，更容易让我们公司的表更加统一和规范，就是统一加上 @CreatedBy、@CreatedDate、@LastModifiedBy、@LastModifiedDate 等。 实际工作中，BaseEntity 可能还更复杂一点，比如说把 ID 和 @Version 加进去，会变成如下形式： 123456789101112131415161718@Data@MappedSuperclass@EntityListeners(AuditingEntityListener.class)public class BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; @CreatedBy private Integer createUserId; @CreatedDate private Instant createTime; @LastModifiedBy private Integer lastModifiedUserId; @LastModifiedDate private Instant lastModifiedTime; @Version private Integer version;&#125; Auditing 在实战应用场景中，比较适合做后台管理项目，对应纯粹的 RESTAPI 项目，提供给用户直接查询的 API 的话，可以考虑一个特殊的 User ID。 Auditing 的实现原理第一步：还是从 @EnableJpaAuditing 入手分析。 我们前面讲了它的使用方法，这次我们分析一下其加载原理，看下面的图： 我们可以知道，首先 Auditing 这套封装是 Spring Data JPA 实现的，而不是 Java Persistence API 规定的，其注解里面还有一项重要功能就是 @Import(JpaAuditingRegistrar.class) 这个类，它帮我们处理 Auditing 的逻辑。 我们看其源码，一步一步地 debug 下去可以发现如下所示： 进一步进入到如下方法中： 可以看到 Spring 容器给 AuditingEntityListener.class 注入了一个 AuditingHandler 的处理类。 第二步：打开 AuditingEntityListener.class 的源码分析 debug 一下。 12345678910111213141516171819202122232425262728@Configurablepublic class AuditingEntityListener &#123; private @Nullable ObjectFactory&lt;AuditingHandler&gt; handler; public void setAuditingHandler(ObjectFactory&lt;AuditingHandler&gt; auditingHandler) &#123; Assert.notNull(auditingHandler, "AuditingHandler must not be null!"); this.handler = auditingHandler; &#125; @PrePersist public void touchForCreate(Object target) &#123; Assert.notNull(target, "Entity must not be null!"); if (handler != null) &#123; AuditingHandler object = handler.getObject(); if (object != null) &#123; object.markCreated(target); &#125; &#125; &#125; @PreUpdate public void touchForUpdate(Object target) &#123; Assert.notNull(target, "Entity must not be null!"); if (handler != null) &#123; AuditingHandler object = handler.getObject(); if (object != null) &#123; object.markModified(target); &#125; &#125; &#125;&#125; 从源码我们可以看到，AuditingEntityListener 的实现还是比较简单的，利用了 Java Persistence API 里面的@PrePersist、@PreUpdate 回调函数，在更新和创建之前通过AuditingHandler 添加了用户信息和时间信息。 原理分析结论查看 Auditing 的实现源码，其实给我们提供了一个思路，就是怎么利用 @PrePersist、@PreUpdate 等回调函数和 @EntityListeners 定义自己的框架代码。这是值得我们学习和参考的，比如说 Auditing 的操作日志场景等。 想成功配置 Auditing 功能，必须将 @EnableJpaAuditing 和 @EntityListeners(AuditingEntityListener.class) 一起使用才有效。 我们是不是可以不通过 Spring data JPA 给我们提供的 Auditing 功能，而是直接使用 @PrePersist、@PreUpdate 回调函数注解在实体上，也可以达到同样的效果呢？答案是肯定的，因为回调函数是实现的本质。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>Auditing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Stream API]]></title>
    <url>%2F2020%2F11%2F05%2FStream%20API%2F</url>
    <content type="text"><![CDATA[Stream 简介什么是流Stream 不是集合元素，它不是数据结构并不保存数据，它是有关算法和计算的。 与Iterator的异同 ​ 相同点： 单向的，数据只能遍历一次。 ​ 不同点： stream可以并行操作 Stream 的另外一个特点是，==数据源本身可以是无限的==。 Stream 的并行操作依赖于 Java7 中引入的 Fork/Join 框架来拆分任务和加速处理过程。 123456Java 的并行API演变历程基本如下： 1. 1.0-1.4 中的 java.lang.Thread 2. 5.0 中的 java.util.concurrent 3. 6.0 中的 Phasers 等 4. 7.0 中的 Fork/Join 框架 5. 8.0 中的 Lambda 生成 Stream Source方式Collection and Array Collection.stream() Collection.parallelStream() Arrays.stream(T array) or Stream.of() BufferedReader java.io.BufferedReader.lines() 静态工厂 java.util.stream.IntStream.range() java.nio.file.Files.walk() 自己构建 java.util.Spliterator 其它 Random.ints() BitSet.stream() Pattern.splitAsStream(java.lang.CharSequence) JarFile.stream() 常见的stream接口继承关系如图 对于基本数值型，目前有三种对应的包装类型 Stream： IntStream、LongStream、DoubleStream。当然我们也可以用 Stream&lt;Integer&gt;、Stream&lt;Long&gt;、Stream&lt;Double&gt;，但是 boxing 和 unboxing 会很耗时，所以特别为这三种基本数值型提供了对应的 Stream。 为什么为不同数据类型设置不同stream接口 提高性能 增加特定接口函数 为什么不把IntStream等设计成Stream的子接口？毕竟这接口中的方法名大部分是一样的。 ​ 虽然这些方法的名字相同，但是返回类型不同，如果设计成父子接口关系，这些方法将不能共存， ​ 因为Java不允许只有返回类型不同的方法重载。 虽然大部分情况下stream是容器调用Collection.stream()方法得到的，但stream和collections有以下不同： 无存储。stream不是一种数据结构，它只是某种数据源的一个视图，数据源可以是一个数组，Java容器或I/O channel等。 为函数式编程而生。对stream的任何修改都不会修改背后的数据源，比如对stream执行过滤操作并不会删除被过滤的元素，而是会产生一个不包含被过滤元素的新stream。 惰式执行。stream上的操作并不会立即执行，只有等到用户真正需要结果的时候才会执行。 可消费性。stream只能被“消费”一次，一旦遍历过就会失效，就像容器的迭代器那样，想要再次遍历必须重新生成。 Stream 操作对stream的操作分为为两类，中间操作(intermediate)和结束操作(terminal)，二者特点是： 中间操作总是会惰式执行，调用中间操作只会生成一个标记了该操作的新stream，仅此而已。 结束操作会触发实际计算，计算发生时会把所有中间操作积攒的操作以pipeline的方式执行，这样可以减少迭代次数。计算完成之后stream就会失效。 Stream接口的部分常见方法 操作类型 接口方法 intermediate concat() 、distinct()、 filter()、 flatMap()、 limit()、 map()、 peek()、 skip()、 sorted() 、parallel() 、sequential()、 unordered() terminal forEach()、forEachOrdered()、findFirst()、 findAny() 、count() 、max()、 min()、anyMatch()、allMatch() 、noneMatch()、toArray()、 collect()、reduce() short-circuiting anyMatch()、 allMatch()、 noneMatch()、 findFirst()、 findAny()、 limit() 区分中间操作和结束操作最简单的方法 ​ 就是看方法的返回值，返回值为stream的大都是中间操作，否则是结束操作。 TerminalforEach() / forEachOrdered()void forEach(Consumer&lt;? super E&gt; action) void forEachOrdered(Consumer&lt;? super E&gt; action) 作用是对容器中的每个元素执行action指定的动作，也就是对元素进行遍历。 由于forEach() / forEachOrdered() 是结束方法，上述代码会立即执行，输出所有字符串。 123// 使用Stream.forEach()迭代 Stream.of("AAA", "BBB", "CCC").parallel().forEach(System.out::println); Stream.of("AAA", "BBB", "CCC").parallel().forEachOrdered(System.out::println); 区别 ​ 在并行流中，forEachOrdered()会保证是顺序执行，而forEach()不会。 findFirst() / findAny()这是一个 terminal 兼 short-circuiting 操作，它总是返回 Stream 的第一个元素，或者空。 返回值类型：Optional 目的是尽可能避免 NullPointerException 123456789101112131415161718192021222324252627public static void main(String[] args)&#123; String strA = " abcd "; String strB = null; print(strA); print(""); print(strB); getLength(strA); getLength(""); getLength(strB);&#125;public static void print(String text) &#123; // Java 8 Optional.ofNullable(text).ifPresent(System.out::println); // Pre-Java 8 if (text != null) &#123; System.out.println(text); &#125; &#125;public static int getLength(String text) &#123; // Java 8 return Optional.ofNullable(text).map(String::length).orElse(-1); // Pre-Java 8 // return if (text != null) ? text.length() : -1; &#125;; count()求结果的个数，类似于list的size()方法 返回值：long型 12long count = Stream.of("111", "2", "33").filter(s -&gt; s.length() == 1).count();System.out.println(count); max() / min()求最大值，最小值 参数：Comparator接口 返回值：optional 123456789String str1 = Stream.of("111", "2", "33").max((s1, s2) -&gt; s1.length() - s2.length()).get();String str2 = Stream.of("111", "2", "33").max(Comparator.comparingInt(String::length)).get();System.out.println(str1);System.out.println(str2);String str3 = Stream.of("111", "2", "33").min((s1, s2) -&gt; s1.length() - s2.length()).get();String str4 = Stream.of("111", "2", "33").min(Comparator.comparingInt(String::length)).get();System.out.println(str3);System.out.println(str4); anyMatch()查找stream中是否存在任何一个元素满足匹配条件，如果条件为true 就停止遍历，否则继续遍历。 参数：Predicate接口 返回值：boolean 1234boolean isStartWithA = Stream.of("d2", "a2", "b1", "b3", "c") .map(String::toUpperCase) .anyMatch(s -&gt; s.startsWith("A"));System.out.println(isStartWithA); 上述例子 ​ 遍历到a2，则满足anyMatch的条件，则退出遍历，并返回true。 allMatch() / noneMatch()allMatch(): 查找stream中的所有元素是否==都满足==给定的匹配条件，如果条件为 false 就停止遍历，否则继续遍历。 noneMatch(): 查找stream中的所有元素是否==都不满足==给定的匹配条件，如果条件为 true 就停止遍历，否则继续遍历。 参数：Predicate 接口 返回值：boolean 12345boolean isStartWithA = Stream.of("d2", "a2", "b1", "b3", "c") .map(String::toUpperCase) .allMatch(s -&gt; s.startsWith("A")); //.noneMatch(s -&gt; s.startsWith("A"));System.out.println(isStartWithA); 查看执行流程，可以选择在表达式里输出检验： ​ 如 allMatch()，可以替换成noneMatch() 和anyMatch() 12345678910&gt; Stream.of("d2", "a2", "b1", "b3", "c")&gt; .map(s -&gt; &#123;&gt; System.out.println("map: " + s);&gt; return s.toUpperCase();&gt; &#125;)&gt; .allMatch(s -&gt; &#123;&gt; System.out.println("allMatch: " + s);&gt; return s.startsWith("A");&gt; &#125;);&gt; toArray()toArray() 将流转换为数组，有两个方法： 12Object[] toArray()&lt;A&gt; A[] toArray(IntFunction&lt;A[]&gt; generator) 无参的，返回一个object对象数组，一般没什么用。 有参的，需要写一个funcation&lt;T,R&gt;接口， 而IntFuncation&lt;R&gt; 是funcation&lt;T,R&gt;接口的子接口，规定了输入是int类型。 12345678// :: 表示引用，一种是方法(静态，实例)的引用;// 一种的构造方法的引用(构造的需要反着写,如new String[] &gt; String[]::new)IntFunction&lt;String[]&gt; function = String[]::new;String[] as = Stream.of("AAA", "BBB", "CCC").parallel().toArray(function);System.out.println(Arrays.toString(as)); collect()1. Collection, Collections, collect, Collector, Collectors Collection是Java集合的祖先接口。Collections是java.util包下的一个工具类，内涵各种处理集合的静态方法。java.util.stream.Stream#collect(java.util.stream.Collector&lt;? super T,A,R&gt;)是Stream的一个函数，负责收集流。java.util.stream.Collector 是一个收集函数的接口, 声明了一个收集器的功能。java.util.stream.Collectos则是一个收集器的工具类，内置了一系列收集器实现。 将 stream 转换成 收集器，有两个方法。 1234&lt;R&gt; R collect(Supplier&lt;R&gt; supplier, BiConsumer&lt;R, ? super T&gt; accumulator, BiConsumer&lt;R, R&gt; combiner);&lt;R, A&gt; R collect(Collector&lt;? super T, A, R&gt; collector) 第一种构造方法： 12345678910111213 List&lt;String&gt; list = Lists.newArrayList("111", "222", "333", "444", "555"); //Supplier&lt;R&gt;接口 用来创建并返回一个指定数据类型的对象 //BiConsumer&lt;R, ? super T&gt; accumulator接口 用来声明一些操作。 // * 参数R 是指supplier创建出来的对象， // * 参数T 是指每次迭代的对象 //BiConsumer&lt;R, R&gt; combiner接口 指定执行的参数类型，以及返回值类型 HashMap&lt;String, String&gt; map = list.stream() .collect(HashMap::new, (hashMap, str) -&gt; hashMap.put(str, str),// (hashMap, hashMap2) -&gt; hashMap.putAll(hashMap2));// HashMap::putAll); Map::putAll); 解释 ​ 1、每次循环都创建一个hashMap ​ 2、每次循环都把当前遍历的值，存储在创建出来的hashMap中 ​ 3、最后再用上一次的hashMap，putAll这次遍历的hashMap，最后遍历完后就会返回一个总的Map 第二种构造方法： ​ 使用Collectors工具类 2.Collectors collect里面的操作稍微复杂一点。 可以把stream，使用某些数据结构存储起来。而collectors就是方便我们构造收集器的。 Collectors.toList() ： 构造一个list Collectors.toSet() ： 构造一个set Collectors.toCollection(ArrayList::new) ： 构造一个自定义收集器，参数用自定义收集器，这里用list只是方便举例 Collectors.joining() ： ==构造一个string==，参数是分割符，前缀，后缀 Collectors.toMap() 和 Collectors.toConcurrentMap() 有三个方法 构造方法差不多，这里只列举toMap的构造方法 1234567891011Collector&lt;T, ?, Map&lt;K,U&gt;&gt; toMap(Function&lt;? super T, ? extends K&gt; keyMapper, Function&lt;? super T, ? extends U&gt; valueMapper) Collector&lt;T, ?, Map&lt;K,U&gt;&gt; toMap(Function&lt;? super T, ? extends K&gt; keyMapper, Function&lt;? super T, ? extends U&gt; valueMapper, BinaryOperator&lt;U&gt; mergeFunction) Collector&lt;T, ?, M&gt; toMap(Function&lt;? super T, ? extends K&gt; keyMapper, Function&lt;? super T, ? extends U&gt; valueMapper, BinaryOperator&lt;U&gt; mergeFunction, Supplier&lt;M&gt; mapSupplier) ​ 12345678910111213141516171819202122232425262728293031323334353637 List&lt;String&gt; list = Lists.newArrayList("AAA", "BBB", "CCC", "DDD", "EEE"); //Function.identity() : 代表当前遍历的对象 //第一种构造方法 //Function&lt;T, K&gt; : 接口，第一个参数是输入，第二个参数是输出 //如 Function&lt;Party,String&gt; fun = party -&gt; party.getPartyType() Map&lt;String, String&gt; collect = list.stream() .collect(Collectors.toMap(Function.identity(), Function.identity())); Map&lt;String, String&gt; collect1 = list.stream() .collect(Collectors.toMap(String::toUpperCase, String::toLowerCase)); //如果key,value是空的话，会报空指针异常， //因为Collectors.toMap()底层使用Map.merge()，merge中key, value不能为空 Map&lt;String, String&gt; collect2 = list.stream()// .collect(Collectors.toMap(String::toUpperCase, null)); .collect(Collectors.toMap(null, String::toUpperCase)); System.out.println(collect2); //第二种构造方法 // 定义当key重复时的操作，Collectors.toMap()方法如果key相同，会报异常，可以自定义当key重复时的操作 // BinaryOperator&lt;U&gt; mergeFunction Map&lt;String, String&gt; collect3 = list.stream() .collect(Collectors.toMap( String::toUpperCase, String::toLowerCase, (oldValue, newValue) -&gt; oldValue)); //第三种构造方法 //Supplier&lt;M&gt; mapSupplier :自定义使用Map的类型 Map&lt;String, String&gt; collect4 = list.stream() .collect(Collectors.toMap( String::toUpperCase, String::toLowerCase, (oldValue, newValue) -&gt; oldValue,// HashMap::new));// ConcurrentHashMap::new)); LinkedHashMap::new)); toMap 和 toConcurrentMap 的异同 Collectors.summarizingInt(String::length) ​ 同理有Double，Long 12345678List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");IntSummaryStatistics collect = list.stream().collect(Collectors.summarizingInt(String::length));System.out.println(collect.getMax());System.out.println(collect.getMin());System.out.println(collect.getSum());System.out.println(collect.getAverage());System.out.println(collect.getCount()); 可以统一求长度最大值，最小值，平均数，求和，求个数 Collectors.groupingBy() 有三个构造方法，列表分组 按function的条件，分成多个组 12345678Collector&lt;T, ?, Map&lt;K, List&lt;T&gt;&gt;&gt; groupingBy(Function&lt;? super T, ? extends K&gt; classifier)Collector&lt;T, ?, Map&lt;K, D&gt;&gt; groupingBy(Function&lt;? super T, ? extends K&gt; classifier, Collector&lt;? super T, A, D&gt; downstream) Collector&lt;T, ?, M&gt; groupingBy(Function&lt;? super T, ? extends K&gt; classifier, Supplier&lt;M&gt; mapFactory, Collector&lt;? super T, A, D&gt; downstream) 示例： 12345List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");Map&lt;Integer, List&lt;String&gt;&gt; collect = list.stream() .collect(Collectors.groupingBy(String::length));Map&lt;Integer, Long&gt; collect1 = list.stream() .collect(Collectors.groupingBy(String::length, counting())); Collectors.partitioningBy() 两个构造方法，列表分割 按照Predicate，分割成两组。 123Collector&lt;T, ?, Map&lt;Boolean, D&gt;&gt; partitioningBy(Predicate&lt;? super T&gt;)Collector&lt;T, ?, Map&lt;Boolean, D&gt;&gt; partitioningBy(Predicate&lt;? super T&gt; predicate, Collector&lt;? super T, A, D&gt; downstream) 123456789101112// 分割数据块Map&lt;Boolean, List&lt;Integer&gt;&gt; collectParty = Stream.of(1, 2, 3, 4) .collect(Collectors.partitioningBy(it -&gt; it % 2 == 0));List&lt;Integer&gt; integers = collectParty.get(true);System.out.println(integers);Map&lt;Boolean, Long&gt; partiCount = Stream.of(1, 2, 3, 4) .collect(Collectors.partitioningBy(it -&gt; it % 2 == 0, Collectors.counting()));System.out.println("partiCount: " + partiCount);// 打印结果// partiCount: &#123;false=2, true=2&#125; 收集器的构成 1. 创建一个新的结果容器(supplier()) 2. 将一个新的数据元素合并到一个结果容器中(accumulator()) 3. 将两个结果容器合并成一个(combiner()) 4. 在容器上执行一个可选的最终转换 (finisher()) 收集器参数列表 toList() toSet() toCollection(Supplier&lt;C&gt;) counting() collectingAndThen(Collector&lt;T, A, R&gt;, Function&lt;R, RR&gt;) summingInt(ToIntFunction&lt;? super T&gt;) summingLong(ToLongFunction&lt;? super T&gt;) summingDouble(ToDoubleFunction&lt;? super T&gt;) maxBy(Comparator&lt;? super T&gt;) minBy(Comparator&lt;? super T&gt;) reducing(BinaryOperator&lt;T&gt;) reducing(T, BinaryOperator&lt;T&gt;) reducing(U, Function&lt;? super T, ? extends U&gt;, BinaryOperator&lt;U&gt;) joining() joining(CharSequence) joining(CharSequence, CharSequence, CharSequence) mapping(Function&lt;? super T, ? extends U&gt;, Collector&lt;? super U, A, R&gt;) toMap(Function&lt;? super T, ? extends K&gt;, Function&lt;? super T, ? extends U&gt;) toMap(Function&lt;? super T, ? extends K&gt;, Function&lt;? super T, ? extends U&gt;, BinaryOperator&lt;U&gt;) toMap(Function&lt;? super T, ? extends K&gt;, Function&lt;? super T, ? extends U&gt;, BinaryOperator&lt;U&gt;, Supplier&lt;M&gt;) toConcurrentMap(Function&lt;? super T, ? extends K&gt;, Function&lt;? super T, ? extends U&gt;) toConcurrentMap(Function&lt;? super T, ? extends K&gt;, Function&lt;? super T, ? extends U&gt;, BinaryOperator&lt;U&gt;) toConcurrentMap(Function&lt;? super T, ? extends K&gt;, Function&lt;? super T, ? extends U&gt;, BinaryOperator&lt;U&gt;, Supplier&lt;M&gt;) groupingBy(Function&lt;? super T, ? extends K&gt;) groupingBy(Function&lt;? super T, ? extends K&gt;, Supplier&lt;M&gt;, Collector&lt;? super T, A, D&gt;) groupingBy(Function&lt;? super T, ? extends K&gt;, Collector&lt;? super T, A, D&gt;) groupingByConcurrent(Function&lt;? super T, ? extends K&gt;) groupingByConcurrent(Function&lt;? super T, ? extends K&gt;, Supplier&lt;M&gt;, Collector&lt;? super T, A, D&gt;) groupingByConcurrent(Function&lt;? super T, ? extends K&gt;, Collector&lt;? super T, A, D&gt;) partitioningBy(Predicate&lt;? super T&gt;) partitioningBy(Predicate&lt;? super T&gt;, Collector&lt;? super T, A, D&gt;) averagingDouble(ToDoubleFunction&lt;? super T&gt;) averagingInt(ToIntFunction&lt;? super T&gt;) averagingLong(ToLongFunction&lt;? super T&gt;) summarizingDouble(ToDoubleFunction&lt;? super T&gt;) summarizingInt(ToIntFunction&lt;? super T&gt;) summarizingLong(ToLongFunction&lt;? super T&gt;) reduce()123Optional&lt;T&gt; reduce(BinaryOperator&lt;T&gt; accumulator);T reduce(T identity, BinaryOperator&lt;T&gt; accumulator);&lt;U&gt; U reduce(U identity,BiFunction&lt;U, ? super T, U&gt; accumulator,BinaryOperator&lt;U&gt; combiner); 方式一： 第一次执行的时候第一个参数的值是Stream的第一个元素，第二个参数是Stream的第二个元素 12345Optional accResult = Stream.of(1, 2, 3, 4) .reduce((acc, item) -&gt; &#123; acc += item; return acc; &#125;) 方式二：在方式一的基础上，加上初始化的值 12345int accResult = Stream.of(1, 2, 3, 4) .reduce(0, (acc, item) -&gt; &#123; acc += item; return acc; &#125;); Intermediatefilter()Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate) 作用是返回一个只包含满足predicate条件元素的Stream。 1234// 保留长度等于3的字符串Stream&lt;String&gt; stream= Stream.of("I", "love", "you", "too");stream.filter(str -&gt; str.length()==3) .forEach(str -&gt; System.out.println(str)); 注意 ​ 由于filter()是个中间操作，如果只调用filter()不会有实际计算，因此也不会输出任何信息。 ​ filter 保留条件为true的元素。 distinct()Stream&lt;T&gt; distinct() 作用是返回一个去除重复元素之后的Stream。 123Stream&lt;String&gt; stream= Stream.of("I", "love", "you", "too", "too");stream.distinct() .forEach(str -&gt; System.out.println(str)); sorted()排序函数有两个，一个是用自然顺序排序，一个是使用自定义比较器排序 分别为Stream&lt;T&gt; sorted()和Stream&lt;T&gt; sorted(Comparator&lt;? super T&gt; comparator) 123Stream&lt;String&gt; stream= Stream.of("I", "love", "you", "too");stream.sorted((str1, str2) -&gt; str1.length()-str2.length()) .forEach(str -&gt; System.out.println(str)); 上述代码将输出按照长度升序排序后的字符串 map()&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T,? extends R&gt; mapper)， 作用是返回一个对当前所有元素执行mapper之后的结果组成的Stream。 123Stream&lt;String&gt; stream = Stream.of("I", "love", "you", "too");stream.map(str -&gt; str.toUpperCase()) .forEach(str -&gt; System.out.println(str)); 上述代码将输出原字符串的大写形式。 flatMap()&lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T,? extends Stream&lt;? extends R&gt;&gt; mapper) 作用是对每个元素执行mapper指定的操作，并用所有mapper返回的stream中的元素组成一个新的stream作为最终返回结果。 通俗的讲flatMap()的作用就相当于把原stream中的所有元素都”摊平”之后组成的stream，转换前后元素的个数和类型都可能会改变。 123Stream&lt;List&lt;Integer&gt;&gt; stream = Stream.of(Arrays.asList(1,2), Arrays.asList(3, 4, 5));stream.flatMap(list -&gt; list.stream()) .forEach(i -&gt; System.out.println(i)); 上述代码中，原来的stream中有两个元素，分别是两个List&lt;Integer&gt;，执行flatMap()之后，将每个List都“摊平”成了一个个的数字，所以会新产生一个由5个数字组成的Stream。所以最终将输出1~5这5个数字。 图示 skip()跳过前面 n 个对象 123456List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");List&lt;String&gt; list2 = list.stream().skip(3).collect(toList());System.out.println(list2); //[DDD, EEE]Optional&lt;String&gt; first = list.stream().skip(2).findFirst();System.out.println(first.get());//CCCC limit()获取限定个数的对象 123456List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");List&lt;String&gt; list2 = list.stream().limit(2).collect(toList());System.out.println(list2); //[A, BBB]Optional&lt;String&gt; first = list.stream().limit(2).findFirst();System.out.println(first.get());//A concat()静态方法，把两个Stream拼在一起 123456List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");List&lt;String&gt; list2 = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");List&lt;String&gt; collect = Stream.concat(list.stream(), list2.stream()) .collect(Collectors.toList());System.out.println(collect); //[A, BBB, CCCC, DDD, EEE, A, BBB, CCCC, DDD, EEE] peek()可以做一些即时的操作，功能和forEach相同 12345678List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE"); //ABBBCCCCDDDEEEList&lt;String&gt; collect = list.stream() .peek(System.out::print) .filter(s -&gt; s.length() == 3) .collect(Collectors.toList());System.out.println();System.out.println(collect);//[BBB, DDD, EEE] 和foreach的不同点： ​ peek: 生成一个包含原stream的所有元素的新stream，同时会提供一个消费函数（Consumer实例）， ​ 新stream每个元素被消费的时候都会执行给定的消费函数； ​ 通俗的讲：peek是个中间操作，它提供了一种对流中所有元素操作的方法，而不会把这个流消费掉 ​ （foreach会把流消费掉） parallel()将stream变成 并行stream 12345List&lt;String&gt; list = Lists.newArrayList("A", "BBB", "CCCC", "DDD", "EEE");boolean isParallel = list.stream().parallel().isParallel();System.out.println(isParallel);//trueboolean parallel = list.parallelStream().isParallel();System.out.println(parallel); //true sequential()unordered()]]></content>
      <categories>
        <category>JDK</category>
        <category>JDK8</category>
      </categories>
      <tags>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Time 相关的类]]></title>
    <url>%2F2020%2F11%2F05%2FTime%20Class%2F</url>
    <content type="text"><![CDATA[Time相关类​ java.time包中的是类是不可变且线程安全的。新的时间及日期API位于java.time中 Instant——它代表的是时间戳 LocalDate——不包含具体时间的日期，比如2014-01-14。它可以用来存储生日，周年纪念日，入职日期等。 LocalTime——它代表的是不含日期的时间 LocalDateTime——它包含了日期及时间，不过还是没有偏移信息或者说时区。 ZonedDateTime——这是一个包含时区的完整的日期时间，偏移量是以UTC/格林威治时间为基准的。 JDK8是如何处理时间及日期的1. 处理日期​ LocalDate类，能用来表示今天的日期。这个类与java.util.Date略有不同，因为它只包含日期，没有时间。 ​ MonthDay类，这个类由月日组合，不包含年信息，可以用来代表每年重复出现的一些日期或其他组合。是不可变且线程安全的，并且它还是一个值类（value class）。 ​ YearMonth类，这个类有年月组合，不包含日信息，可以用来查找这个月在某一年里有多少天。 ​ Period类，计算两个日期之间包含多少天、周、月、年。 1.1 获取当天的日期123LocalDate today = LocalDate.now();System.out.println("今天的日期是：" + today);// 今天的日期是：2019-01-15 ​ 创建的日期不包含时间信息，并且格式化了日期。 1.2 获取当前的年月日12345678910LocalDate today = LocalDate.now();int year = today.getYear();int month = today.getMonthValue();int day = today.getDayOfMonth();System.out.println("年：" + year + "\n月：" + month + "\n日：" + day);/** * 年：2019 * 月：1 * 日：15 */ 1.3 获取某个特定的日期​ 通过 of 方法，可以创建出任意一个日期，它接受年月日的参数，然后返回一个等价的LocalDate实例。 ​ 在这个方法里，需要的日期你填写什么就是什么，不像之前的API中月份必须从0开始。 123LocalDate today = LocalDate.of(2018,12,15);System.out.println("你输入的日期是：" + today);// 你输入的日期是：2018-12-15 1.4 检查两个日期是否相等​ LocalDate重写了equals方法来进行日期的比较，如下所示： 1234LocalDate today = LocalDate.of(2019,1,15);LocalDate now = LocalDate.now();System.out.println("今天的日期是2019-1-15吗：" + today.equals(now));// 今天的日期是2019-1-15吗：true 1.5 如何检查重复事件，比如生日在java中还有一个与时间日期相关的任务就是检查重复事件，比如每月的账单日 使用MonthDay类判断是否是某个节日或者重复事件。 12345678910LocalDate dateOfBirth = LocalDate.of(1997,4,13);LocalDate now = LocalDate.now();MonthDay birthDay = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth());MonthDay currentMonthDay = MonthDay.from(now);if (currentMonthDay.equals(birthDay))&#123; System.out.println("今天是你的生日");&#125;else&#123; System.out.println("对不起，今天不是你的生日");&#125;//对不起，今天不是你的生日 ​ MonthDay只存储了月日，对比两个日期的月日即可知道是否重复 1.6 获取1周后的日期​ 这个与使用LocalTime获取2小时后的时间的例子很相似，这里我们获取的是1周后的日期。 ​ LocalDate是用来表示无时间的日期，他有一个plus()方法可以用来增加日，星期，月。 ​ ChronoUnit则用来表示时间单位，LocalDate也是不可变的，因此任何修改操作都会返回一个新的实例。 123456LocalDate today = LocalDate.now();System.out.println("今天的日期是：" + today);LocalDate oneToDay = today.plus(1, ChronoUnit.WEEKS);System.out.println("一周后的日期是：" + oneToDay);//今天的日期是：2019-01-15//一周后的日期是：2019-01-22 也可以用这个方法来增加一个月，一年，一小时，一分等等 1.7 一年前的日期​ 使用LocalDate的plus()方法来给日期增加日周月， ​ 现在我们用minus()方法来给日期减少日周月，往前找。 123456LocalDate today = LocalDate.now();System.out.println("今天的日期是：" + today);LocalDate previousYear = today.minus(1, ChronoUnit.YEARS);System.out.println("一年前的日期是：" + previousYear);//今天的日期是：2019-01-15//一年前的日期是：2018-01-15 1.8 如何判断某个日期在另一个日期的前面还是后面​ LocalDate类中使用isBefore()、isAfter()、equals()方法来比较两个日期。 ​ 如果调用方法的那个日期比给定的日期要早的话，isBefore()方法会返回true。 ​ 如果调用方法的那个日期比给定的日期要晚的话，isAfter()方法会返回true。 12345678910LocalDate today = LocalDate.now();System.out.println("今天的日期是：" + today);LocalDate tomorrow = today.plus(1, ChronoUnit.DAYS);System.out.println("明天的日期是：" + tomorrow);System.out.println("日期：" + tomorrow + "是否在日期：" + today + "之后：" + tomorrow.isAfter(today));System.out.println("日期：" + tomorrow + "是否在日期：" + today + "之前：" + tomorrow.isBefore(today));//今天的日期是：2019-01-15//明天的日期是：2019-01-16//日期：2019-01-16是否在日期：2019-01-15之后：true//日期：2019-01-16是否在日期：2019-01-15之前：false java8中比较日期非常简单，不再需要使用Calendar这样另外的类来完成类似的任务了 1.9 如何表示固定的日期，比如信用卡过期时间正如MonthDay表示的是某个重复出现的日子，YearMonth是另外一个组合，代表的是像信用卡还款日，定期存款到期日，options到期日这类的日期。你可以用这个类找出这个月有多少天，LengthOfMonth()这个方法返回的是这个YearMonth实例有多少天，这对于检查2月是否润2月很有用。 123456YearMonth currentYearMonth = YearMonth.now();System.out.printf("今年这个月 %s: 有 %d 天%n", currentYearMonth, currentYearMonth.lengthOfMonth());YearMonth creditCardExpiry = YearMonth.of(2018, Month.FEBRUARY);System.out.printf("你输入的日期是 %s %n", creditCardExpiry);//今年这个月 2019-01: 有 31 天//你输入的日期是 2018-02 1.10 如何在java8中检查闰年​ LocalDate类由一个isLeapYear()方法来返回当前LocalDate对应的那年是否是闰年 123LocalDate now = LocalDate.now();System.out.println("今天是不是闰年：" + now.isLeapYear());//今天是不是闰年：false 1.11 两个日期之间包含多少天，多少月计算两个日期之间包含多少天、周、月、年。 可以用java.time.Period类完成该功能。 下面例子中将计算日期与将来的日期之间一共有几个月 12345LocalDate today = LocalDate.now();LocalDate date = LocalDate.of(2018, Month.MARCH, 14);Period periodToNextJavaRelease = Period.between(today, date);System.out.printf("日期%s和日期%s相差%s月 ", today, date, periodToNextJavaRelease.getMonths());//日期2019-01-15和日期2018-03-14相差-10月 2. 处理时间​ LocalTime类，默认的格式是hh:mm:ss:nnn，这个时间是不包含日期的 2.1 获取当前时间123LocalTime localTime = LocalTime.now();System.out.println(localTime);// 15:18:30.974 2.2 增加时间里面的小时数​ 很多时候需要对时间进行操作，比如加一个小时来计算之后的时间，java8提供了更方便的方法 如plusHours，这些方法返回的是一个新的LocalTime实例的引用，因为LocalTime是不可变的。 12345678LocalTime localTime = LocalTime.now();System.out.println("现在的时间是：" + localTime);LocalTime two = localTime.plusHours(2);System.out.println("两个小时后的时间是：" + two);System.out.println("这是相同的对象：" + localTime.equals(two));//现在的时间是：15:21:42.518//两个小时后的时间是：17:21:42.518//这是相同的对象：false 3. 处理时区​ java8中不仅将日期和时间进行了分离，同时还有时区。比如ZonId代表的是某个特定时区，ZonedDateTime代表带时区的时间，等同于以前的GregorianCalendar类。使用该类，可以将本地时间转换成另一个时区中的对应时间。 12345LocalDateTime localDateTime = LocalDateTime.now();ZoneId darWin = ZoneId.of(ZoneId.SHORT_IDS.get("ACT"));ZonedDateTime dateTimeInDarwin = ZonedDateTime.of(localDateTime, darWin);System.out.println("现在时区的时间和特定时区的时间：" + dateTimeInDarwin);//现在时区的时间和特定时区的时间：2019-01-15T15:58:31.859+09:30[Australia/Darwin] 4. 获取时间戳Instant类由一个静态的工厂方法now()可以返回当前时间戳 123Instant timestamp = Instant.now();System.out.println("当前的时间戳是：" + timestamp);//当前的时间戳是：2019-01-15T08:27:59.561Z 当前时间戳是包含日期和时间的，与java.util.Date很类似，事实上Instant就是java8以前的Date，可以使用这个两个类中的方法在这两个类型之间进行转换，比如Date.from(Instant)就是用来把Instant转换成java.util.date的，而Date。toInstant()就是将Date转换成Instant的 5.全新的日期时间格式器DateTimeFormatter​ 在java8之前，时间日期的格式化非常麻烦，经常使用SimpleDateFormat来进行格式化，但是SimpleDateFormat并不是线程安全的。 ​ 在java8中，引入了一个全新的线程安全的日期与时间格式器==DateTimeFormatter==。并且预定义好了格式。 5.1 预置的格式器来格式化日期​ 本例中使用的BASICISODATE格式会将20160414格式化成2016-04-14 1234String dayAfterTomorrow = "20180116";LocalDate formatted = LocalDate.parse(dayAfterTomorrow, DateTimeFormatter.BASIC_ISO_DATE);System.out.printf("字符 %s 格式化后的日期格式是 %s %n",dayAfterTomorrow,formatted);//字符 20180116 格式化后的日期格式是 2018-01-16 5.2 自定义格式器来解析日期​ 我们使用了预置的时间日期格式器来解析日期字符串了，但是有时预置的不能满足的时候就需要我们自定义日期格式器了，下面的例子中的日期格式是”MM dd yyyy”.你可以给DateTimeFormatter的ofPattern静态方法()传入任何的模式，它会返回一个实例，这个模式的字面量与前例中是相同的。比如M代表月，m仍代表分，无效的模式会抛异常DateTimeParseException。 12345String day = "01 15 2019";DateTimeFormatter formatter = DateTimeFormatter.ofPattern("MM dd yyyy");LocalDate holiday = LocalDate.parse(day, formatter);System.out.printf("字符 %s 转换成功后的日期是 %s%n", day, holiday);//字符 01 15 2019 转换成功后的日期是 2019-01-15 20、对日期进行格式化，转换成字符串 ​ 我们主要是对日期字符串来进行解析转换成日期，在这个例子我们相反，是把日期转换成字符。这里我们有个LocalDateTime类的实例，我们要把他转换成一个格式化好的日期串，与前例相同的是，我们仍需要制定模式串去创建一个DateTimeFormatter类的实例，但调用的是LocalDate.format()。这个方法会返回一个代表当前日期的字符串，对应的模式就是传入的DateTimeFormatter实例中定义好的。 12345LocalDateTime arrivalDate = LocalDateTime.now();DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern("MM dd yyyy HH:mm a");String format = arrivalDate.format(dateTimeFormatter);System.out.println("转换后的日期:" + format);//转换后的日期:01 15 2019 17:30 PM java8中日期与时间API的几个关键点回顾一下 ●它提供了javax.time.ZoneId用来处理时区。 ●它提供了LocalDate与LocalTime类 ●Java 8中新的时间与日期API中的所有类都是不可变且线程安全的，这与之前的Date与Calendar API中的恰好相反，那里面像java.util.Date以及SimpleDateFormat这些关键的类都不是线程安全的。 ●新的时间与日期API中很重要的一点是它定义清楚了基本的时间与日期的概念，比方说，瞬时时间，持续时间，日期，时间，时区以及时间段。它们都是基于ISO日历体系的。 每个Java开发人员都应该至少了解这套新的API中的这五个类： ●Instant 它代表的是时间戳，比如2016-04-14T14:20:13.592Z，这可以从java.time.Clock类中获取，像这样： Instant current = Clock.system(ZoneId.of(“Asia/Tokyo”)).instant(); ●LocalDate 它表示的是不带时间的日期，比如2016-04-14。它可以用来存储生日，周年纪念日，入职日期等。 ●LocalTime - 它表示的是不带日期的时间 ●LocalDateTime - 它包含了时间与日期，不过没有带时区的偏移量 ●ZonedDateTime - 这是一个带时区的完整时间，它根据UTC/格林威治时间来进行时区调整 ●这个库的主包是java.time，里面包含了代表日期，时间，瞬时以及持续时间的类。它有两个子package，一个是java.time.foramt，这个是什么用途就很明显了，还有一个是java.time.temporal，它能从更低层面对各个字段进行访问。 ●时区指的是地球上共享同一标准时间的地区。每个时区都有一个唯一标识符，同时还有一个地区/城市(Asia/Tokyo)的格式以及从格林威治时间开始的一个偏移时间。比如说，东京的偏移时间就是+09:00。 ●OffsetDateTime类实际上包含了LocalDateTime与ZoneOffset。它用来表示一个包含格林威治时间偏移量（+/-小时：分，比如+06:00或者 -08：00）的完整的日期（年月日）及时间（时分秒，纳秒）。 ●DateTimeFormatter类用于在Java中进行日期的格式化与解析。与SimpleDateFormat不同，它是不可变且线程安全的，如果需要的话，可以赋值给一个静态变量。DateTimeFormatter类提供了许多预定义的格式器，你也可以自定义自己想要的格式。当然了，根据约定，它还有一个parse()方法是用于将字符串转换成日期的，如果转换期间出现任何错误，它会抛出DateTimeParseException异常。类似的，DateFormatter类也有一个用于格式化日期的format()方法，它出错的话则会抛出DateTimeException异常。 ●再说一句，“MMM d yyyy”与“MMm dd yyyy”这两个日期格式也略有不同，前者能识别出”Jan 2 2014”与”Jan 14 2014”这两个串，而后者如果传进来的是”Jan 2 2014”则会报错，因为它期望月份处传进来的是两个字符。为了解决这个问题，在天为个位数的情况下，你得在前面补0，比如”Jan 2 2014”应该改为”Jan 02 2014”。]]></content>
      <categories>
        <category>JDK</category>
        <category>JDK8</category>
      </categories>
      <tags>
        <tag>Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle的一些常用SQL]]></title>
    <url>%2F2020%2F11%2F05%2FOracle%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8SQL%2F</url>
    <content type="text"><![CDATA[Oracle的一些常用SQL查外键建在哪个表上有时候删除某张表记录的时候，会报错外键约束不能删除。 如果不了解表之间的关系，可以通过以下语句查询到外键是建在哪张表上的： 1select * from dba_constraints where constraint_name='xxx' and constraint_type = 'R'; 在垃圾桶查被删除的表123456select object_name, original_name, partition_name, type, ts_name, createtime, droptimefrom recyclebin-- where original_name = 'extra_charge_detail'where to_date(droptime, 'yyyy-mm-dd hh24:mi:ss') &gt;= sysdate - 1 and type = 'table'order by droptime desc 回滚被删除的表123SELECT * FROM "BIN$l5cbNm6DbqDgU54w3gpQfA==$0";FLASHBACK TABLE "BIN$l5l+JWZRPQ7gU54w3goSNw==$0" TO BEFORE DROP;FLASHBACK TABLE user2 TO BEFORE DROP rename to user2_v2; -- 重命名 如果已经新建了同名表 将Clob转成字符串WM_CONCAT 函数出来的数据是clob类型的，组合成的数据自动用&#39;,&#39;分割 方式一to_char 函数 长度只能到4000，超出会截取 12345SELECT O.UUID, O.SOURCE, TO_CHAR(WM_CONCAT(OI.REQ_NO)) AS REQNOSFROM ORDER O LEFT JOIN ORDER_DETAIL OI ON O.UUID = OI.ORDER_UUIDWHERE OI.REQ_NO IS NOT NULLGROUP BY O.UUID, O.SOURCE; 方式二dbms_lob.substr 函数 可以指定长度，超出会截取 12345SELECT O.UUID, O.SOURCE, DBMS_LOB.SUBSTR((WM_CONCAT(OI.REQ_NO), 8000)) AS REQNOSFROM ORDER O LEFT JOIN ORDER_DETAIL OI ON O.UUID = OI.ORDER_UUIDWHERE OI.REQ_NO IS NOT NULLGROUP BY O.UUID, O.SOURCE; 方式三用代码转换 12345678910 Reader is = ((Clob)tuple[i]).getCharacterStream();// 得到流 BufferedReader br = new BufferedReader(is); String s = br.readLine(); StringBuffer sb = new StringBuffer();// 执行循环将字符串全部取出付值给StringBuffer由StringBuffer转成STRING while (s != null) &#123; sb.append(s); s = br.readLine(); &#125; return sb.toString();]]></content>
      <categories>
        <category>数据库</category>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 核心技术]]></title>
    <url>%2F2020%2F10%2F19%2FJVM%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[JVM 基础知识什么是 JVMJVM 是 Java Virtual Machine（ Java 虚拟机）的缩写，是通过在实际的计算机上仿真模拟各种计算机功能来实现的。由一套字节码指令集、一组寄存器、一个栈、一个垃圾回收堆和一个存储方法域等组成。JVM 屏蔽了与操作系统平台相关的信息，使得 Java 程序只需要生成在 Java 虚拟机上运行的目标代码（字节码），就可在多种平台上不加修改的运行，这也是 Java 能够“一次编译，到处运行的”原因。 JRE、JDK 和 JVM 的关系 JVM（ Java Virtual Machine， Java 虚拟机）是 JRE 的一部分。JVM 主要工作是解释自己的指令集（即字节码）并映射到本地的CPU指令集和OS的系统调用。Java 语言是跨平台运行的，不同的操作系统会有不同的 JVM 映射规则，使之与操作系统无关，完成跨平台性。 JRE（ Java Runtime Environment， Java 运行环境）：在 Java 平台，所有的程序都要在 JRE 下才能够运行。包括 JVM 和 Java 核心类库和支持文件。 JDK（ Java Development Kit，Java 开发工具包）：是用来编译、调试 Java 程序的开发工具包，包括 Java 工具（ javac/java/jdb 等）和 Java 基础的类库。 📌 包含关系：JVM &lt; JRE &lt; JDK JVM 原理Java 体系结构介绍 Class Loader（类加载器）：用于装载 .class 文件。 Execution Engine（执行引擎）：用于执行字节码或者本地方法。 运行时数据区：方法区、堆、java 栈、pc 寄存器、本地方法栈。 JVM 生命周期介绍Java 实例对应一个独立运行的 Java 程序（进程级别） 启动 启动一个 Java 程序，一个 JVM 实例就产生。拥有 public static void main(String[] args) 函数的 class 可以作为 JVM 实例运行的起点。 运行 main() 作为程序初始线程的起点，任何其他线程均可由该线程启动。 JVM 内部有两种线程：守护线程和非守护线程，main() 属于非守护线程，守护线程通常由 JVM 使用，程序可以指定创建的线程为守护线程。 消亡 当程序中的所有非守护线程都终止时，JVM 才退出；若安全管理器允许，程序也可以使用 Runtime 类或者 System.exit() 来退出。 JVM 执行引擎实例则对应了属于用户运行程序线程，它是线程级别的。 JVM 类加载器参考资料类加载器 类的生命周期 装载（loading）： 负责找到 Class 文件并加载至 JVM 中，JVM 通过类名、类所在的包名、ClassLoader完成类的加载。因此，标识一个被加载了的类：类名 + 包名 + ClassLoader 实例 ID。 链接（linking）： 验证（Verifying）：验证 Class 文件的格式以及依赖，保证 JVM 能够执行 准备（Preparing）：为由 static 修饰的成员变量分配内存，并设置默认的初始值 默认初始值如下： 八种基本数据类型默认的初始值是0 引用类型默认的初始值是 null 有 static final 修饰的会直接赋值 解析（Resolving）：把常量池中的符号引用转换为直接引用。 即 JVM 会将所有的类或接口名、字段名、方法名转换为具体的内存地址 初始化（Initializing）： 负责将类中的静态变量（类变量）赋值的过程，即只有 static 修饰的才能被初始化。 执行顺序：父类静态域或着静态代码块，然后是子类静态域或者子类静态代码块 使用（Using）： 对象实例化：执行类中构造函数的内容 如果存在父类，JVM 会通过显示或者隐示的方式先执行父类的构造函数，在堆内存中为父类的实例变量开辟空间，并赋予默认的初始值，然后在根据构造函数的代码内容将真正的值赋予实例变量本身，然后，引用变量获取对象的首地址，通过操作对象来调用实例变量和方法 垃圾收集：当对象不再被引用的时候，就会被虚拟机标上特别的垃圾记号，在堆中等待 GC 回收 对象终结：对象被 GC 回收后，对象就不再存在，对象的生命也就走到了尽头 卸载（Unloading）： 即类的生命周期走到了最后一步，程序中不再有该类的引用，该类也就会被 JVM 执行垃圾回收，从此生命结束… 类初始化顺序 类初始化的一些规则： 类从顶至底的顺序初始化，所以声明在顶部的字段的早于底部的字段初始化 超类早于子类和衍生类的初始化 如果类的初始化是由于访问静态域而触发，那么只有声明静态域的类才被初始化，而不会触发超类的初始化或者子类的 初始化即使静态域被子类或子接口或者它的实现类所引用 接口初始化不会导致父接口的初始化 静态域的初始化是在类的静态初始化期间，非静态域的初始化时在类的实例创建期间。这意味这静态域初始化在非静态域之前 非静态域通过构造器初始化，子类在做任何初始化之前构造器会隐含地调用父类的构造器，他保证了非静态或实例变量（父类）初始化早于子类 类的加载时机 当虚拟机启动时，初始化用户指定的主类，就是启动执行的 main 方法所在的类； 当遇到用以新建目标类实例的 new 指令时，初始化 new 指令的目标类，就是 new 一个类的时候要初始化； 当遇到调用静态方法的指令时，初始化该静态方法所在的类； 当遇到访问静态字段的指令时，初始化该静态字段所在的类； 子类的初始化会触发父类的初始化； 如果一个接口定义了 default 方法，那么直接实现或者间接实现该接口的类的初始化， 会 触发该接口的初始化； 使用反射 API 对某个类进行反射调用时，初始化这个类，其实跟前面一样，反射调用 要么是已经有实例了，要么是静态方法，都需要初始化； 当初次调用 MethodHandle 实例时，初始化该 MethodHandle 指向的方法所在的类。 不会初始化（可能会加载）三类加载器显示当前 ClassLoader 加载了哪些 Jar?自定义 ClassLoader添加引用类的几种方式JVM 内存模型JVM 内存结构JVM 内存整体结构JVM 栈内存结构JVM队内存结构CPU与内存行为小结：什么是 JMM ?JAVA 字节码参考资料JAVA字节码的探秘 轻松看懂JAVA字节码 什么是字节码？字节码由单字节的指令组成，理论上最多支持256个操作码（opcode），实际上Java只使用了200左右的操作码，还有一些操作码则保留给调试操作 根据指令的性质，主要分为四个大类： 栈操作指令，包括与局部变量交互的指令 程序流程控制指令 对象操作指令，包括方法调用指令 算术运算以及类型转换指令 生成字节码最简单的字节码复杂点的例子字节码的运行时结构从助记符到二进制四则运行的例子数值处理与本地变量表算数操作与类型转换一个完整的循环控制方法调用的指令 invokestatic：顾名思义，这个指令用于调用某个类的静态方法，这是方法调用指令中最快的一个。 invokespecial：用来调用构造函数，但也可以用于调用同一个类中的 private 方法, 以及可见的超类方法。 invokevirtual：如果是具体类型的目标对象，invokevirtual 用于调用公共，受保护和 package 级的私有方法。 invokeinterface：当通过接口引用来调用方法时，将会编译为 invokeinterface 指令。 invokedynamic：JDK7 新增加的指令，是实现“动态类型语言”（Dynamically Typed Language）支持而进行的升级改进，同时也是 JDK8 以后支持 lambda 表达式的实现基础。 一个动态例子JVM 启动参数系统属性运行模式堆内存GC相关分析诊断JavaAgents]]></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa 的 Entity 注解]]></title>
    <url>%2F2020%2F10%2F16%2FSpringDataJpa%E7%9A%84Entity%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa 的 Entity 注解JPA 协议中关于 Entity 的相关规定 JPA 协议里面关于实体做了一些规定。（推荐一个查看 JPA 协议的官方地址） 实体是直接进行数据库持久化操作的领域对象（即一个简单的 POJO，可以按照业务领域划分），必须通过 @Entity 注解进行标示。 实体必须有一个 public 或者 protected 的无参数构造方法。 持久化映射的注解可以标示在 Entity 的字段 field 上，如下所示： 12@Column(length = 20, nullable = false)private String userName; 除此之外，也可以将持久化注解运用在 Entity 里面的 get/set 方法上，通常我们是放在 get 方法中，如下所示： 1234@Column(length = 20, nullable = false)public String getUserName()&#123; return userName;&#125; 概括起来，就是 Entity 里面的注解生效只有两种方式：将注解写在字段上或者将注解写在方法上（JPA 里面称 Property）。 注意：在同一个 Entity 里面只能有一种方式生效。也就是说，注解要么全部写在 field 上面，要么就全部写在 Property 上面。 只要是在 @Entity 的实体里面被注解标注的字段，都会被映射到数据库中，除了使用 @Transient 注解的字段之外。 实体里面必须要有一个主键，主键标示的字段可以是单个字段，也可以是复合主键字段。 🎯有兴趣可以读一读 Java Persistence API 协议，这样在做 JPA 开发的时候就会顺手很多，可以理解很多 Hibernate 里面实现方法。当遇到解决不了的问题时，就去看协议、阅读官方文档，深入挖掘一下，可能就会找到答案。 JPA 里面支持的哪些注解首先，我们利用 IDEA 工具，打开 @Entity 所在的包，就可以看到 JPA 里面支持的注解有哪些。如下所示： 在 jakarta.persistence-api 的包路径下面大概有一百多个注解，没事的时候可以到这里面一个一个地看，也可以到 JPA 的协议里面对照查看文档。 这里只提及一些最常见的，包括 @Entity、@Table、@Access、@Id、@GeneratedValue、@Enumerated、@Basic、@Column、@Transient、@Lob、@Temporal 等。 @Entity 用于定义对象将会成为被 JPA 管理的实体，必填，将字段映射到指定的数据库表中，使用起来很简单，直接用在实体类上面即可，通过源码表达的语法如下： 12345@Target(TYPE) //表示此注解只能用在class上面public @interface Entity &#123; //可选，默认是实体类的名字，整个应用里面全局唯一。 String name() default "";&#125; @Table 用于指定数据库的表名，表示此实体对应的数据库里面的表名，非必填，默认表名和 entity 名字一样。 1234567891011@Target(TYPE) //一样只能用在类上面public @interface Table &#123; //表的名字，可选。如果不填写，系统认为和实体的名字一样为表名。 String name() default ""; //此表所在schema，可选 String schema() default ""; //唯一性约束，在创建表的时候有用，表创建之后后面就不需要了。 UniqueConstraint[] uniqueConstraints() default &#123; &#125;; //索引，在创建表的时候使用，表创建之后后面就不需要了。 Index[] indexes() default &#123;&#125;;&#125; @Access 用于指定 entity 里面的注解是写在字段上面，还是 get/set 方法上面生效，非必填。在默认不填写的情况下，当实体里面的第一个注解出现在字段上或者 get/set 方法上面，就以第一次出现的方式为准；也就是说，一个实体里面的注解既有用在 field 上面，又有用在 properties 上面的时候，看下面的代码你就会明白。 123456@Idprivate Long id;@Column(length = 20, nullable = false)public String getUserName()&#123; return userName;&#125; 那么由于 @Id 是实体里面第一个出现的注解，并且作用在字段上面，所以所有写在 get/set 方法上面的注解就会失效。而 @Access 可以干预默认值，指定是在 fields 上面生效还是在 properties 上面生效。我们通过源码看下语法： 123456789101112//表示此注解可以运用在class上(那么这个时候就可以指定此实体的默认注解生效策略了)，也可以用在方法上或者字段上(表示可以独立设置某一个字段或者方法的生效策略)；@Target( &#123; TYPE, METHOD, FIELD &#125;)@Retention(RUNTIME)public @interface Access &#123; //指定是字段上面生效还是方法上面生效 AccessType value();&#125;public enum AccessType &#123; FIELD, PROPERTY&#125; @Id 定义属性为数据库的主键，一个实体里面必须有一个主键，但不一定是这个注解，可以和 @GeneratedValue 配合使用或成对出现。 @GeneratedValue 主键生成策略，如下所示： 1234567public @interface GeneratedValue &#123; //Id的生成策略 GenerationType strategy() default AUTO; //通过 Sequences 生成 Id，常见的是 Oracle 数据库 ID 生成规则， //这个时候需要配合 @SequenceGenerator 使用 String generator() default "";&#125; 其中，GenerationType 一共有以下四个值： 12345678910public enum GenerationType &#123; //通过表产生主键，框架借由表模拟序列产生主键，使用该策略可以使应用更易于数据库移植 TABLE, //通过序列产生主键，通过 @SequenceGenerator 注解指定序列名，MySql 不支持这种方式 SEQUENCE, //采用数据库 ID 自增长， 一般用于mysql数据库 IDENTITY, //JPA 自动选择合适的策略，是默认选项 AUTO&#125; @Enumerated 这个注解很好用，因为它对 enum 提供了下标和 name 两种方式，用法直接映射在 enum 枚举类型的字段上。请看下面源码： 123456789101112@Target(&#123;METHOD, FIELD&#125;) //作用在方法和字段上public @interface Enumerated &#123; //枚举映射的类型，默认是ORDINAL（即枚举字段的下标）。 EnumType value() default ORDINAL;&#125;public enum EnumType &#123; //映射枚举字段的下标 ORDINAL, //映射枚举的Name STRING&#125; 再来看一个 User 里面关于性别枚举的例子，你就会知道 @Enumerated 在这里没什么作用了，如下所示： 123456789101112131415161718//有一个枚举类，用户的性别public enum Gender &#123; MALE("男性"), FEMALE("女性"); private String value; private Gender(String value) &#123; this.value = value; &#125;&#125;//实体类@Enumerated的写法如下@Entity@Table(name = "tb_user")public class User implements Serializable &#123; @Enumerated(EnumType.STRING) @Column(name = "user_gender") private Gender gender; .......................&#125; 这时候插入两条数据，数据库里面的值会变成 MALE/FEMALE，而不是“男性” / 女性。 📌经验分享： 如果我们用 @Enumerated（EnumType.ORDINAL），这时候数据库里面的值是 0、1。但是实际工作中，不建议用数字下标，因为枚举里面的属性值是会不断新增的，如果新增一个，位置变化了就惨了。并且 0、1、2 这种下标在数据库里面看着非常痛苦，时间长了就会一点也看不懂了。 @Basic 表示属性是到数据库表的字段的映射。如果实体的字段上没有任何注解，默认即为 @Basic。也就是说默认所有的字段肯定是和数据库进行映射的，并且默认为 Eager 类型。 123456public @interface Basic &#123; //可选，EAGER（默认）：立即加载；LAZY：延迟加载。（LAZY主要应用在大字段上面） FetchType fetch() default EAGER; //可选。这个字段是否可以为null，默认是true。 boolean optional() default true;&#125; @Transient 表示该属性并非一个到数据库表的字段的映射，表示非持久化属性。JPA 映射数据库的时候忽略它，与 @Basic 有相反的作用。也就是每个字段上面 @Transient 和 @Basic 必须二选一，而什么都不指定的话，默认是 @Basic。 @Column 定义该属性对应数据库中的列名。 12345678910111213141516public @interface Column &#123; //数据库中的表的列名；可选，如果不填写认为字段名和实体属性名一样。 String name() default ""; //是否唯一。默认false，可选。 boolean unique() default false; //数据字段是否允许空。可选，默认true。 boolean nullable() default true; //执行insert操作的时候是否包含此字段，默认，true，可选。 boolean insertable() default true; //执行update的时候是否包含此字段，默认，true，可选。 boolean updatable() default true; //表示该字段在数据库中的实际类型。 String columnDefinition() default ""; //数据库字段的长度，可选，默认255 int length() default 255;&#125; @Temporal 用来设置 Date 类型的属性映射到对应精度的字段，存在以下三种情况： @Temporal(TemporalType.DATE) 映射为日期（只有日期） @Temporal(TemporalType.TIME) 映射为日期（只有时间） @Temporal(TemporalType.TIMESTAMP) 映射为日期（日期+时间） 看一个完整的例子，感受一下上面提到的注解的完整用法，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.example.jpa.example1;import lombok.Data;import javax.persistence.*;import java.util.Date;@Entity@Table(name = "user_topic")@Access(AccessType.FIELD)@Datapublic class UserTopic &#123; @Id @Column(name = "id", nullable = false) @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; @Column(name = "title", nullable = true, length = 200) private String title; @Basic @Column(name = "create_user_id", nullable = true) private Integer createUserId; @Basic(fetch = FetchType.LAZY) @Column(name = "content", nullable = true, length = -1) @Lob private String content; @Basic(fetch = FetchType.LAZY) @Column(name = "image", nullable = true) @Lob private byte[] image; @Basic @Column(name = "create_time", nullable = true) @Temporal(TemporalType.TIMESTAMP) private Date createTime; @Basic @Column(name = "create_date", nullable = true) @Temporal(TemporalType.DATE) private Date createDate; @Enumerated(EnumType.STRING) @Column(name = "topic_type") private Type type; @Transient private String transientSimple; //非数据库映射字段，业务类型的字段 public String getTransientSimple() &#123; return title + "auto:jack" + type; &#125; //有一个枚举类，主题的类型 public enum Type &#123; EN("英文"), CN("中文"); private final String des; Type(String des) &#123; this.des = des; &#125; &#125;&#125; 其实这里面的很多注解都可以省略，直接使用默认的就可以。如 @Basic、@Column 名字有一定的映射策略，所以可以省略。 此外，@Access 也可以省略，我们只要在这些类里面保持一致就可以了。 生成这些注解的小技巧有时候老的 Table 非常多，一个一个去写 entity 会特别累，因此可以利用 IDEA 工具直接帮我们生成 Entity 类。关键步骤如下： 首先，打开 Persistence 视图，点击 Generate Persistence Mapping，接着点击选中数据源，如下图所示： 然后，选择表和字段，并点击 OK。 这样就可以生成我们想要的实体了，多简单。如果是新库、新表，我们也可以先定义好实体，通过实体配置 JPA 的 spring.jpa.generate-ddl=true，反向直接生成 DDL 操作数据库生成表结构。 🎯注意：在生产环境中要把外键关联关系关闭，不然会出现意想不到的 ERROR，毕竟生产环境不同开发环境，我们可以通过在开发环境生成的表导出 DDL 到生产执行。利用生成 DDL 来做测试和写案例，可以省去创建表的时间，只需要关注代码就行了。 联合主键可以通过 javax.persistence.EmbeddedId 和 javax.persistence.IdClass 两个注解实现联合主键的效果。 如何通过 @IdClass 做到联合主键？第一步：新建一个 UserInfoID 类里面是联合主键。 12345678910111213package com.example.jpa.example1;import lombok.AllArgsConstructor;import lombok.Builder;import lombok.Data;import lombok.NoArgsConstructor;import java.io.Serializable;@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class UserInfoID implements Serializable &#123; private String name, telephone;&#125; 第二步：再新建一个 UserInfo 的实体，采用 @IdClass 引用联合主键类。 12345678910111213@Entity@Data@Builder@IdClass(UserInfoID.class)@AllArgsConstructor@NoArgsConstructorpublic class UserInfo &#123; private Integer ages; @Id private String name; @Id private String telephone;&#125; 第三步：新增一个 UserInfoRepository 类来做 CRUD 操作。 1234package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;public interface UserInfoRepository extends JpaRepository&lt;UserInfo, UserInfoID&gt; &#123;&#125; 第四步：写一个测试用例，测试一下。 123456789101112131415161718package com.example.jpa.example1;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;import java.util.Optional;@DataJpaTestpublic class UserInfoRepositoryTest &#123; @Autowired private UserInfoRepository userInfoRepository; public void testIdClass() &#123; userInfoRepository.save(UserInfo.builder().ages(1) .name("jack").telephone("123456789").build()); Optional&lt;UserInfo&gt; userInfo = userInfoRepository.findById( UserInfoID.builder().name("jack").telephone("123456789").build()); System.out.println(userInfo.get()); &#125;&#125; 输出结果如下： 123Hibernate: create table user_info (name varchar(255) not null, telephone varchar(255) not null, ages integer, primary key (name, telephone))Hibernate: select userinfo0_.name as name1_3_0_, userinfo0_.telephone as telephon2_3_0_, userinfo0_.ages as ages3_3_0_ from user_info userinfo0_ where userinfo0_.name=? and userinfo0_.telephone=?UserInfo(ages=1, name=jack, telephone=123456789) 上面的例子，表的主键是 (name, telephone)，而 Entity 里面不再是一个 @Id 字段了。 @Embeddable 与 @EmbeddedId 注解使用第一步：在我们上面例子中的 UserInfoID 里面添加 @Embeddable 注解。 12345678@Data@Builder@AllArgsConstructor@NoArgsConstructor@Embeddablepublic class UserInfoID implements Serializable &#123; private String name, telephone;&#125; 第二步：改一下刚才的 User 对象，删除 @IdClass，添加 @EmbeddedId 注解，如下： 123456789101112@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class UserInfo &#123; private Integer ages; @EmbeddedId private UserInfoID userInfoID; @Column(unique = true) private String uniqueNumber;&#125; 第三步：UserInfoRepository 不变，我们直接修改一下测试用例。 12345678910111213@Test public void testIdClass() &#123; userInfoRepository.save( UserInfo.builder() .ages(1) .userInfoID(UserInfoID.builder().name("jack") .telephone("123456789").build()) .build()); Optional&lt;UserInfo&gt; userInfo = userInfoRepository.findById( UserInfoID.builder().name("jack").telephone("123456789").build()); System.out.println(userInfo.get()); &#125; 运行完之后，你可以得到相同的结果。那么 @IdClass 和 @EmbeddedId 的区别是什么？有以下两个方面： 在使用的时候，Embedded 用的是对象，而 IdClass 用的是具体的某一个字段； 二者的JPQL 也会不一样： 用 @IdClass JPQL 的写法：SELECT u.name FROM UserInfo u 用 @EmbeddedId 的 JPQL 的写法：select u.userInfoId.name FROM UserInfo u 联合主键还有需要注意的就是，它与唯一性索引约束的区别是写法不同，如上面所讲，唯一性索引的写法如下： 12@Column(unique = true)private String uniqueNumber; 实体之间的继承关系如何实现在 Java 面向对象的语言环境中，@Entity 之间的关系多种多样，而根据 JPA 的规范，大致可以将其分为以下几种： 纯粹的继承，和表没关系，对象之间的字段共享。利用注解 @MappedSuperclass，协议规定父类不能是 @Entity。 单表多态问题，同一张 Table，表示了不同的对象，通过一个字段来进行区分。利用@Inheritance(strategy = InheritanceType.SINGLE_TABLE)注解完成，只有父类有 @Table。 多表多态，每一个子类一张表，父类的表拥有所有公用字段。通过@Inheritance(strategy = InheritanceType.JOINED)注解完成，父类和子类都是表，有公用的字段在父表里面。 Object 的继承，数据库里面每一张表是分开的，相互独立不受影响。通过@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS)注解完成，父类（可以是一张表，也可以不是）和子类都是表，相互之间没有关系。 单表多态InheritanceType.SINGLE_TABLE 父类实体对象与各个子实体对象共用一张表，通过一个字段的不同值代表不同的对象。 举例，抽象一个 Book 对象，如下所示： 12345678910111213package com.example.jpa.example1.book;import lombok.Data;import javax.persistence.*;@Entity(name="book")@Data@Inheritance(strategy = InheritanceType.SINGLE_TABLE)@DiscriminatorColumn(name="color", discriminatorType = DiscriminatorType.STRING)public class Book &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String title;&#125; 再新建一个 BlueBook 对象，作为 Book 的子对象。 123456789101112package com.example.jpa.example1.book;import lombok.Data;import lombok.EqualsAndHashCode;import javax.persistence.DiscriminatorValue;import javax.persistence.Entity;@Entity@Data@EqualsAndHashCode(callSuper=false)@DiscriminatorValue("blue")public class BlueBook extends Book&#123; private String blueMark;&#125; 再新建一个 RedBook 对象，作为 Book 的另一子对象。 12345678//红皮书@Entity@DiscriminatorValue("red")@Data@EqualsAndHashCode(callSuper=false)public class RedBook extends Book &#123; private String redMark;&#125; 这时，一共新建了三个 Entity 对象，其实都是指 book 这一张表，通过 book 表里面的 color 字段来区分红书还是绿书。 测试看看结果，新建一个 RedBookRepository 类，结果如下： 123package com.example.jpa.example1.book;import org.springframework.data.jpa.repository.JpaRepository;public interface RedBookRepository extends JpaRepository&lt;RedBook, Long&gt;&#123;&#125; 然后再新建一个测试用例： 123456789101112131415161718192021package com.example.jpa.example1;import com.example.jpa.example1.book.RedBook;import com.example.jpa.example1.book.RedBookRepository;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;@DataJpaTestpublic class RedBookRepositoryTest &#123; @Autowired private RedBookRepository redBookRepository; @Test public void testRedBook() &#123; RedBook redBook = new RedBook(); redBook.setTitle("redbook"); redBook.setRedMark("redmark"); redBook.setId(1L); redBookRepository.saveAndFlush(redBook); RedBook r = redBookRepository.findById(1L).get(); System.out.println(r.getId() + ":" + r.getTitle() + ":" + r.getRedMark()); &#125;&#125; 最后看一下执行结果： 1Hibernate: create table book (color varchar(31) not null, id bigint not null, title varchar(255), blue_mark varchar(255), red_mark varchar(255), primary key (id)) 发现只创建了一张表，insert 了一条数据，但是 color 字段默认给的是 red。 1Hibernate: insert into book (title, red_mark, color, id) values (?, ?, 'red', ?) 那么再看一下打印结果： 11:redbook:redmark 结果完全和预期的一样，这说明了 RedBook、BlueBook、Book，都是一张表，通过字段 color 的值不一样，来区分不同的实体。 多表多态InheritanceType.JOINED 在这种映射策略里面，继承结构中的每一个实体（entity）类都会映射到数据库里一个单独的表中。也就是说，每个实体（entity）都会被映射到数据库中，一个实体（entity）类对应数据库中的一个表。 其中根实体（root entity）对应的表中定义了主键（primary key），所有的子类对应的数据库表都要共同使用 Book 里面的 @ID 这个主键。 首先，我们改一下上面的三个实体，测试一下InheritanceType.JOINED，改动如下： 123456789101112package com.example.jpa.example1.book;import lombok.Data;import javax.persistence.*;@Entity(name="book")@Data@Inheritance(strategy = InheritanceType.JOINED)public class Book &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String title;&#125; 其次，我们 Book 父类、改变 Inheritance 策略、删除 DiscriminatorColumn，你会看到如下结果。 123456789101112package com.example.jpa.example1.book;import lombok.Data;import lombok.EqualsAndHashCode;import javax.persistence.Entity;import javax.persistence.PrimaryKeyJoinColumn;@Entity@Data@EqualsAndHashCode(callSuper=false)@PrimaryKeyJoinColumn(name = "book_id", referencedColumnName = "id")public class BlueBook extends Book&#123; private String blueMark;&#125; 123456789101112package com.example.jpa.example1.book;import lombok.Data;import lombok.EqualsAndHashCode;import javax.persistence.Entity;import javax.persistence.PrimaryKeyJoinColumn;@Entity@PrimaryKeyJoinColumn(name = "book_id", referencedColumnName = "id")@Data@EqualsAndHashCode(callSuper=false)public class RedBook extends Book &#123; private String redMark;&#125; 然后，BlueBook 和 RedBook 也删除 DiscriminatorColumn，新增@PrimaryKeyJoinColumn(name = &quot;book_id&quot;, referencedColumnName = &quot;id&quot;)，和 book 父类共用一个主键值，而 RedBookRepository 和测试用例不变，我们执行看一下结果。 12345Hibernate: create table blue_book (blue_mark varchar(255), book_id bigint not null, primary key (book_id))Hibernate: create table book (id bigint not null, title varchar(255), primary key (id))Hibernate: create table red_book (red_mark varchar(255), book_id bigint not null, primary key (book_id))Hibernate: alter table blue_book add constraint FK9uuwgq7a924vtnys1rgiyrlk7 foreign key (book_id) references bookHibernate: alter table red_book add constraint FKk8rvl61bjy9lgsr9nhxn5soq5 foreign key (book_id) references book 上述代码可以看到，我们一共创建了三张表，并且新增了两个外键约束； 而我们 save 的时候也生成了两个 insert 语句，如下： 12Hibernate: insert into book (title, id) values (?, ?)Hibernate: insert into red_book (red_mark, book_id) values (?, ?) 而打印结果依然不变。 11:redbook:redmark 这个方法和上面的 InheritanceType.SINGLE_TABLE 区别在于表的数量和关系不一样，这是表设计的另一种方式。 Object 的继承InheritanceType.TABLE_PER_CLASS 和 @MappedSuperClass 一样 我们在使用 @MappedSuperClass 主键的时候，如果不指定 @Inheritance，默认就是此种 TABLE_PER_CLASS 模式。当然了，我们也显示指定，要求继承基类的都是一张表，而父类不是表，是 java 对象的抽象类。我们看一个例子。 首先，还是改一下上面的三个实体。 123456789101112package com.example.jpa.example1.book;import lombok.Data;import javax.persistence.*;@Entity(name="book")@Data@Inheritance(strategy = InheritanceType.TABLE_PER_CLASS)public class Book &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String title;&#125; 其次，Book 表采用 TABLE_PER_CLASS 策略，其子实体类都代表各自的表，实体代码如下： 12345678910package com.example.jpa.example1.book;import lombok.Data;import lombok.EqualsAndHashCode;import javax.persistence.Entity;@Entity@Data@EqualsAndHashCode(callSuper=false)public class RedBook extends Book &#123; private String redMark;&#125; 12345678910package com.example.jpa.example1.book;import lombok.Data;import lombok.EqualsAndHashCode;import javax.persistence.Entity;@Entity@Data@EqualsAndHashCode(callSuper=false)public class BlueBook extends Book&#123; private String blueMark;&#125; 这时，从 RedBook 和 BlueBook 里面去掉 PrimaryKeyJoinColumn， 而 RedBookRepository 和测试用例不变，我们执行看一下结果。 123Hibernate: create table blue_book (id bigint not null, title varchar(255), blue_mark varchar(255), primary key (id))Hibernate: create table book (id bigint not null, title varchar(255), primary key (id))Hibernate: create table red_book (id bigint not null, title varchar(255), red_mark varchar(255), primary key (id)) 这里可以看到，我们还是创建了三张表，但三张表什么关系也没有。 而 insert 语句也只有一条，如下： 1Hibernate: insert into red_book (title, red_mark, id) values (?, ?, ?) 打印结果还是不变。 11:redbook:redmark 这个方法与上面两个相比较，语义更加清晰，是比较常用的一种做法。 关于继承关系的经验之谈 🎯个人经验来看，@Inheritance 的这种使用方式会逐渐被淘汰，因为这样的表的设计很复杂，本应该在业务层面做的事情（多态），却都在 dataSource 的表级别做了。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>Entity 注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa @Query的使用]]></title>
    <url>%2F2020%2F10%2F14%2FSpringDataJpa%E7%9A%84%40Query%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Spring Data Jpa @Query的使用 快速体验 @Query 的方法、 JpaQueryLookupStrategy 关键源码剖析、 @Query 的基本用法、 @Query 之 Projections 应用返回指定 DTO、 @Query 动态查询解决方法 快速体验 @Query 的方法12345678910package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import org.springframework.data.repository.query.Param;public interface UserDtoRepository extends JpaRepository&lt;User,Long&gt; &#123; //通过query注解根据name查询user信息 @Query("From User where name=:name") User findByQuery(@Param("name") String nameParam);&#125; 然后，我们新增一个测试类： 12345678910111213141516171819package com.example.jpa.example1;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;@DataJpaTestpublic class UserRepositoryQueryTest &#123; @Autowired private UserDtoRepository userDtoRepository; @Test public void testQueryAnnotation() &#123;//新增一条数据方便测试 userDtoRepository.save(User.builder().name("jackxx").email("123456@126.com").sex("man").address("shanghai").build()); //调用上面的方法查看结果 User user2 = userDtoRepository.findByQuery("jack"); System.out.println(user2); &#125;&#125; 最后，看到运行的结果如下： 123Hibernate: insert into user (address, email, name, sex, version, id) values (?, ?, ?, ?, ?, ?)Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_, user0_.version as version6_0_ from user user0_ where user0_.name=?User(id=1, name=jack, email=123456@126.com, version=0, sex=man, address=shanghai) 通过上面的例子不是通过方法名来生成查询语法，而是 @Query 注解在其中起了作用， 使 From User where name=:name JPQL 生效了。 JpaQueryLookupStrategy 关键源码剖析打开 QueryExecutorMethodInterceptor 类，找到如下代码： 再运行上面的测试用例，这时候在99行设置一个断点，可以看到默认的策略是CreateIfNotFound，也就是如果有@Query注解，那么以@Query的注解内容为准，可以忽略方法名。 进入到 lookupStrategy.resolveQuery 里面，如下所示： 通过上图的断点和红框之处，发现Spring Data JPA 这个地方使用了策略、模式，当我们自己写策略模式的时候也可以进行参考。 往下 debug，进入到 resolveQuery 方法里面，如下图所示： 图中 ①处，如果 Query 注解找到了，就不会走到 ② 处了。 查看 Query 属性，你会发现这里同时生成了两个 SQL： 查询总数的 Query 定义 查询结果的 Query 定义 如果想看 Query 具体是怎么生成的、上面的 @Param 注解是怎么生效的，可以在上面的图 ① 处 debug 继续往里面看，如下所示： @Query 的基本用法基本用法123456789101112131415161718192021222324252627282930313233343536373839package org.springframework.data.jpa.repository;public @interface Query &#123; /** * 指定JPQL的查询语句。（nativeQuery=true的时候，是原生的Sql语句） */ String value() default ""; /** * 指定count的JPQL语句，如果不指定将根据query自动生成。 * （如果当nativeQuery=true的时候，指的是原生的Sql语句） */ String countQuery() default ""; /** * 根据哪个字段来count，一般默认即可。 */ String countProjection() default ""; /** * 默认是false，表示value里面是不是原生的sql语句 */ boolean nativeQuery() default false; /** * 可以指定一个query的名字，必须唯一的。 * 如果不指定，默认的生成规则是： * &#123;$domainClass&#125;.$&#123;queryMethodName&#125; */ String name() default ""; /* * 可以指定一个count的query的名字，必须唯一的。 * 如果不指定，默认的生成规则是： * &#123;$domainClass&#125;.$&#123;queryMethodName&#125;.count */ String countName() default "";&#125; @Query 用法是使用 JPQL 为实体创建声明式查询方法。我们一般只需要关心 @Query 里面的 value 和 nativeQuery、countQuery 的值即可。 使用声明式 JPQL 查询有个好处，就是启动的时候就知道你的语法正确不正确。 JPQL 的语法查询的语法结构，代码如下： 1234SELECT ... FROM ...[WHERE ...][GROUP BY ... [HAVING ...]][ORDER BY ...] 它的语法结构有点类似 SQL，唯一的区别就是 JPQL FROM 后面跟的是对象，而 SQL 里面的字段对应的是对象里面的属性字段。 同理我们看一下 update 和 delete 的语法结构： 12DELETE FROM ... [WHERE ...]UPDATE ... SET ... [WHERE ...] 其中“…”省略的部分是实体对象名字和实体对象里面的字段名字，而其中类似 SQL 一样包含的语法关键字有： 1SELECT FROM WHERE UPDATE DELETE JOIN OUTER INNER LEFT GROUP BY HAVING FETCH DISTINCT OBJECT NULL TRUE FALSE NOT AND OR BETWEEN LIKE IN AS UNKNOWN EMPTY MEMBER OF IS AVG MAX MIN SUM COUNT ORDER BY ASC DESC MOD UPPER LOWER TRIM POSITION CHARACTER_LENGTH CHAR_LENGTH BIT_LENGTH CURRENT_TIME CURRENT_DATE CURRENT_TIMESTAMP NEW EXISTS ALL ANY SOME 通过查看这个 Oracle 的 JPQL 文档，找到解决问题的办法。 @Query 用法案例 @Query 怎么使用、怎么传递参数、怎么分页 案例 1要在 Repository 的查询方法上声明一个注解，这里就是 @Query 注解标注的地方。 1234public interface UserRepository extends JpaRepository&lt;User, Long&gt;&#123; @Query("select u from User u where u.emailAddress = ?1") User findByEmailAddress(String emailAddress);&#125; 案例 2LIKE 查询，注意 firstname 不会自动加上“%”关键字。 1234public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query("select u from User u where u.firstname like %?1") List&lt;User&gt; findByFirstnameEndsWith(String firstname);&#125; 案例 3直接用原始 SQL，nativeQuery = true 即可。 1234public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query(value = "SELECT * FROM USERS WHERE EMAIL_ADDRESS = ?1", nativeQuery = true) User findByEmailAddress(String emailAddress);&#125; 注意：nativeQuery 不支持直接 Sort 的参数查询。 案例 4下面是nativeQuery 的排序错误的写法，会导致无法启动。 1234public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query(value = "select * from user_info where first_name=?1",nativeQuery = true) List&lt;UserInfoEntity&gt; findByFirstName(String firstName,Sort sort);&#125; 案例 5nativeQuery 排序的正确写法。 1234@Query(value = "select * from user_info where first_name=?1 order by ?2",nativeQuery = true)List&lt;UserInfoEntity&gt; findByFirstName(String firstName,String sort);//调用的地方写法last_name是数据里面的字段名，不是对象的字段名repository.findByFirstName("jackzhang","last_name"); 通过上面几个案例，我们看到了 @Query 的几种用法，你就会明白排序、参数、使用方法、LIKE、原始 SQL 怎么写。 @Query 的排序@Query中在用JPQL的时候，想要实现排序，方法上直接用 PageRequest 或者 Sort 参数都可以做到。 在排序实例中，实际使用的属性需要与实体模型里面的字段相匹配，这意味着它们需要解析为查询中使用的属性或别名。我们看一下例子，这是一个state_field_path_expression JPQL的定义，并且 Sort 的对象支持一些特定的函数。 案例 6Sort and JpaSort 的使用，它可以进行排序。 1234567891011121314public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query("select u from User u where u.lastname like ?1%") List&lt;User&gt; findByAndSort(String lastname, Sort sort); @Query("select u.id, LENGTH(u.firstname) as fn_len from User u where u.lastname like ?1%") List&lt;Object[]&gt; findByAsArrayAndSort(String lastname, Sort sort);&#125;//调用方的写法，如下：repo.findByAndSort("lannister", new Sort("firstname")); repo.findByAndSort("stark", new Sort("LENGTH(firstname)")); repo.findByAndSort("targaryen", JpaSort.unsafe("LENGTH(firstname)"));repo.findByAsArrayAndSort("bolton", new Sort("fn_len")); 上面这个案例讲述的是排序用法，再来看下 @Query 的分页用法。 @Query 的分页@Query 的分页分为两种情况，分别为 JQPl 的排序和 nativeQuery 的排序。 案例 7直接用 Page 对象接受接口，参数直接用 Pageable 的实现类即可。 12345678public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query(value = "select u from User u where u.lastname = ?1") Page&lt;User&gt; findByLastname(String lastname, Pageable pageable);&#125;//调用者的写法repository.findByFirstName("jackzhang",new PageRequest(1,10)); 案例 8@Query 对原生 SQL 的分页支持，并不是特别友好，因为这种写法比较“骇客”，可能随着版本的不同会有所变化。我们以 MySQL 为例。 12345678910111213 public interface UserRepository extends JpaRepository&lt;UserInfoEntity, Integer&gt;, JpaSpecificationExecutor&lt;UserInfoEntity&gt; &#123; @Query(value = "select * from user_info where first_name=?1 /* #pageable# */", countQuery = "select count(*) from user_info where first_name=?1", nativeQuery = true) Page&lt;UserInfoEntity&gt; findByFirstName(String firstName, Pageable pageable);&#125;//调用者的写法return userRepository.findByFirstName("jackzhang",new PageRequest(1,10, Sort.Direction.DESC,"last_name"));//打印出来的sqlselect * from user_info where first_name=? /* #pageable# */ order by last_name desc limit ?, ? 注意：这个注释 / #pageable# / 必须有。 另外，随着版本的变化，这个方法有可能会进行优化。 此外还有一种实现方法，就是自己写两个查询方法，自己手动分页。 @Param 用法@Param 注解指定方法参数的具体名称，通过绑定的参数名字指定查询条件，这样不需要关心参数的顺序。比较推荐这种做法，因为它比较利于代码重构。如果不用 @Param 也是可以的，参数是有序的，这使得查询方法对参数位置的重构容易出错。我们看个案例。 案例 9根据 firstname 和 lastname 参数查询 user 对象。 123456public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query("select u from User u where u.firstname = :firstname or u.lastname = :lastname") User findByLastnameOrFirstname(@Param("lastname") String lastname, @Param("firstname") String firstname);&#125; 案例 10根据参数进行查询，top 10 前面说的“query method”关键字照样有用，如下所示： 1234567public interface UserRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query("select u from User u where u.firstname = :firstname or u.lastname = :lastname") User findTop10ByLastnameOrFirstname(@Param("lastname") String lastname, @Param("firstname") String firstname);&#125; 通过 @Query 定义自己的查询方法时，建议也用 Spring Data JPA 的 name query 的命名方法，这样下来风格就比较统一了。 @Query 之 Projections 应用返回指定 DTOJPA文档 - 3.3.1 关于 Projections 新增一张表 UserExtend，里面包含身份证、学号、年龄等信息，最终我们的实体变成如下模样： 123456789101112131415161718192021222324252627282930313233343536@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class UserExtend &#123; //用户扩展信息表 @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private Long userId; private String idCard; private Integer ages; private String studentNumber;&#125;@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class User &#123; //用户基本信息表 @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; @Version private Long version; private String sex; private String address;&#125; 如果我们想定义一个 DTO 对象，里面只要 name、email、idCard，这个时候我们怎么办呢？这种场景非常常见，但好多人使用的都不是最佳实践，我在这里介绍几种方式做一下对比。 我们先看一下，刚学 JPA 的时候别手别脚的写法： 1234567891011public interface UserDtoRepository extends JpaRepository&lt;User,Long&gt; &#123; /** * 查询用户表里面的name、email和UserExtend表里面的idCard * @param id * @return */ @Query("select u.name,u.email,e.idCard from User u,UserExtend e where u.id= e.userId and u.id=:id") List&lt;Object[]&gt; findByUserId(@Param("id") Long id);&#125; 我们通过下面的测试用例来取上面 findByUserId 方法返回的数据组结果值，再塞到 DTO 里面，代码如下： 12345678910111213@Testpublic void testQueryAnnotation() &#123;//新增一条用户数据 userDtoRepository.save(User.builder().name("jack").email("123456@126.com").sex("man").address("shanghai").build());//再新增一条和用户一对一的UserExtend数据 userExtendRepository.save(UserExtend.builder().userId(1L).idCard("shengfengzhenghao").ages(18).studentNumber("xuehao001").build());//查询我们想要的结果 List&lt;Object[]&gt; userArray = userDtoRepository.findByUserId(1L); System.out.println(String.valueOf(userArray.get(0)[0])+String.valueOf(userArray.get(0)[1])); UserDto userDto = UserDto.builder().name(String.valueOf(userArray.get(0)[0])).build(); System.out.println(userDto);&#125; 其实经验的丰富的“老司机”一看就知道这肯定不是最佳实践，这多麻烦呀，肯定会有更优解。那么我们再对此稍加改造，用 UserDto 接收返回结果。 利用 class UserDto 获取我们想要的结果首先，我们新建一个 UserDto 类的内容。 1234567891011package com.example.jpa.example1;import lombok.AllArgsConstructor;import lombok.Builder;import lombok.Data;@Data@Builder@AllArgsConstructorpublic class UserDto &#123; private String name,email,idCard;&#125; 其次，我们看下利用 @Query 在 Repository 里面怎么写。 123456public interface UserDtoRepository extends JpaRepository&lt;User, Long&gt; &#123; @Query("select new com.example.jpa.example1.UserDto(CONCAT(u.name,'JK123'),u.email,e.idCard) from User u,UserExtend e where u.id= e.userId and u.id=:id") UserDto findByUserDtoId(@Param("id") Long id);&#125; 我们利用 JPQL，new 了一个 UserDto；再通过构造方法，接收查询结果。其中你会发现，我们用 CONCAT 的关键字做了一个字符串拼接。 你可以查看JPQL的 Oracle 文档，也可以通过源码来看支持的关键字有哪些。 打开 ParameterizedFunctionExpression 会发现 Hibernate 支持的关键字有这么多，都是 MySQL 数据库的查询关键字。 然后，我们写一个测试方法，调用上面的方法测试一下。 123456789101112131415@Testpublic void testQueryAnnotationDto() &#123; userDtoRepository.save( User.builder().name("jack").email("123456@126.com") .sex("man").address("shanghai").build()); userExtendRepository.save( UserExtend.builder() .userId(1L) .idCard("shengfengzhenghao") .ages(18) .studentNumber("xuehao001") .build()); UserDto userDto = userDtoRepository.findByUserDtoId(1L); System.out.println(userDto);&#125; 最后，我们运行一下测试用例，结果如下。这时你会发现，我们按照预期操作得到了 UserDto 的结果。 1234Hibernate: insert into user (address, email, name, sex, version, id) values (?, ?, ?, ?, ?, ?)Hibernate: insert into user_extend (ages, id_card, student_number, user_id, id) values (?, ?, ?, ?, ?)Hibernate: select (user0_.name||'JK123') as col_0_0_, user0_.email as col_1_0_, userextend1_.id_card as col_2_0_ from user user0_ cross join user_extend userextend1_ where user0_.id=userextend1_.user_id and user0_.id=?UserDto(name=jackJK123, email=123456@126.com, idCard=shengfengzhenghao) 那么还有更简单的方法吗？答案是有，下面我们利用 UserDto 接口来实现一下。 利用 UserDto 接口获得我们想要的结果首先，新增一个 UserSimpleDto 接口来得到我们想要的 name、email、idCard 信息。 1234567package com.example.jpa.example1;public interface UserSimpleDto &#123; String getName(); String getEmail(); String getIdCard();&#125; 其次，在 UserDtoRepository 里面新增一个方法，返回结果是 UserSimpleDto 接口。 123456public interface UserDtoRepository extends JpaRepository&lt;User, Long&gt; &#123;//利用接口DTO获得返回结果，需要注意的是每个字段需要as和接口里面的get方法名字保持一样@Query("select CONCAT(u.name,'JK123') as name,UPPER(u.email) as email ,e.idCard as idCard from User u,UserExtend e where u.id= e.userId and u.id=:id")UserSimpleDto findByUserSimpleDtoId(@Param("id") Long id);&#125; 然后，测试用例写法如下。 12345678910111213141516@Test public void testQueryAnnotationDto() &#123; userDtoRepository.save( User.builder().name("jack").email("123456@126.com") .sex("man").address("shanghai").build()); userExtendRepository.save( UserExtend.builder() .userId(1L) .idCard("shengfengzhenghao") .ages(18) .studentNumber("xuehao001") .build()); UserSimpleDto userDto = userDtoRepository.findByUserSimpleDtoId(1L); System.out.println(userDto); System.out.println(userDto.getName() + ":" + userDto.getEmail() + ":" + userDto.getIdCard()); &#125; 最后，我们执行可以得到如下结果。 12org.springframework.data.jpa.repository.query.AbstractJpaQuery$TupleConverter$TupleBackedMap@373c28e5jackJK123:123456@126.COM:shengfengzhenghao 我们发现，比起 DTO 我们不需要 new 了，并且接口只能读，那么我们返回的结果 DTO 的职责就更单一了，只用来查询。 接口的方式是比较推荐的做法，因为它是只读的，对构造方法没有要求，返回的实际是 HashMap。 @Query 动态查询解决方法我们看一个例子，来了解一下如何实现 @Query 的动态参数查询。 首先，新增一个 UserOnlyName 接口，只查询 User 里面的 name 和 email 字段。 1234567package com.example.jpa.example1;//获得返回结果public interface UserOnlyName &#123; String getName(); String getEmail();&#125; 其次，在我们的 UserDtoRepository 里面新增两个方法：一个是利用 JPQL 实现动态查询，一个是利用原始 SQL 实现动态查询。 12345678910111213141516171819202122232425package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import org.springframework.data.repository.query.Param;import java.util.List;public interface UserDtoRepository extends JpaRepository&lt;User, Long&gt; &#123; /** * 利用JQPl动态查询用户信息 * @param name * @param email * @return UserSimpleDto接口 */ @Query("select u.name as name,u.email as email from User u where (:name is null or u.name =:name) and (:email is null or u.email =:email)") UserOnlyName findByUser(@Param("name") String name,@Param("email") String email); /** * 利用原始sql动态查询用户信息 * @param user * @return */@Query(value = "select u.name as name,u.email as email from user u where (:#&#123;#user.name&#125; is null or u.name =:#&#123;#user.name&#125;) and (:#&#123;#user.email&#125; is null or u.email =:#&#123;#user.email&#125;)",nativeQuery = true)UserOnlyName findByUser(@Param("user") User user); 然后，我们新增一个测试类，测试一下上面方法的结果。 12345678910@Testpublic void testQueryDinamicDto() &#123;userDtoRepository.save(User.builder().name("jack").email("123456@126.com").sex("man").address("shanghai").build()); UserOnlyName userDto = userDtoRepository.findByUser("jack", null); System.out.println(userDto.getName() + ":" + userDto.getEmail()); UserOnlyName userDto2 = userDtoRepository.findByUser(User.builder().email("123456@126.com").build()); System.out.println(userDto2.getName() + ":" + userDto2.getEmail());&#125; 最后，运行结果如下。 1234567891011121314151617181920Hibernate: insert into user (address, email, name, sex, version, id) values (?, ?, ?, ?, ?, ?) : binding parameter [1] as [VARCHAR] - [shanghai] : binding parameter [2] as [VARCHAR] - [123456@126.com] : binding parameter [3] as [VARCHAR] - [jack] : binding parameter [4] as [VARCHAR] - [man] : binding parameter [5] as [BIGINT] - [0] : binding parameter [6] as [BIGINT] - [1]Hibernate: select user0_.name as col_0_0_, user0_.email as col_1_0_ from user user0_ where (? is null or user0_.name=?) and (? is null or user0_.email=?) : binding parameter [1] as [VARCHAR] - [jack] : binding parameter [2] as [VARCHAR] - [jack] : binding parameter [3] as [VARCHAR] - [null] : binding parameter [4] as [VARCHAR] - [null]jack:123456@126.comHibernate: select u.name as name,u.email as email from user u where (? is null or u.name =?) and (? is null or u.email =?) : binding parameter [1] as [VARBINARY] - [null] : binding parameter [2] as [VARBINARY] - [null] : binding parameter [3] as [VARCHAR] - [123456@126.com] : binding parameter [4] as [VARCHAR] - [123456@126.com]jack:123456@126.com 注意：其中我们打印了一下 SQL 传入的参数，是为了让我们更清楚参数都传入了什么值。上面的两个方法，分别采用了 JPQL 的动态参数和 SPEL 的表达式方式获取参数 通过上面的实例可以看得出来，我们采用了 :email is null or s.email = :email 这种方式来实现动态查询的效果，实际工作中也可能演变得很复杂。 能用方法名表示的，尽量用方法名表示，因为这样语义清晰、简单快速，基本上只要编译通过，一定不会有问题； 能用 @Query 里面的 JPQL 表示的，就用 JPQL，这样与 SQL 无关，万一哪天换数据库了，基本上代码不用改变； 最后实在没有办法了，可以选择 nativeQuery 写原始 SQL，特别是一开始从 MyBatis 转过来的同学，选择写 SQL 会更容易一些。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>Defining Query Methods</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data Jpa DataSource 加载过程]]></title>
    <url>%2F2020%2F10%2F14%2FSpringDataJpa%E7%9A%84DataSource%E5%8A%A0%E8%BD%BD%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[DataSource 为何物？加载过程是怎样的？数据源是什么？当我们用第三方工具去连接数据库（Mysql，Oracle 等）的时候，一般都会让我们选择数据源，如下图所示： 以 MySQL 为例，当选择 MySQL 的时候就会弹出如下图显示的界面： 其中，我们在选择了 Driver（驱动）和 Host、User、Password 等之后，就可以创建一个 Connection，然后连接到数据库里面了。 同样的道理，在 Java 里面我们也需要用到 DataSource 去连接数据库，而 Java 定义了一套 JDBC 的协议标准，其中有一个 javax.sql.DataSource 接口类，通过实现此类就可以进行数据库连接，我们通过源码来分析一下。 DataSource 源码分析DataSource 接口里面主要的代码如下所示： 1234public interface DataSource extends CommonDataSource, Wrapper &#123; Connection getConnection() throws SQLException; Connection getConnection(String username, String password) throws SQLException;&#125; 我们通过源码可以很清楚地看到，DataSource 的主要目的就是获得数据库连接，就像我们前面用工具连接数据库一样，只不过工具是通过界面实现的，而 DataSource 是通过代码实现的。 那么在程序里面如何实现呢？也有很多第三方的实现方式，常见的有C3P0、BBCP、Proxool、Druid、Hikari，而目前 Spring Boot 里面是采用 Hikari 作为默认数据源。Hikari 的优点是：开源，社区活跃，性能高，监控完整。我们通过工具看一下项目里面DataSource 的实现类有哪些，如下图所示： 其中，当采用默认数据源的时候，可以看到数据源的实现类有： h2 里面的 JdbcDataSource MySQL 连接里面的 MysqlDataSource HikariDataSource（默认数据源，也是 Spring 社区推荐的最佳数据源） 直接打开 HikariDataSource 的源码看一下，它的关键代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class HikariDataSource extends HikariConfig implements DataSource, Closeable&#123; private volatile HikariPool pool; public HikariDataSource(HikariConfig configuration) &#123; configuration.validate(); configuration.copyStateTo(this); LOGGER.info("&#123;&#125; - Starting...", configuration.getPoolName()); pool = fastPathPool = new HikariPool(this); LOGGER.info("&#123;&#125; - Start completed.", configuration.getPoolName()); this.seal(); &#125; //这个是最主要的实现逻辑，即通过连接池获得连接的逻辑 public Connection getConnection() throws SQLException&#123; if (isClosed()) &#123; throw new SQLException("HikariDataSource " + this + " has been closed."); &#125; if (fastPathPool != null) &#123; return fastPathPool.getConnection(); &#125; // See http://en.wikipedia.org/wiki/Double-checked_locking#Usage_in_Java HikariPool result = pool; if (result == null) &#123; synchronized (this) &#123; result = pool; if (result == null) &#123; validate(); LOGGER.info("&#123;&#125; - Starting...", getPoolName()); try &#123; pool = result = new HikariPool(this); this.seal(); &#125; catch (PoolInitializationException pie) &#123; if (pie.getCause() instanceof SQLException) &#123; throw (SQLException) pie.getCause(); &#125; else &#123; throw pie; &#125; &#125; LOGGER.info("&#123;&#125; - Start completed.", getPoolName()); &#125; &#125; &#125; return result.getConnection(); &#125;......&#125; 从上面的源码可以看到关键的两点问题： 数据源的关键配置属性有哪些？ 连接怎么获得？连接池的作用如何？ Hikari 数据源的关键配置属性有哪些？第一个问题，HikariConfig 的配置里面描述了 Hikari 数据源主要的配置属性，我们打开来看一下，如图所示： 通过上面的源码我们可以看到数据源的关键配置信息：用户名、密码、连接池的配置、jdbcUrl、驱动的名字等等 Hikari 数据源的连接怎么获得上面提到的第 2 个问题，我们通过 getConnection 方法里面的代码可以看到 HikariPool 的用法，也就是说，是通过连接池来获得连接的，这个连接用过之后没有断开，而是重新放回到连接池里面（这个地方你一定要谨记，它也说明了 connection 是可以共享的）。 连接池的用途： 创建连接是非常昂贵的，连接池技术可以共享现有的连接，以增加代码的执行效率。 数据源、驱动、连接、连接池的关系 数据源（DataSource）的作用是给应用程序提供不同 DB 的连接 connection； 连接（Connection）是通过连接池（Pool）获取的，这主要是出于连接性能的考虑； 创建好连接之后，通过数据库的驱动（Driver）来进行数据库操作； 而不同的 DB（MySQL / H2 / Oracle），都有自己的驱动类和相应的驱动 Jar 包。 用一个图来表示一下： 而我们常说的 MySQL 驱动，其实就是 com.mysql.cj.jdbc.Driver，而这个类主要存在于 mysql-connection-java:8.0* 的 jar 里面，也就是我们经常说的不同的数据库所代表的驱动 jar 包。 这里我们用的是 spring boot 2.3.3 版本引用的 mysql-connection-java 8.0 版本驱动 jar 包，不同的数据库引用的 jar 包是不一样的。例如，H2 数据源中，我们用的驱动类是 org.h2.Driver，其包含在 com.h2database:h2:1.4.*jar 包里面。 数据源的加载原理和过程是什么样的？ spring.factories 文件可以看到 JDBC 数据源相关的自动加载的类 DataSourceAutoConfiguration，那么就从这个类开始分析。 DataSourceAutoConfiguration 数据源的加载过程分析DataSourceAutoConfiguration 的关键源码如下所示： 12345678910111213141516171819202122//将spring.datasource.**的配置放到DataSourceProperties对象里面；@EnableConfigurationProperties(DataSourceProperties.class)@Import(&#123; DataSourcePoolMetadataProvidersConfiguration.class, DataSourceInitializationConfiguration.class &#125;)public class DataSourceAutoConfiguration &#123; //默认集成的数据源，一般指的是H2，方便我们快速启动和上手，一般不在生产环境应用； @Configuration(proxyBeanMethods = false) @Conditional(EmbeddedDatabaseCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;) @Import(EmbeddedDataSourceConfiguration.class) protected static class EmbeddedDatabaseConfiguration &#123; &#125; //加载不同的数据源的配置 @Configuration(proxyBeanMethods = false) @Conditional(PooledDataSourceCondition.class) @ConditionalOnMissingBean(&#123; DataSource.class, XADataSource.class &#125;) @Import(&#123; DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class &#125;) protected static class PooledDataSourceConfiguration &#123; &#125; ....&#125; 从源码中我们可以得到以下三点最关键的信息：第一，通过 @EnableConfigurationProperties(DataSourceProperties.class) 可以看得出来 spring.datasource 的配置项有哪些 DataSourceProperties 关键代码如下： 123456789101112131415161718192021222324252627282930@ConfigurationProperties(prefix = "spring.datasource")public class DataSourceProperties implements BeanClassLoaderAware, InitializingBean &#123; private ClassLoader classLoader; private String name; private boolean generateUniqueName = true; private Class&lt;? extends DataSource&gt; type; private String driverClassName; private String url; private String username; private String password; //计算确定 driverName 的值是什么 public String determineDriverClassName() &#123; if (StringUtils.hasText(this.driverClassName)) &#123; Assert.state(driverClassIsLoadable(), () -&gt; "Cannot load driver class: " + this.driverClassName); return this.driverClassName; &#125; String driverClassName = null; //此段逻辑是，当我们没有配置自己的 driverName 的时候，它会根据我们配置的 DB 的 url自动计算出来 driverName 的值是什么，所以就会发现我们现在很多 datasource 里面的配置都省去了 driver-name 的配置，这是 Spring Boot 的功劳。 if (StringUtils.hasText(this.url)) &#123; driverClassName = DatabaseDriver.fromJdbcUrl(this.url).getDriverClassName(); &#125; if (!StringUtils.hasText(driverClassName)) &#123; driverClassName = this.embeddedDatabaseConnection.getDriverClassName(); &#125; if (!StringUtils.hasText(driverClassName)) &#123; throw new DataSourceBeanCreationException("Failed to determine a suitable driver class", this, this.embeddedDatabaseConnection); &#125; return driverClassName;&#125; 我们通过 DatabaseDriver 的源码可以看到 MySQL 的默认驱动 Spring Boot 是采用 com.mysql.cj.jdbc.Driver 来实现的。 同时，@ConfigurationProperties(prefix = &quot;spring.datasource&quot;) 也告诉我们，application.properties 里面的 datasource 相关的公共配置可以以 spring.datasource 为开头，这样当启动的时候，DataSourceProperties 就会将 datasource 的一切配置自动加载进来。正如我们前面在 application.properties 里面的配置的一样，如下图所示： 这里有 url、username、password、driver-class-name 等关键配置，不同数据源的公共配置也不多。 第二，我们通过下面这一段代码也可以看得出来不同的数据源的配置是什么样的。 123456@Import(&#123; DataSourceConfiguration.Hikari.class, DataSourceConfiguration.Tomcat.class, DataSourceConfiguration.Dbcp2.class, DataSourceConfiguration.Generic.class, DataSourceJmxConfiguration.class &#125;) 打开 DataSourceConfiguration 的源码，如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162abstract class DataSourceConfiguration &#123; @SuppressWarnings("unchecked") protected static &lt;T&gt; T createDataSource(DataSourceProperties properties, Class&lt;? extends DataSource&gt; type) &#123; return (T) properties.initializeDataSourceBuilder().type(type).build(); &#125; /** * Tomcat连接池数据源的配置，前提条件需要引入tomcat-jdbc*.jar */ @Configuration(proxyBeanMethods = false) @ConditionalOnClass(org.apache.tomcat.jdbc.pool.DataSource.class) @ConditionalOnMissingBean(DataSource.class) @ConditionalOnProperty(name = "spring.datasource.type", havingValue = "org.apache.tomcat.jdbc.pool.DataSource", matchIfMissing = true) static class Tomcat &#123; @Bean @ConfigurationProperties(prefix = "spring.datasource.tomcat") org.apache.tomcat.jdbc.pool.DataSource dataSource(DataSourceProperties properties) &#123; org.apache.tomcat.jdbc.pool.DataSource dataSource = createDataSource(properties, org.apache.tomcat.jdbc.pool.DataSource.class); DatabaseDriver databaseDriver = DatabaseDriver.fromJdbcUrl(properties.determineUrl()); String validationQuery = databaseDriver.getValidationQuery(); if (validationQuery != null) &#123; dataSource.setTestOnBorrow(true); dataSource.setValidationQuery(validationQuery); &#125; return dataSource; &#125; &#125; /** * Hikari数据源的配置，默认Spring Boot加载的是Hikari数据源 */ @Configuration(proxyBeanMethods = false) @ConditionalOnClass(HikariDataSource.class) @ConditionalOnMissingBean(DataSource.class) @ConditionalOnProperty(name = "spring.datasource.type", havingValue = "com.zaxxer.hikari.HikariDataSource", matchIfMissing = true) static class Hikari &#123; @Bean @ConfigurationProperties(prefix = "spring.datasource.hikari") HikariDataSource dataSource(DataSourceProperties properties) &#123; HikariDataSource dataSource = createDataSource(properties, HikariDataSource.class); if (StringUtils.hasText(properties.getName())) &#123; dataSource.setPoolName(properties.getName()); &#125; return dataSource; &#125; &#125; /** * DBCP数据源的配置，按照Spring Boot的语法，我们必须引入CommonsDbcp**.jar依赖才有用 */ @Configuration(proxyBeanMethods = false) @ConditionalOnClass(org.apache.commons.dbcp2.BasicDataSource.class) @ConditionalOnMissingBean(DataSource.class) @ConditionalOnProperty(name = "spring.datasource.type", havingValue = "org.apache.commons.dbcp2.BasicDataSource", matchIfMissing = true) static class Dbcp2 &#123; @Bean @ConfigurationProperties(prefix = "spring.datasource.dbcp2") org.apache.commons.dbcp2.BasicDataSource dataSource(DataSourceProperties properties) &#123; return createDataSource(properties, org.apache.commons.dbcp2.BasicDataSource.class); &#125; &#125; 通过上述源码可以看到最常见的三种数据源的配置： HikariDataSource tomcat的JDBC apache的dbcp 而最终用哪个，就看你引用了哪个 dataSource 的 jar 包。不过 Spring Boot 2.0 之后就推荐使用 Hikari 数据源了。 第三，通过 @ConfigurationProperties(prefix = &quot;spring.datasource.hikari&quot;) 和 HikariDataSource dataSource(DataSourceProperties properties) 可以知道，application.properties 里面 spring.datasource.hikari 开头的配置会被映射到 HikariDataSource 对象中， HikariDataSource 继承了 HikariConfig。 所以顺理成章地，就可以知道 Hikari 数据源的配置有哪些了，如下图所示： Hikari 的配置比较多，你实际工作中想要了解详细配置，可以看一下 这里只说一下最需要关心的配置，有如下几个： 123456789101112131415## 最小空闲链接数量spring.datasource.hikari.minimum-idle=5## 空闲链接存活最大时间，默认600000（10分钟）spring.datasource.hikari.idle-timeout=180000## 链接池最大链接数，默认是10spring.datasource.hikari.maximum-pool-size=10## 此属性控制从池返回的链接的默认自动提交行为,默认值：truespring.datasource.hikari.auto-commit=true## 数据源链接池的名称spring.datasource.hikari.pool-name=MyHikariCP## 此属性控制池中链接的最长生命周期，值0表示无限生命周期，默认1800000即30分钟spring.datasource.hikari.max-lifetime=1800000## 数据库链接超时时间,默认30秒，即30000spring.datasource.hikari.connection-timeout=30000spring.datasource.hikari.connection-test-query=SELECT 1mysql 这里介绍的主要是针对连接池的配置，连接池我们不能配置得太大，因为连接池太大的话，会有额外的 CPU 开销，处理连接池的线程切换反而会增加程序的执行时间，减低性能；相应的，连接池也不能配置太小，太小的话可能会增加请求的等待时间，也会降低业务处理的吞吐量。 Hikari 数据源下的 MySQL 配置最佳实践1234567891011121314151617181920##数据源的配置：logger=Slf4JLogger&amp;profileSQL=true是用来debug显示sql的执行日志的spring.datasource.url=jdbc:mysql://localhost:3306/test?logger=Slf4JLogger&amp;profileSQL=truespring.datasource.username=rootspring.datasource.password=E6kroWaR9F##采用默认的#spring.datasource.hikari.connectionTimeout=30000#spring.datasource.hikari.idleTimeout=300000##指定一个链接池的名字，方便我们分析线程问题spring.datasource.hikari.pool-name=jpa-hikari-pool##最长生命周期15分钟够了spring.datasource.hikari.maxLifetime=900000spring.datasource.hikari.maximumPoolSize=8##最大和最小相对应减少创建线程池的消耗；spring.datasource.hikari.minimumIdle=8spring.datasource.hikari.connectionTestQuery=select 1 from dual##当释放连接到连接池之后，采用默认的自动提交事务spring.datasource.hikari.autoCommit=true##用来显示链接测trace日志logging.level.com.zaxxer.hikari.HikariConfig=DEBUG logging.level.com.zaxxer.hikari=TRACE 通过上面的日志配置，我们在启动的时候可以看到连接池的配置结果和 MySQL 的执行日志： 如下日志，显示了 Hikari 的 config 配置。 当我们执行一个方法的时候，到底要在一个 MySQL 的 connection 上面执行哪些 SQL 呢？通过如下日志我们可以看得出来。 通过开启 com.zaxxer.hikari.pool.HikariPool 类的 debug 级别，可以实时看到连接池的使用情况：软件日志如下（上图也有体现）： 1com.zaxxer.hikari.pool.HikariPool : jpa-hikari-pool - Pool stats (total=8, active=1, idle=7, waiting=0) 通过上面的监控日志，你在实际工作中可以根据主机的 CPU 情况和业务处理的耗时情况，再对连接池做适当的调整，但是注意差距不要太大，不要一下将连接池配置几百个，那是错误的配置。 Hikari 数据通过 Prometheus 的监控指标应用就像日志里面打印的一样 1com.zaxxer.hikari.pool.HikariPool : jpa-hikari-pool - Pool stats (total=8, active=0, idle=8, waiting=0) Hikari 的 Metric 也帮我们提供了 Prometheus 的监控指标，实现方法很简单，代码如下所示： gradle 依赖里面添加 1implementation &apos;io.micrometer:micrometer-registry-prometheus&apos; application.properties 里面添加 12345#Metrics related configurationsmanagement.endpoint.metrics.enabled=truemanagement.endpoints.web.exposure.include=*management.endpoint.prometheus.enabled=truemanagement.metrics.export.prometheus.enabled=true 然后我们启动项目，通过下图中的地址就可以看到，Prometheus 的 Metrics 里面多了很多 HikariCP 的指标。 当看到这些指标之后，就可以根据 Grafana 社区里面提供的 HikariCP 的监控 Dashboards 的配置文档地址，导入到我们自己的 Grafana 里面，可以通过图表看到如下界面： 通过这种标准的模板就可以知道 JDBC 的连接情况、Hikari 的连接情况，以及每个连接请求时间、使用时间。这样对诊断 DB 性能问题非常有帮助。 下面对其中一些关键指标作一下说明： totalConnections：总连接数，包括空闲的连接和使用中的连接，即 totalConnections = activeConnection + idleConnections； idleConnections：空闲连接数，也叫可用连接数，也就是连接池里面现成的 DB 连接数； activeConnections：活跃连接数，非业务繁忙期一般都是 0，很快就会释放到连接池里面去； pendingThreads：正在等待连接的线程数量。排查性能问题时，这个指标是一个重要的参考指标，如果正在等待连接的线程在相当长一段时间内数量较多，说明我们的连接没有利用好，是不是占用连接的时间过长了？一旦有 pendingThreads 的数量了可以发个告警，查查原因，或者优化一下连接池； maxConnections：最大连接数，统计指标，统计到目前为止连接的最大数量。 minConnections：最小连接数，统计指标，统计到目前为止连接的最小数量。 usageTime：每个连接使用的时间，当连接被回收的时候会记录此指标；一般都在 m、s 级别，一旦到 s 级别了可以发个告警； acquireTime：获取每个连接需要等待时间，一个请求获取数据库连接后或者因为超时失败后，会记录此指标。 connectionCreateTime：连接创建时间。 在 Grafana 图表或者 Prometheus 里面都可以配置一些邮件或者短信等告警，这样当我们 DB 连接池发生问题的时候就能实时知道。 感兴趣可以研究一下 Prometheus Operator。 AliDruidDataSource 的配置与介绍在实际工作中，由于 HikariCP 和 Druid 各有千秋，国内的很多开发者都使用 AliDruid 作为数据源。 第一步：引入 Gradle 依赖1implementation &apos;com.alibaba:druid-spring-boot-starter:1.2.1&apos; 第二步：配置数据源1234spring.datasource.druid.url= # 或spring.datasource.url= spring.datasource.druid.username= # 或spring.datasource.username=spring.datasource.druid.password= # 或spring.datasource.password=spring.datasource.druid.driver-class-name= #或 spring.datasource.driver-class-name= 第三步：配置连接池1234567891011121314151617spring.datasource.druid.initial-size=spring.datasource.druid.max-active=spring.datasource.druid.min-idle=spring.datasource.druid.max-wait=spring.datasource.druid.pool-prepared-statements=spring.datasource.druid.max-pool-prepared-statement-per-connection-size= spring.datasource.druid.max-open-prepared-statements= #和上面的等价spring.datasource.druid.validation-query=spring.datasource.druid.validation-query-timeout=spring.datasource.druid.test-on-borrow=spring.datasource.druid.test-on-return=spring.datasource.druid.test-while-idle=spring.datasource.druid.time-between-eviction-runs-millis=spring.datasource.druid.min-evictable-idle-time-millis=spring.datasource.druid.max-evictable-idle-time-millis=spring.datasource.druid.filters= #配置多个英文逗号分隔....//more 通过以上三步就可以完成 Druid 数据源的配置了，需要注意的是，我们需要把 HikariCP 数据源给排除掉，而其他 Druid 的配置，比如监控，参考官方文档。 其官方的源码也比较简单，按照上面分析 HikariCP 数据源的方法，可以找一下 aliDruid 的源码，其加载的入口类，一步一步去查看即可。 Naming 命名策略详解及其实践在配置 @Entity 时，一定会好奇表名、字段名、外键名、实体字段、@Column 和数据库的字段之间，映射关系是怎么样的？默认规则映射规则又是什么？如果和默认不一样该怎么扩展？ Hibernate 5 的命名策略Hibernate 5 里面把实体和数据库的字段名和表名的映射分成了两个步骤 第一步：通过ImplicitNamingStrategy先找到实例里面定义的逻辑的字段名字。 这是通过ImplicitNamingStrategy 的实现类指定逻辑字段查找策略，也就是当实体里面定义了 @Table、@Column 注解的时候，以注解指定名字返回；而当没有这些注解的时候，返回的是实体里面的字段的名字。 其中，org.hibernate.boot.model.naming.ImplicitNamingStrategy 是一个接口，ImplicitNamingStrategyJpaCompliantImpl 这个实现类兼容 JPA 2.0 的字段映射规范。除此之外，还有如下四个实现类： ImplicitNamingStrategyLegacyHbmImpl：兼容 Hibernate 老版本中的命名规范； ImplicitNamingStrategyLegacyJpaImpl：兼容 JPA 1.0 规范中的命名规范； ImplicitNamingStrategyComponentPathImpl：@Embedded 等注解标志的组件处理是通过 attributePath 完成的，因此如果我们在使用 @Embedded 注解的时候，如果要指定命名规范，可以直接继承这个类来实现； SpringImplicitNamingStrategy：默认的 spring data 2.2.3 的策略，只是扩展了 ImplicitNamingStrategyJpaCompliantImpl 里面的 JoinTableName 的方法，如下图所示： 这里只需要关心 SpringImplicitNamingStrategy 就可以了，其他的基本上用不到。 SpringImplicitNamingStrategy 效果如何呢？代码如下： 1234567891011@Entity@Table(name = "userInfo")public class UserInfo extends BaseEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private Integer ages; private String lastName; @Column(name = "myAddress") private String emailAddress;&#125; 通过第一步可以得到如下逻辑字段的映射结果： 12345UserInfo -&gt; userInfoid-&gt;idages-&gt;ageslastName -&gt; lastNameemailAddress -&gt; myAddress 第二步：通过 PhysicalNamingStrategy 将逻辑字段转化成数据库的物理字段名字。 它的实现类负责将逻辑字段转化成带下划线，或者统一给字段加上前缀，又或者加上双引号等格式的数据库字段名字，其主要的接口是：org.hibernate.boot.model.naming.PhysicalNamingStrategy，而它的实现类也只有两个，如下图所示： PhysicalNamingStrategyStandardImpl：这个类什么都没干，即直接将第一个步骤得到的逻辑字段名字当成数据库的字段名字使用。这个主要的应用场景是，如果某些字段的命名格式不是下划线的格式，我们想通过 @Column 的方式显示声明的话，可以把默认第二步的策略改成 PhysicalNamingStrategyStandardImpl。那么如果再套用第一步的例子，经过这个类的转化会变成如下形式： 12345userInfo -&gt; userInfoid-&gt;idages-&gt;ageslastName -&gt; lastNamemyAddress -&gt; myAddress 可以看出来逻辑名字到物理名字是保持不变的。 SpringPhysicalNamingStrategy：这个类是将第一步得到的逻辑字段名字的大写字母前面加上下划线，并且全部转化成小写，将会标识出是否需要加上双引号。此种是默认策略。我们举个例子，第一步得到的逻辑字段就会变成如下映射： 12345userInfo -&gt; user_infoid-&gt;idages-&gt;ageslastName -&gt; last_namemyAddress -&gt; my_address 把刚才的实体执行一下，可以看到生成的表的结构如下： 1Hibernate: create table user_info (id bigint not null, create_time timestamp, create_user_id integer, last_modified_time timestamp, last_modified_user_id integer, version integer, ages integer, my_address varchar(255), last_name varchar(255), telephone varchar(255), primary key (id))； 也可以通过在 SpringPhysicalNamingStrategy 类里面设置断点来验证，如下图所示： 加载原理与自定义方法修改默认策略，只需要在 application.properties 里面修改下面代码所示的两个配置，换成自己的自定义的类即可。 12spring.jpa.hibernate.naming.implicit-strategy=org.springframework.boot.orm.jpa.hibernate.SpringImplicitNamingStrategyspring.jpa.hibernate.naming.physical-strategy=org.springframework.boot.orm.jpa.hibernate.SpringPhysicalNamingStrategy 如果直接搜索：spring.jpa.hibernate 就会发现，其默认配置是在 org.springframework.boot.autoconfigure.orm.jpa.HibernateProerties 这类里面的，如下图所示的方法中进行加载。 其中，IMPLICIT_NAMING_STRATEGY 和 PHYSICAL_NAMING_STRATEGY 的值如下述代码所示，它是 Hibernate 5 的配置变量，用来改变 Hibernate的 naming 的策略。 12String IMPLICIT_NAMING_STRATEGY = &quot;hibernate.implicit_naming_strategy&quot;;String PHYSICAL_NAMING_STRATEGY = &quot;hibernate.physical_naming_strategy&quot;; 如果自定义的话，直接继承 SpringPhysicalNamingStrategy 这个类，然后覆盖需要实现的方法即可。那么它实际的应用场景都有哪些呢？ 自定义的实际应用场景有时候我们接触到的系统可能是老系统，表和字段的命名规范不一定是下划线形式，有可能驼峰式的命名法，也有可能不同的业务有不同的表名前缀。不管是哪一种，都可以通过修改第二阶段：物理映射的策略，改成 PhysicalNamingStrategyStandardImpl 的形式 1spring.jpa.hibernate.naming.physical-strategy=org.hibernate.boot.model.naming.PhysicalNamingStrategyStandardImpl 这样可以使 @Column/@Table 等注解的自定义值生效，或者改成自定义的 MyPhysicalNamingStrategy。不过不建议修改 implicit-strategy，因为没有必要，需只要在 physical-strategy 上做文章就足够了。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>DataSource</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[缓存的原理、引入与设计]]></title>
    <url>%2F2020%2F10%2F14%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98-%E7%BC%93%E5%AD%98%E7%9A%84%E5%8E%9F%E7%90%86%E3%80%81%E5%BC%95%E5%85%A5%E4%B8%8E%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[缓存的原理、引入与设计业务数据访问性能太低怎么办 缓存的基本思想 缓存的优点 缓存的代价 缓存的定义 缓存最初的含义，是指用于加速 CPU 数据交换的 RAM，即随机存取存储器，通常这种存储器使用更昂贵但快速的静态 RAM（SRAM）技术，用以对 DRAM 进行加速。这是一个狭义缓存的定义。 而广义缓存的定义则更宽泛，任何可以用于数据高速交换的存储介质都是缓存，可以是硬件也可以是软件。 缓存存在的意义就是通过开辟一个新的数据交换缓冲区，来解决原始数据获取代价太大的问题，让数据得到更快的访问。目前讲的缓存主要是指广义缓存，特别是互联网产品大量使用的各种缓存组件和技术。 缓存原理缓存的基本思想 缓存构建的基本思想是利用时间局限性原理，通过空间换时间来达到加速数据获取的目的，同时由于缓存空间的成本较高，在实际设计架构中还要考虑访问延迟和成本的权衡问题。这里面有 3 个关键点。 时间局限性原理，即被获取过一次的数据在未来会被多次引用，比如一条微博被一个人感兴趣并阅读后，它大概率还会被更多人阅读，当然如果变成热门微博后，会被数以百万/千万计算的更多用户查看。 以空间换时间，因为原始数据获取太慢，所以我们开辟一块高速独立空间，提供高效访问，来达到数据获取加速的目的。 性能成本 Tradeoff，构建系统时希望系统的访问性能越高越好，访问延迟越低小越好。但维持相同数据规模的存储及访问，性能越高延迟越小，成本也会越高，所以在系统架构设计时，你需要在系统性能和开发运行成本之间做取舍。比如左边这张图，相同成本的容量，SSD 硬盘容量会比内存大 10～30 倍以上，但读写延迟却高 50～100 倍。 缓存的优势缓存的优势主要有以下几点： 提升访问性能 降低网络拥堵 减轻服务负载 增强可扩展性 缓存存储原始数据，可以大幅提升访问性能。不过在实际业务场景中，缓存中存储的往往是需要频繁访问的中间数据甚至最终结果，这些数据相比 DB 中的原始数据小很多，这样就可以减少网络流量，降低网络拥堵，同时由于减少了解析和计算，调用方和存储服务的负载也可以大幅降低。缓存的读写性能很高，预热快，在数据访问存在性能瓶颈或遇到突发流量，系统读写压力大增时，可以快速部署上线，同时在流量稳定后，也可以随时下线，从而使系统的可扩展性大大增强。 缓存的代价然而不幸的是，任何事情都有两面性，缓存也不例外，我们在享受缓存带来一系列好处的同时，也注定需要付出一定的代价。 首先，服务系统中引入缓存，会增加系统的复杂度。 其次，由于缓存相比原始 DB 存储的成本更高，所以系统部署及运行的费用也会更高。 最后，由于一份数据同时存在缓存和 DB 中，甚至缓存内部也会有多个数据副本，多份数据就会存在一致性问题，同时缓存体系本身也会存在可用性问题和分区的问题。这就需要我们加强对缓存原理、缓存组件以及优秀缓存体系实践的理解，从系统架构之初就对缓存进行良好设计，降低缓存引入的副作用，让缓存体系成为服务系统高效稳定运行的强力基石。 一般来讲，服务系统的全量原始数据存储在 DB 中（如 MySQL、HBase 等），所有数据的读写都可以通过 DB 操作来获取。但 DB 读写性能低、延迟高，如 MySQL 单实例的读写 QPS 通常只有千级别（3000～6000），读写平均耗时 10～100ms 级别，如果一个用户请求需要查 20 个不同的数据来聚合，仅仅 DB 请求就需要数百毫秒甚至数秒。而 cache 的读写性能正好可以弥补 DB 的不足，比如 Memcached 的读写 QPS 可以达到 10～100万 级别，读写平均耗时在 1ms 以下，结合并发访问技术，单个请求即便查上百条数据，也可以轻松应对。 但 cache 容量小，只能存储部分访问频繁的热数据，同时，同一份数据可能同时存在 cache 和 DB，如果处理不当，就会出现数据不一致的问题。所以服务系统在处理业务请求时，需要对 cache 的读写方式进行适当设计，既要保证数据高效返回，又要尽量避免数据不一致等各种问题。 如何根据业务来选择缓存模式和组件 缓存的读写模式 缓存的分类 缓存读写模式如下图，业务系统读写缓存有 3 种模式： Cache Aside（旁路缓存） Read/Write Through（读写穿透） Write Behind Caching（异步缓存写入） Cache Aside 如上图所示，Cache Aside 模式中，业务应用方 对于写，是更新 DB 后，直接将 key 从 cache 中删除，由 DB 驱动缓存数据的更新 对于读，是先读 cache，如果 cache miss，则读 DB，同时将数据回写到 cache 这种模式的特点是，业务端处理所有数据访问细节，同时利用 Lazy 计算的思想，更新 DB 后，直接删除 cache 并通过 DB 更新，确保数据以 DB 结果为准，则可以大幅降低 cache 和 DB 中数据不一致的概率。 如果没有专门的存储服务，同时是对数据一致性要求比较高的业务，或者是缓存数据更新比较复杂的业务，这些情况都比较适合使用 Cache Aside 模式。如微博发展初期，不少业务采用这种模式，这些缓存数据需要通过多个原始数据进行计算后设置。在部分数据变更后，直接删除缓存。同时，使用一个 Trigger 组件，实时读取 DB 的变更日志，然后重新计算并更新缓存。如果读缓存的时候，Trigger 还没写入 cache，则由调用方自行到 DB 加载计算并写入 cache。 感觉直接用@Cacheable和@CachePut注解就基本达到这个效果 Read/Write Through 如上图，对于 Cache Aside 模式，业务应用需要同时维护 cache 和 DB 两个数据存储方，过于繁琐，于是就有了 Read/Write Through 模式。 在这种模式下，业务应用只关注一个存储服务即可，业务方的读写 cache 和 DB 的操作，都由存储服务代理。存储服务收到业务应用的写请求时，会首先查 cache，如果数据在 cache 中不存在，则只更新 DB，如果数据在 cache 中存在，则先更新 cache，然后更新 DB。而存储服务收到读请求时，如果命中 cache 直接返回，否则先从 DB 加载，回种到 cache 后返回响应。( cache 为准) 这种模式的特点是，存储服务封装了所有的数据处理细节，业务应用端代码只用关注业务逻辑本身，系统的隔离性更佳。另外，进行写操作时，如果 cache 中没有数据则不更新，有缓存数据才更新，内存效率更高。 微博 Feed 的 Outbox Vector（即用户最新微博列表）就采用这种模式。一些粉丝较少且不活跃的用户发表微博后，Vector 服务会首先查询 Vector Cache，如果 cache 中没有该用户的 Outbox 记录，则不写该用户的 cache 数据，直接更新 DB 后就返回，只有 cache 中存在才会通过 CAS 指令进行更新。 Write Behind Caching Write Behind Caching 模式与 Read/Write Through 模式类似，也由数据存储服务来管理 cache 和 DB 的读写。不同点是，数据更新时，Read/write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是改为异步批量的方式来更新 DB。该模式的特点是，数据存储的写性能最高，非常适合一些变更特别频繁的业务，特别是可以合并写请求的业务，比如对一些计数业务，一条 Feed 被点赞 1万 次，如果更新 1万 次 DB 代价很大，而合并成一次请求直接加 1万，则是一个非常轻量的操作。但这种模型有个显著的缺点，即数据的一致性变差，甚至在一些极端场景下可能会丢失数据。比如系统 Crash、机器宕机时，如果有数据还没保存到 DB，则会存在丢失的风险。 📌这种读写模式适合变更频率特别高，但对一致性要求不太高的业务，这样写操作可以异步批量写入 DB，减小 DB 压力。 三种模式各有优劣，不存在最佳模式。 实际上，我们也不可能设计出一个最佳的完美模式出来，如同前面讲到的空间换时间、访问延迟换低成本一样，高性能和强一致性从来都是有冲突的，系统设计从来就是取舍，随处需要 trade-off。 重要的是思想：即如何根据业务场景，更好的做 trade-off，从而设计出更好的服务系统。 缓存分类及常用缓存介绍按宿主层次分类分为本地 Cache、进程间 Cache 和远程 Cache。 本地 Cache：是指业务进程内的缓存，这类缓存由于在业务系统进程内，所以读写性能超高且无任何网络开销，但不足是会随着业务系统重启而丢失。 进程间 Cache：是本机独立运行的缓存，这类缓存读写性能较高，不会随着业务系统重启丢数据，并且可以大幅减少网络开销，但不足是业务系统和缓存都在相同宿主机，运维复杂，且存在资源竞争。( cache 和 project 都在同一个 Linux 部署 ) 远程 Cache：是指跨机器部署的缓存，这类缓存因为独立设备部署，容量大且易扩展，在互联网企业使用最广泛。不过远程缓存需要跨机访问，在高读写压力下，带宽容易成为瓶颈。 本地 Cache 的缓存组件有 Ehcache、Guava Cache 等，开发者自己也可以用 Map、Set 等轻松构建一个自己专用的本地 Cache。 进程间 Cache 和远程 Cache 的缓存组件相同，只是部署位置的差异罢了，这类缓存组件有 Memcached、Redis、Pika 等。 按存储介质分类分为内存型缓存和持久化型缓存。 内存型缓存将数据存储在内存，读写性能很高，但缓存系统重启或 Crash 后，内存数据会丢失。 持久化型缓存将数据存储到 SSD/Fusion-IO 硬盘中，相同成本下，这种缓存的容量会比内存型缓存大 1 个数量级以上，而且数据会持久化落地，重启不丢失，但读写性能相对低 1～2 个数量级。Memcached 是典型的内存型缓存，而 Pika 以及其他基于 RocksDB 开发的缓存组件等则属于持久化型缓存。 设计缓存架构时需要考量哪些因素缓存的引入及架构设计缓存组件选择在设计架构缓存时，首先要选定缓存组件，比如要用 Local-Cache，还是 Redis、Memcached、Pika 等开源缓存组件，如果业务缓存需求比较特殊，还要考虑是直接定制开发一个新的缓存组件，还是对开源缓存进行二次开发，来满足业务需要。 缓存数据结构设计确定好缓存组件后，还要根据业务访问的特点，进行缓存数据结构的设计。 对于直接简单 KV 读写的业务，你可以将这些业务数据封装为 String、Json、Protocol Buffer 等格式，序列化成字节序列，然后直接写入缓存中。读取时，先从缓存组件获取到数据的字节序列，再进行反序列化操作即可。对于只需要存取部分字段或需要在缓存端进行计算的业务，可以把数据设计为 Hash、Set、List、Geo 等结构，存储到支持复杂集合数据类型的缓存中，如 Redis、Pika 等。 缓存分布设计确定了缓存组件，设计好了缓存数据结构，接下来就要设计缓存的分布。可以从 3 个维度来进行缓存分布设计。 首先，要选择分布式算法，是采用取模还是一致性 Hash 进行分布。取模分布的方案简单，每个 key 只会存在确定的缓存节点，一致性 Hash 分布的方案相对复杂，一个 key 对应的缓存节点不确定。但一致性 Hash 分布，可以在部分缓存节点异常时，将失效节点的数据访问均衡分散到其他正常存活的节点，从而更好地保证了缓存系统的稳定性。 其次，分布读写访问如何进行实施，是由缓存 Client 直接进行 Hash 分布定位读写，还是通过 Proxy 代理来进行读写路由？Client 直接读写，读写性能最佳，但需要 Client 感知分布策略。在缓存部署发生在线变化时，也需要及时通知所有缓存 Client，避免读写异常，另外，Client 实现也较复杂。而通过 Proxy 路由，Client 只需直接访问 Proxy，分布逻辑及部署变更都由 Proxy 来处理，对业务应用开发最友好，但业务访问多一跳，访问性能会有一定的损失。 最后，缓存系统运行过程中，如果待缓存的数据量增长过快，会导致大量缓存数据被剔除，缓存命中率会下降，数据访问性能会随之降低，这样就需要将数据从缓存节点进行动态拆分，把部分数据水平迁移到其他缓存节点。这个迁移过程需要考虑，是由 Proxy 进行迁移还是缓存 Server 自身进行迁移，甚至根本就不支持迁移。对于 Memcached，一般不支持迁移，对 Redis，社区版本是依靠缓存 Server 进行迁移，而对 Codis 则是通过 Admin、Proxy 配合后端缓存组件进行迁移。 缓存架构部署及运维管理设计完毕缓存的分布策略后，接下来就要考虑缓存的架构部署及运维管理了。架构部署主要考虑如何对缓存进行分池、分层、分 IDC，以及是否需要进行异构处理。 核心的、高并发访问的不同数据，需要分别分拆到独立的缓存池中，进行分别访问，避免相互影响；访问量较小、非核心的业务数据，则可以混存。 对海量数据、访问超过 10～100万 级的业务数据，要考虑分层访问，并且要分摊访问量，避免缓存过载。 如果业务系统需要多 IDC 部署甚至异地多活，则需要对缓存体系也进行多 IDC 部署，要考虑如何跨 IDC 对缓存数据进行更新，可以采用直接跨 IDC 读写，也可以采用 DataBus 配合队列机进行不同 IDC 的消息同步，然后由消息处理机进行缓存更新，还可以由各个 IDC 的 DB Trigger 进行缓存更新。 某些极端场景下，还需要把多种缓存组件进行组合使用，通过缓存异构达到最佳读写性能。 站在系统层面，要想更好得管理缓存，还要考虑缓存的服务化，考虑缓存体系如何更好得进行集群管理、监控运维等。 缓存设计架构的常见考量点在缓存设计架构的过程中，有一些非常重要的考量点，如下图所示，只有分析清楚了这些考量点，才能设计架构出更佳的缓存体系。 读写方式首先是 value 的读写方式。是全部整体读写，还是只部分读写及变更？是否需要内部计算？比如，用户粉丝数，很多普通用户的粉丝有几千到几万，而大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表肯定不能采用整体读写的方式，只能部分获取。另外在判断某用户是否关注了另外一个用户时，也不需要拉取该用户的全部关注列表，直接在关注列表上进行检查判断，然后返回 True/False 或 0/1 的方式更为高效。 KV size然后是不同业务数据缓存 KV 的 size。如果单个业务的 KV size 过大，需要分拆成多个 KV 来缓存。但是，不同缓存数据的 KV size 如果差异过大，也不能缓存在一起，避免缓存效率的低下和相互影响。 key 的数量key 的数量也是一个重要考虑因素。如果 key 数量不大，可以在缓存中存下全量数据，把缓存当 DB 存储来用，如果缓存读取 miss，则表明数据不存在，根本不需要再去 DB 查询。如果数据量巨大，则在缓存中尽可能只保留频繁访问的热数据，对于冷数据直接访问 DB。 读写峰值另外，对缓存数据的读写峰值，如果小于 10万 级别，简单分拆到独立 Cache 池即可。而一旦数据的读写峰值超过 10万 甚至到达 100万 级的QPS，则需要对 Cache 进行分层处理，可以同时使用 Local-Cache 配合远程 cache，甚至远程缓存内部继续分层叠加分池进行处理。微博业务中，大多数核心业务的 Memcached 访问都采用的这种处理方式。 命中率缓存的命中率对整个服务体系的性能影响甚大。对于核心高并发访问的业务，需要预留足够的容量，确保核心业务缓存维持较高的命中率。比如微博中的 Feed Vector Cache，常年的命中率高达 99.5% 以上。为了持续保持缓存的命中率，缓存体系需要持续监控，及时进行故障处理或故障转移。同时在部分缓存节点异常、命中率下降时，故障转移方案，需要考虑是采用一致性 Hash 分布的访问漂移策略，还是采用数据多层备份策略。 过期策略 可以设置较短的过期时间，让冷 key 自动过期； 也可以让 key 带上时间戳，同时设置较长的过期时间，比如很多业务系统内部有这样一些 key:key_20190801。 平均缓存穿透加载时间平均缓存穿透加载时间在某些业务场景下也很重要，对于一些缓存穿透后，加载时间特别长或者需要复杂计算的数据，而且访问量还比较大的业务数据，要配置更多容量，维持更高的命中率，从而减少穿透到 DB 的概率，来确保整个系统的访问性能。 缓存可运维性对于缓存的可运维性考虑，则需要考虑缓存体系的集群管理，如何进行一键扩缩容，如何进行缓存组件的升级和变更，如何快速发现并定位问题，如何持续监控报警，最好有一个完善的运维平台，将各种运维工具进行集成。 缓存安全性对于缓存的安全性考虑，一方面可以限制来源 IP，只允许内网访问，同时对于一些关键性指令，需要增加访问权限，避免被攻击或误操作时，导致重大后果。]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CollectionUtils工具类]]></title>
    <url>%2F2020%2F10%2F13%2FCollectionUtils%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[CollectionUtils工具类Apache Commons Collections 使操作集合时，代码更加简洁 常用API元素不为null时，向集合添加元素 1CollectionUtils.addIgnoreNull(targetList, targetObject); 将两个已排序的集合a和b合并为一个已排序的列表，保留元素的自然顺序 1CollectionUtils.collate(Iterable&lt;? extends O&gt; a, Iterable&lt;? extends O&gt; b) 将两个已排序的集合a和b合并到一个已排序的列表中，根据比较器顺序 1CollectionUtils.collate(Iterable&lt;? extends O&gt; a, Iterable&lt;? extends O&gt; b, Comparator&lt;? super O&gt; c) 如果参数是null，则返回不可变的空集合（new EmptyList&lt;&gt;()），否则返回参数本身 1CollectionUtils.emptyIfNull(Collection&lt;T&gt; collection) 空安全检查指定的集合是否为空 12CollectionUtils.isEmpty(Collection&lt;?&gt; coll)CollectionUtils.isNotEmpty(Collection&lt;?&gt; coll) 反转给定数组的顺序 1CollectionUtils.reverseArray(Object[] array) 差集 1CollectionUtils.subtract(Iterable&lt;? extends O&gt; a, Iterable&lt;? extends O&gt; b) 并集 1CollectionUtils.union(Iterable&lt;? extends O&gt; a, Iterable&lt;? extends O&gt; b) 交集 1CollectionUtils.intersection(Collection a, Collection b) 交集的补集 1CollectionUtils.disjunction(Collection a, Collection b)]]></content>
      <categories>
        <category>工具类</category>
      </categories>
      <tags>
        <tag>集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Data JPA 自定义返回值]]></title>
    <url>%2F2020%2F09%2F27%2FSpringDataJpa%E8%87%AA%E5%AE%9A%E4%B9%89%E8%BF%94%E5%9B%9E%E5%80%BC%2F</url>
    <content type="text"><![CDATA[Spring Data JPA 自定义返回值打开 SimpleJpaRepository 直接看它的 Structure ，从它实现的方法以及父类接口的方法看到，返回类型包括：Optional、Iterable、List、Page、Long、Boolean、Entity 等，如下图所示： 而实际上支持的返回类型还要多一些。 由于 Repository 里面支持 Iterable，所以 java 标准的 List、Set 都可以作为返回结果，并且也会支持其子类。 查询结果的返回值StreamableSpring Data 里面定义了一个特殊的子类 Streamable，Streamable 可以替代 Iterable 或任何集合类型。它还提供了方便的方法来访问 Stream，可以直接在元素上进行 ….filter(…) 和 ….map(…) 操作，并将 Streamable 连接到其他元素。 我们看个关于 UserRepository 直接继承 JpaRepository 的例子 12public interface UserRepository extends JpaRepository&lt;User,Long&gt; &#123;&#125; UserRepository 类，在测试类里面做如下调用： 12345678User user = userRepository.save( User.builder().name("jackxx").email("123456@126.com") .sex("man").address("shanghai").build());Assert.assertNotNull(user);Streamable&lt;User&gt; userStreamable = userRepository .findAll(PageRequest.of(0,10)) .and(User.builder().name("jack222").build());userStreamable.forEach(System.out::println); 输出如下： 12User(id=1, name=jackxx, email=123456@126.com, sex=man, address=shanghai)User(id=null, name=jack222, email=null, sex=null, address=null) 这个例子 Streamable&lt;User&gt; userStreamable，实现了 Streamable 的返回结果。 自定义 Streamable官方给我们提供了自定义 Streamable 的方法，不过在实际工作中很少出现要自定义保证结果类的情况。 123456789101112131415class Product &#123; (1) MonetaryAmount getPrice() &#123; … &#125;&#125;@RequiredArgConstructor(staticName = "of")class Products implements Streamable&lt;Product&gt; &#123; (2) private Streamable&lt;Product&gt; streamable; public MonetaryAmount getTotal() &#123; (3) return streamable.stream() // .map(Priced::getPrice) .reduce(Money.of(0), MonetaryAmount::add); &#125;&#125;interface ProductRepository implements Repository&lt;Product, Long&gt; &#123; Products findAllByDescriptionContaining(String text); (4)&#125; 以上四个步骤介绍了自定义 Streamable 的方法，分别为： （1）Product 实体，公开 API 以访问产品价格。 （2）Streamable&lt;Product&gt; 的包装类型可以通过 Products.of(…) 构造（通过 Lombok 注解创建的工厂方法）。 （3）包装器类型在 Streamable&lt;Product&gt; 上公开了计算新值的其他 API。 （4）可以将包装器类型直接用作查询方法返回类型。无须返回 Streamable&lt;Product&gt; 并将其手动包装在存储库 Client 端中。 其原理很简单，就是实现 Streamable接口，自己定义自己的实现类即可。 源码在 QueryExecutionResultHandler ，它里面有对 Streamable 子类的判断，来支持自定义 Streamable，关键源码如下： 通过源码你会发现 Streamable 为什么生效，下面来看看常见的集合类的返回实现。 List/Stream/Page/Slice首先，新建我们的 UserRepository： 12345678910111213package com.example.jpa.example1;import org.springframework.data.domain.Pageable;import org.springframework.data.jpa.repository.JpaRepository;import org.springframework.data.jpa.repository.Query;import java.util.stream.Stream;public interface UserRepository extends JpaRepository&lt;User,Long&gt; &#123; //自定义一个查询方法，返回Stream对象，并且有分页属性 @Query("select u from User u") Stream&lt;User&gt; findAllByCustomQueryAndStream(Pageable pageable); //测试Slice的返回结果 @Query("select u from User u") Slice&lt;User&gt; findAllByCustomQueryAndSlice(Pageable pageable);&#125; 然后，修改一下我们的测试用例类，如下，验证一下结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.example.jpa.example1;import com.fasterxml.jackson.core.JsonProcessingException;import com.fasterxml.jackson.databind.ObjectMapper;import org.assertj.core.util.Lists;import org.junit.Assert;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.autoconfigure.orm.jpa.DataJpaTest;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Slice;import org.springframework.data.util.Streamable;import java.util.List;import java.util.stream.Stream;@DataJpaTestpublic class UserRepositoryTest &#123; @Autowired private UserRepository userRepository; @Test public void testSaveUser() throws JsonProcessingException &#123; //我们新增7条数据方便测试分页结果 userRepository.save(User.builder().name("jack1").email("123456@126.com") .sex("man").address("shanghai").build()); userRepository.save(User.builder().name("jack2").email("123456@126.com") .sex("man").address("shanghai").build()); userRepository.save(User.builder().name("jack3").email("123456@126.com") .sex("man").address("shanghai").build()); userRepository.save(User.builder().name("jack4").email("123456@126.com") .sex("man").address("shanghai").build()); userRepository.save(User.builder().name("jack5").email("123456@126.com") .sex("man").address("shanghai").build()); userRepository.save(User.builder().name("jack6").email("123456@126.com") .sex("man").address("shanghai").build()); userRepository.save(User.builder().name("jack7").email("123456@126.com") .sex("man").address("shanghai").build()); //我们利用ObjectMapper将我们的返回结果Json to String ObjectMapper objectMapper = new ObjectMapper(); //返回Stream类型结果（1） Stream&lt;User&gt; userStream = userRepository.findAllByCustomQueryAndStream(PageRequest.of(1,3)); userStream.forEach(System.out::println); //返回分页数据（2） Page&lt;User&gt; userPage = userRepository.findAll(PageRequest.of(0,3)); System.out.println(objectMapper.writeValueAsString(userPage)); //返回Slice结果（3） Slice&lt;User&gt; userSlice = userRepository.findAllByCustomQueryAndSlice(PageRequest.of(0,3)); System.out.println(objectMapper.writeValueAsString(userSlice)); //返回List结果（4） List&lt;User&gt; userList = userRepository.findAllById(Lists.newArrayList(1L,2L)); System.out.println(objectMapper.writeValueAsString(userList)); &#125;&#125; 这个时候我们分别看下四种测试结果： 第一种：通过 Stream&lt;User&gt;取第二页的数据得到结果如下： 123User(id=4, name=jack4, email=123456@126.com, sex=man, address=shanghai)User(id=5, name=jack5, email=123456@126.com, sex=man, address=shanghai)User(id=6, name=jack6, email=123456@126.com, sex=man, address=shanghai) Spring Data 的支持可以通过使用 Java 8 Stream 作为返回类型来逐步处理查询方法的结果。需要注意的是：流的关闭问题，try cache 是一种常用的关闭方法，如下所示： 1234567891011Stream&lt;User&gt; stream;try &#123; stream = repository.findAllByCustomQueryAndStream() stream.forEach(…);&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; if (stream!=null)&#123; stream.close(); &#125;&#125; 第二种：返回 Page&lt;User&gt; 的分页数据结果得到结果如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&#123; "content":[ &#123; "id":1, "name":"jack1", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125;, &#123; "id":2, "name":"jack2", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125;, &#123; "id":3, "name":"jack3", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125; ], "pageable":&#123; "sort":&#123; "sorted":false, "unsorted":true, "empty":true &#125;, "pageNumber":0, // 当前页码 "pageSize":3, // 页码大小 "offset":0, // 偏移量 "paged":true, // 是否分页了 "unpaged":false &#125;, "totalPages":3, // 一共有多少页 "last":false, // 是否是到最后 "totalElements":7, // 总记录数 "numberOfElements":3, // 当前数据下标 "sort":&#123; "sorted":false, "unsorted":true, "empty":true &#125;, "size":3, // 当前content大小 "number":0, // 当前页面码的索引 "first":true, // 是否是第一页 "empty":false // 是否有数据&#125; 这里我们可以看到 Page&lt;User&gt; 返回了第一个页的数据，并且告诉我们一共有三个部分的数据： content：数据的内容，现在指 User 的 List 3 条。 pageable：分页数据，包括排序字段是什么及其方向、当前是第几页、一共多少页、是否是最后一条等。 当前数据的描述： “size”：3，当前 content 大小 “number”：0，当前页面码的索引 “first”：true，是否是第一页 “empty”：false，是否没有数据 第三种：返回 Slice&lt;User&gt; 结果得到结果如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&#123; "content":[ &#123; "id":4, "name":"jack4", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125;, &#123; "id":5, "name":"jack5", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125;, &#123; "id":6, "name":"jack6", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125; ], "pageable":&#123; "sort":&#123; "sorted":false, "unsorted":true, "empty":true &#125;, "pageNumber":1, "pageSize":3, "offset":3, "paged":true, "unpaged":false &#125;, "numberOfElements":3, "sort":&#123; "sorted":false, "unsorted":true, "empty":true &#125;, "size":3, "number":1, "first":false, "last":false, "empty":false&#125; 这时我们发现上面的 Page 返回结果少了，少了跟记录数相关的内容 1234"totalPages":3, // 一共有多少页"last":false, // 是否是到最后"totalElements":7, // 总记录数"numberOfElements":3, // 当前数据下标 再比较一下第二种和第三种测试结果的执行 SQL： 第二种执行的是普通的分页查询 SQL： 1234-- 查询分页数据Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_ from user user0_ limit ?-- 计算分页数据Hibernate: select count(user0_.id) as col_0_0_ from user user0_ 第三种执行的 SQL 如下： 1Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_ from user user0_ limit ? offset ? 通过对比可以看出，只查询偏移量，不计算分页数据，这就是 Page 和 Slice 的主要区别。我们接着看第四种测试结果。 第四种：返回 List&lt;User&gt;得到结果如下： 12345678910111213141516[ &#123; "id":1, "name":"jack1", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125;, &#123; "id":2, "name":"jack2", "email":"123456@126.com", "sex":"man", "address":"shanghai" &#125;] 可以很简单地查询出来 ID=1 和 ID=2 的数据，没有分页信息。 上面四种方法介绍了常见的多条数据返回结果的形式，单条的查询无非就是对 JDK8 的 Optional 的支持。比如支持了 Null 的优雅判断，再一个就是支持直接返回 Entity，或者一些存在 / 不存在的 Boolean 的结果和一些 count 条数的返回结果而已。 异步查询返回值Feature/CompletableFuture/ListenableFuture我们可以使用 Spring 的异步方法执行Repository查询，这意味着方法将在调用时立即返回，并且实际的查询执行将发生在已提交给 Spring TaskExecutor 的任务中，比较适合定时任务的实际场景。异步使用起来比较简单，直接加@Async 注解即可，如下所示： 123456@AsyncFuture&lt;User&gt; findByFirstname(String firstname); (1)@AsyncCompletableFuture&lt;User&gt; findOneByFirstname(String firstname); (2)@AsyncListenableFuture&lt;User&gt; findOneByLastname(String lastname);(3) 上述三个异步方法的返回结果，分别做如下解释： 第一处：使用 java.util.concurrent.Future 的返回类型； 第二处：使用 java.util.concurrent.CompletableFuture 作为返回类型； 第三处：使用 org.springframework.util.concurrent.ListenableFuture 作为返回类型。 以上是对 @Async 的支持，关于实际使用需要注意以下三点内容： 在实际工作中，直接在 Repository 这一层使用异步方法的场景不多，一般都是把异步注解放在 Service 的方法上面，这样的话，可以有一些额外逻辑，如发短信、发邮件、发消息等配合使用； 使用异步的时候一定要配置线程池，这点切记，否则“死”得会很难看； 📌 万一失败要怎么处理？关于事务是怎么处理的呢? 接下来看看 Repository 对 Reactive 是如何支持的。 对 Reactive 支持 flux 与 MonoSpring Data Common 里面对 Reactive 还是有支持的，但是 Common 里面提供的只是接口，而 JPA 里面没有做相关的 Reactive 的实现，所以Spring Data JPA 不支持 Reactive 其他子模块，如 mongo实现了该接口 下面我们在 gradle 里面引用一个 Spring Data Common 的子模块 implementation ‘org.springframework.boot:spring-boot-starter-data-mongodb’ 来加载依赖，这时候我们打开 Repository 看 Hierarchy 就可以看到，这里多了一个 Mongo 的 Repository 的实现，天然地支持着 Reactive 这条线。 相信到这里你能感受到 Spring Data Common 的强大支持，对 Repository 接口的不同实现也有了一定的认识。 总结下面打开 ResultProcessor 类的源码看一下支持的类型有哪些。 从上图可以看出 processResult 的时候分别对 PageQuery、Stream、Reactive 有了各自的判断，我们 debug 到这里的时候来看一下 convert，进入到类里面。 可以看到 QueryExecutorConverters 里面对 JDK8、Guava、vavr 也做了各种支持，自行阅读源码 下表列出了 Spring Data JPA Query Method 机制支持的方法的返回值类型： 📌 Geospatial types (such as GeoResult, GeoResults, and GeoPage) are available only for data stores that support geospatial queries. Return type Description void 不返回结果，一般是更新操作 Primitives Java 的基本类型，一般常见的是统计操作 （如 long、boolean 等） Wrapper types Wrapper types Java 的包装类 T 返回唯一的实体，没有查询结果返回null ，超过一个结果会抛出IncorrectResultSizeDataAccessException Iterator&lt;T&gt; An Iterator. Collection&lt;T&gt; A Collection. List&lt;T&gt; A List. Optional&lt;T&gt; A Java 8 or Guava Optional. Expects the query method to return one result at most. If no result is found, Optional.empty() or Optional.absent() is returned. More than one result triggers an IncorrectResultSizeDataAccessException. Option&lt;T&gt; Either a Scala or Vavr Option type. Semantically the same behavior as Java 8’s Optional, described earlier. Stream&lt;T&gt; A Java 8 Stream. Streamable&lt;T&gt; A convenience extension of Iterable that directly exposes methods to stream, map and filter results, concatenate them etc. Types that implement Streamable and take a Streamable constructor or factory method argument Types that expose a constructor or ….of(…)/….valueOf(…) factory method taking a Streamable as argument. See Returning Custom Streamable Wrapper Types for details. Vavr Seq, List, Map, Set Vavr collection types. See Support for Vavr Collections for details. Future&lt;T&gt; A Future. Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. CompletableFuture&lt;T&gt; A Java 8 CompletableFuture. Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. ListenableFuture A org.springframework.util.concurrent.ListenableFuture. Expects a method to be annotated with @Async and requires Spring’s asynchronous method execution capability to be enabled. Slice A sized chunk of data with an indication of whether there is more data available. Requires a Pageable method parameter. Page&lt;T&gt; A Slice with additional information, such as the total number of results. Requires a Pageable method parameter. GeoResult&lt;T&gt; A result entry with additional information, such as the distance to a reference location. GeoResults&lt;T&gt; A list of GeoResult&lt;T&gt; with additional information, such as the average distance to a reference location. GeoPage&lt;T&gt; A Page with GeoResult&lt;T&gt;, such as the average distance to a reference location. Mono&lt;T&gt; A Project Reactor Mono emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException. Flux&lt;T&gt; A Project Reactor Flux emitting zero, one, or many elements using reactive repositories. Queries returning Flux can emit also an infinite number of elements. Single&lt;T&gt; A RxJava Single emitting a single element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException. Maybe&lt;T&gt; A RxJava Maybe emitting zero or one element using reactive repositories. Expects the query method to return one result at most. If no result is found, Mono.empty() is returned. More than one result triggers an IncorrectResultSizeDataAccessException. Flowable&lt;T&gt; A RxJava Flowable emitting zero, one, or many elements using reactive repositories. Queries returning Flowable can emit also an infinite number of elements. 常见的 DTO 返回结果的支持方法Projections 的概念 Spring JPA 对 Projections 扩展的支持，这是个非常好的东西，从字面意思上理解就是映射，指的是和 DB 的查询结果的字段映射关系。一般情况下，返回的字段和 DB 的查询结果的字段是一一对应的；但有的时候，需要返回一些指定的字段，或者返回一些复合型的字段，而不需要全部返回。 一般的做法是自己写各种 entity 到 view 的各种 convert 的转化逻辑，而 Spring Data 正是考虑到了这一点，允许对专用返回类型进行建模，有选择地返回同一个实体的不同视图对象。 下面还以 User 查询对象为例，看看怎么自定义返回 DTO： 1234567891011121314@Entity@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class User &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email; private String sex; private String address;&#125; 看上面的原始 User 实体代码，如果我们只想返回 User 对象里面的 name 和 email，应该怎么做？下面介绍三种方法。 第一种：同一张 Table，不同 Entity首先，我们新增一个Entity类：通过 @Table 指向同一张表，这张表和 User 实例里面的表一样都是 user，完整内容如下： 12345678910111213@Entity@Table(name = "user")@Data@Builder@AllArgsConstructor@NoArgsConstructorpublic class UserOnlyNameEmailEntity &#123; @Id @GeneratedValue(strategy= GenerationType.AUTO) private Long id; private String name; private String email;&#125; 然后，新增一个 UserOnlyNameEmailEntityRepository，做单独的查询： 1234package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;public interface UserOnlyNameEmailEntityRepository extends JpaRepository&lt;UserOnlyNameEmailEntity, Long&gt; &#123;&#125; 最后，我们的测试用例里面的写法如下： 1234567891011@Testpublic void testProjections() &#123; userRepository.save(User.builder().id(1L) .name("jack12") .email("123456@126.com").sex("man") .address("shanghai").build()); List&lt;User&gt; users= userRepository.findAll(); System.out.println(users); UserOnlyNameEmailEntity uName = userOnlyNameEmailEntityRepository.getOne(1L); System.out.println(uName);&#125; 我们看一下输出结果： 12345Hibernate: insert into user (address, email, name, sex, id) values (?, ?, ?, ?, ?)Hibernate: select user0_.id as id1_0_, user0_.address as address2_0_, user0_.email as email3_0_, user0_.name as name4_0_, user0_.sex as sex5_0_ from user user0_[User(id=1, name=jack12, email=123456@126.com, sex=man, address=shanghai)]Hibernate: select useronlyna0_.id as id1_0_0_, useronlyna0_.email as email3_0_0_, useronlyna0_.name as name4_0_0_ from user useronlyna0_ where useronlyna0_.id=?UserOnlyNameEmailEntity(id=1, name=jack12, email=123456@126.com) 上述结果可以看到，当在 user 表里面插入了一条数据，而 userRepository 和 userOnlyNameEmailEntityRepository 查询的都是同一张表 user。 好处：简单、方便，很容易可以想到； 缺点：通过两个实体都可以进行 update 操作，如果同一个项目里面这种实体比较多， ​ 到时候就容易不知道是谁更新的，从而导致出 bug 不好定位， ​ 实体职责划分不明确。 第二种：定义 DTO 类首先，我们新建一个 DTO 类来返回我们想要的字段，它是 UserOnlyNameEmailDto，用来接收 name、email 两个字段的值，具体如下： 123456@Data@Builder@AllArgsConstructorpublic class UserOnlyNameEmailDto &#123; private String name, email;&#125; 其次，在 UserRepository 里面做如下用法： 1234public interface UserRepository extends JpaRepository&lt;User,Long&gt; &#123; //测试只返回name和email的DTO UserOnlyNameEmailDto findByEmail(String email);&#125; 然后，测试用例里面写法如下： 12345678@Testpublic void testProjections() &#123;userRepository.save(User.builder().id(1L) .name("jack12").email("123456@126.com").sex("man") .address("shanghai").build());UserOnlyNameEmailDto userOnlyNameEmailDto = userRepository.findByEmail("123456@126.com");System.out.println(userOnlyNameEmailDto);&#125; 最后，输出结果如下： 12Hibernate: select user0_.name as col_0_0_, user0_.email as col_1_0_ from user user0_ where user0_.email=?UserOnlyNameEmailDto(name=jack12, email=123456@126.com) 这里需要注意的是，如果我们去看源码的话，看关键的 PreferredConstructorDiscoverer 类时会发现，UserDTO 里面只能有一个全参数构造方法，如下所示： 如上图所示，Constructor 选择的时候会帮我们做构造参数的选择，如果 DTO 里面有多个构造方法，就会报转化错误的异常，这一点需要注意，异常是这样的： 1No converter found capable of converting from type [com.example.jpa.example1.User] to type [com.example.jpa.example1.UserOnlyNameEmailDto 优点：返回的结果不需要是个实体对象，对 DB 不能进行除了查询之外的任何操作； 缺点：有 set 方法还可以改变里面的值，构造方法不能更改，必须全参数， ​ 这样如果是不熟悉 JPA 的新人操作的时候很容易引发 Bug。 第三种：定义 DTO 接口我们再来学习一种返回不同字段的方式，这种方式与上面两种的区别是只需要定义接口，它的好处是只读，不需要添加构造方法，我们使用起来非常灵活，一般很难产生 Bug，那么它怎么实现呢？ 首先，定义一个 UserOnlyName 的接口： 12345package com.example.jpa.example1;public interface UserOnlyName &#123; String getName(); String getEmail();&#125; 12345678910package com.example.jpa.example1;import org.springframework.data.jpa.repository.JpaRepository;public interface UserRepository extends JpaRepository&lt;User,Long&gt; &#123; /** * 接口的方式返回DTO * @param address * @return */ UserOnlyName findByAddress(String address);&#125; 然后，测试用例的写法如下： 12345678@Testpublic void testProjections() &#123;userRepository.save(User.builder().name("jack12") .email("123456@126.com").sex("man") .address("shanghai").build());UserOnlyName userOnlyName = userRepository.findByAddress("shanghai");System.out.println(userOnlyName);&#125; 最后，我们的运行结果如下： 12Hibernate: select user0_.name as col_0_0_, user0_.email as col_1_0_ from user user0_ where user0_.address=?org.springframework.data.jpa.repository.query.AbstractJpaQuery$TupleConverter$TupleBackedMap@1d369521 这个时候会发现我们的 userOnlyName 接口成了一个代理对象，里面通过 Map 的格式包含了我们的要返回字段的值（如：name、email），我们用的时候直接调用接口里面的方法即可，如 userOnlyName.getName() 即可；这种方式的优点是接口为只读，并且语义更清晰，所以这种是我比较推荐的做法。 其中源码是如何实现的，我来说一个类，你可以通过 debug，看一下最终 DTO 和接口转化执行的 query 有什么不同，看下图中 debug 显示的 Query 语句的位置： 图一：是返回 DTO 接口形式的 query 生成的 JPQL： 图二：是返回 DTO 类的时候 QueryStructure 生成的 JPQL 语句： 两种最大的区别是 DTO 类需要构造方法 new 一个对象出来，这就是我们第二种方法里面需要注意的 DTO 构造函数的问题；而通过图一我们可以看到接口直接通过 as 别名，映射成 hashmap 即可，非常灵活。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>自定义返回值</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Oracle的基础知识]]></title>
    <url>%2F2020%2F09%2F22%2FOracle%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Oracle 的基础知识数据库设计 数据库设计是画出数据库的ER图，通过ER图生成 sql 建库代码(DDL) 生成的建库代码的约束都是追加的，方便到时删除 SQL类型 DQL：数据查询语言，关键字 select， 功能：查询 DML：数据操作语言，关键字 insert、delete、update，功能：操作数据 DDL：数据定义语言，关键字 create、drop、alter，功能：操作数据库对象 DCL：数据控制语言，关键字 grant、revoke，功能：用户授权撤权 TCL：事务控制语言，关键字 commit、rollback、savepoint，功能：事务处理 查询12345select [distinct]&#123;* | &lt;字段名&gt; | 表名.*&#125; from &lt;表名&gt;-- 1. 给表一个别名select * from emp e;-- 2. 给返回字段一个别名select emp.ename as 员工名 from emp; 条件查询where 关键字 查询运算符对比运算符 运算符 说明 = 等号；oracle 里面的赋值语句是=: &gt; 大于 &lt; 小于 &gt;= 大于等于 &lt;= 小于等于 &lt;&gt; 不等于，也可以用 != is 一般用于判空 is null，is not null in 集合 between ..and .. 在..之间 like 模糊查询 &lt;&gt; 是标准 sql 的不等于，任何关系型数据库都支持，!= 只有部分数据库支持 逻辑运算符 运算符 说明 and 与 or 或 not 非 模糊查询 查询员工的名字有 A 或者 E 的员工 1select * from emp where ename like '%A%' or ename like '%E%'; 查询第三个字符是 A 的员工 1select * from emp where ename like '__A%'; 查询员工名有 _ 的员工，需要转义；escape 是一个定义转义字符的意思 1select * from emp where ename like '%\_%' escape '\'; 分组查询以某个字段作为分类的条件统计需求，有条件就需要用到 having 筛选 查询语句的关键字的优先级别问题： 1from &gt; where &gt; group by &gt; having &gt; select &gt; order by 结论： 分组前：确定的条件使用 where， 分组后：确定的条件使用 having 原因就是 where 的优先级大于 group by 多表查询笛卡尔积 笛卡尔积是两个集合X和Y的直积，表示为X*Y； X集合的每个元素和Y集合的每个元素所形成的有序对集合就是X和Y的笛卡尔积 等值连接当条件为“=”的连接为等值连接，是连接属性值相等的那些元组； 其结果是连接的表的所有列，包括重复列，相当于内连接 123select * from emp, deptwhere emp.deptno = dept.deptno; 不等值连接两个表中相关的两列进行不等连接； 比较符号一般为 &lt;&gt;、&gt;、&lt;、&gt;=、&lt;=、LIKE、IN、BETWEEN…AND 内连接123select * from emp inner join depton emp.deptno = dept.deptno; 外连接外连接就是连接两个表，根据左外还是右外连接，基表的字段会全部显示； 如果字段不对应，非基表的表所有的字段值会设为null 左外连接左边的表作为基表，右表如有左表不对应的字段则设为null 右外连接右边的表作为基表，左表如有右表不对应的字段则设为null Oracle 外连接的特殊写法（鸡肋） 等值查询的条件上加一个(+)表示不是基表，则不加(+)才是基表 右外连接 1234&gt; select emp.*, dept.dname&gt; from emp,dept&gt; where emp.deptno(+) = dept.deptno;&gt; 左外连接 1234&gt; select emp.*, dept.dname&gt; from emp,dept&gt; where emp.deptno = dept.deptno(+);&gt; 自连接排序 语法： 12345&gt; select * | 字段 &gt; from &lt;表名&gt;&gt; [where &lt;条件&gt;]&gt; order by &lt;字段1, 字段2&gt; desc | asc&gt; 查询员工的信息，按部门编号排序，再按工资排序 1select * from emp order by deptno, sal; 查询员工的信息，按部门编号排序(正序)后，再按工资排序(倒序) 1select * from emp order by deptno, sal desc; 按员工的奖金排序，倒序的时候，null 放在后面 12select * from emp order by comm desc nulls first; -- 将null值放在最前select * from emp order by comm desc nulls last; -- 将null值放在最后 函数单行函数数组函数四舍五入函数 round(原值, 精度) 求员工的平均工资，保留两位小数 12-- 如果精度是正数，精确的是小数从小数点第一位开始，往右移select round(avg(sal), 2) avg(sal) from emp; 如：平均工资 = 2073.214 ​ round(平均工资, 2) = 2073.21 12-- 如果精度是负数，精确的是从个位开始，往左移select round(avg(sal), -1), avg(sal) from emp; 如：平均工资 = 2073.214 ​ round(平均工资, -1) = 2070 ​ round(平均工资, -2) = 2100 ​ round(平均工资, -3) = 2000 ​ round(平均工资, -4) = 0 截取函数 trunc(原值, 精度) 12-- 和四舍五入差不多区别就是不会向上进位，就是截取精度的位置select trunc(avg(sal), 2) avg(sal) from emp; 字符函数查询员工的姓名，格式为 ‘员工名:姓名’ 方式一：使用 || 连接 1select '员工名:' || ename from emp; 方式二：使用 concat 函数 1select concat('员工名:', ename) from emp; 长度函数 length(原值) 1select ename, length(ename) from emp; 替换函数 replace(c1, c2, c3) c1：原值 c2：需要替换的值 c3：新值 将 my name is leo 的 my 修改为 your 1select replace('my name is leo', 'my', 'your') from dual; 因为这个修改没有表，所以给它一个临时表 dual 时间函数获取系统当前时间 1select sysdate from dual; 月份对比函数 months_between(d1, d2) d1 &lt; d2，返回负数 d1 = d2，返回 0 d1 &gt; d2，返回正数 返回值：两个日期的月份间隔 作用： 用于计算两个日期间隔的月份数 对比两个日期的大小 2018-06-26 与 2020-09-22 相隔多少个月 1234select months_between( to_date('2018-06-26', 'yyyy-mm-dd'), to_date('2020-09-22', 'yyyy-mm-dd')) from dual; 月份增加函数 add_months(原值, 增加的月份数) 给当前日期加三个月 1select add_months(sysdate, 3) from dual; 给当前日期减三个月 1select add_months(sysdate, -3) from dual; 日期截取函数 extract( 类型 from 时间|日期) 类型包括： year month day hour minute second 截取年 1select extract(year from sysdate) from dual; 截取月 1select extract(month from sysdate); 截取日 1select extract(day from sysdate); 截取时 方式一： 123select extract(hour from to_timestamp('2020-09-22 13:12:20', 'yyyy-mm-dd hh24:mi:ss'))from dual; 方式二： 12select extract(hour from timestamp'2020-09-22 13:12:20') from dual; 截取分 12select extract(minute from timestamp'2020-09-22 13:12:20)from dual; 截取秒 12select extract(second from timestamp'2020-09-22 13:12:20)from dual; 查询 1981 年入职的员工 123select * from emp where extract(year from HIRE_DATE) = 1981; 转换函数字符串转成日期 to_date(原值, ‘格式’); 年：yyyy月：mm日：dd时：hh 表示使用12小时制，hh24 表示使用24小时制分：mi秒：ss 123select to_date('2020-09-22', 'yyyy-mm-dd') from dual; select to_date('2020/09/22', 'yyyy/mm/dd') from dual;select to_date('09/22/2020', 'mm/dd/yyyy') from dual; 字符串转时间 to_timestamp(原值, ‘格式’) 12select to_timestamp('2020-09-22 13:12:20', 'yyyy-mm-dd hh24:mi:ss')from dual; 其他数据类型转成字符 to_char(原值, ‘格式’) 12select to_char(sysdate, 'yyyy-mm-dd') from dual;select to_char(sysdate, 'yyyy/mm/dd') from dual; 将字符格式转成数值格式 to_number(原值, 格式) oracle里面，数值占位符使用9 12select to number('$8,898,765', '$9,999,999')from dual; 通用函数空处理函数 nvl(原值, 替换值) 如果原值为null，那么就自动转成替换值，替换值的类型必须和数据表字段声明的兼容 查询员工的奖金，如果奖金为 null，那么修改为 0 1select ename, bonus, nvl(bonus, 0) from emp; nvl2(原值, 替换值1, 替换值2) 如果原值不为空，替换成替换值1，否则替换成替换值2 查询员工的奖金，如果奖金为 null ，那么修改为 0，使用 nvl2 实现 1select ename, bonus, nvl2(bonus, bonus, 0) from dual; 条件函数 decode(原值, ‘值1’, ‘翻译值1’, ‘值2’, ‘翻译值2’, ‘默认值’) 一个根据值来判断输出的信息；类似于 if / else 查询员工的姓名和员工的部门编号； 如果编号为10，返回综合部，如果编号为20，返回研发部，如果编号为30，返回销售部，其他返回，人力资源部 12select ename decode(deptno, 10, '综合部', 20, '研发部', 30, '销售部', '人力资源部')from dual; 查询员工的奖金，如果奖金为 null，那么修改为 ‘没奖金’ 1select decode(comm, 'null', '没奖金', bonus); 多行函数 多行函数也叫聚合函数 max()：最大值 min()：最小值 avg()：平均值 count()：统计个数 sum()：求和]]></content>
      <categories>
        <category>数据库</category>
        <category>Oracle</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>Oracle</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Defining Query Methods语法]]></title>
    <url>%2F2020%2F09%2F22%2FSpringDataJpa%E7%9A%84DQM%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Defining Query MethodsSpring Data JPA 的方法名定义查询方法 （ Defining Query Methods ）可以使语义更加清晰，提升开发效率。 DQM 语法共有 2 种 直接通过方法名实现 @Query 手动在方法上定义 定义查询方法的配置和使用方法若想要实现 CRUD 的操作，常规做法是写一大堆 SQL 语句。但在 JPA 里面，只需要继承 Spring Data Common 里面的任意 Repository 接口或者子接口，然后直接通过方法名就可以实现。具体使用步骤： User 实体的 UserRepository 继承 Spring Data Common 里面的 Repository 接口 123public interface UserRepository extends CrudRepository&lt;User, Long&gt; &#123; User findByEmailAddress(String emailAddress);&#125; 对于 Service 层就可以直接使用 UserRepository 接口 12345678@Servicepublic class UserServiceImpl &#123; @Autowired private UserRepository userRepository; public void testJpa() &#123; userRepository.deleteAll(); userRepository.findAll(); userRepository.findByEmailAddress("zjk@126.com");&#125; 这个时候就可以直接调用 CrudRepository 里面暴露的所有接口方法，以及 UserRepository 里面定义的方法，不需要写任何 SQL 语句，也不需要写任何实现方法。 选择性暴露方法有时不想暴露 CrudRepository 里面的所有方法，可以直接继承我们认为需要暴露的那些方法的接口。 假如 UserRepository 只想暴露 findOne 和 save，除了这两个方法之外不允许任何的 User 操作，其做法如下： 选择性地暴露 CRUD 方法，直接继承Repository（因为这里面没有任何方法），把CrudRepository 里面的 save 和 findOne 方法复制到我们自己的 MyBaseRepository 接口即可，代码如下： 12345678@NoRepositoryBeaninterface MyBaseRepository&lt;T, ID extends Serializable&gt; extends Repository&lt;T, ID&gt; &#123; T findOne(ID id); T save(T entity);&#125;interface UserRepository extends MyBaseRepository&lt;User, Long&gt; &#123; User findByEmailAddress(String emailAddress);&#125; 这样在 Service 层就只有 findOne、save、findByEmailAddress 这 3 个方法可以调用，不会有更多方法了，我们可以对 SimpleJpaRepository 里面任意已经实现的方法做选择性暴露。 综上所述，得出以下 2 点结论： MyRepository Extends Repository 接口可以实现 Defining Query Methods 的功能； 继承其他 Repository 的子接口，或者自定义子接口，可以选择性地暴露 SimpleJpaRepository 里面已经实现的基础公用方法。 方法的查询策略设置鸡肋。。 通过 @EnableJpaRepositories 注解来配置方法的查询策略，详细配置方法如下： 1@EnableJpaRepositories(queryLookupStrategy= QueryLookupStrategy.Key.CREATE_IF_NOT_FOUND) QueryLookupStrategy.Key 的值共 3 个，具体如下： Create：直接根据方法名进行创建，规则是根据方法名称的构造进行尝试，一般的方法是从方法名中删除给定的一组已知前缀，并解析该方法的其余部分。如果方法名不符合规则，启动的时候会报异常，这种情况可以理解为，即使配置了 @Query 也是没有用的。 USE_DECLARED_QUERY：声明方式创建，启动的时候会尝试找到一个声明的查询，如果没有找到将抛出一个异常，可以理解为必须配置 @Query。 CREATE_IF_NOT_FOUND：这个是==默认的==，除非有特殊需求，可以理解为这是以上 2 种方式的兼容版。先用声明方式（@Query）进行查找，如果没有找到与方法相匹配的查询，那用 Create 的方法名创建规则创建一个查询；这两者都不满足的情况下，启动就会报错。 以 Spring Boot 项目为例，更改其配置方法如下： 123456@EnableJpaRepositories(queryLookupStrategy= QueryLookupStrategy.Key.CREATE_IF_NOT_FOUND)public class Example1Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Example1Application.class, args); &#125;&#125; Defining Query Method 语法 带查询功能的方法名：查询策略（关键字）+ 查询字段 + 一些限制性条件 具有语义清晰、功能完整的特性，我们实际工作中 80% 的 API 查询都可以简单实现。 举例说明： 1234567891011121314interface PersonRepository extends Repository&lt;User, Long&gt; &#123; // and 的查询关系 List&lt;User&gt; findByEmailAddressAndLastname(EmailAddress emailAddress, String lastname); // 包含 distinct 去重，or 的 sql 语法 List&lt;User&gt; findDistinctPeopleByLastnameOrFirstname(String lastname, String firstname); // 根据 lastname 字段查询忽略大小写 List&lt;User&gt; findByLastnameIgnoreCase(String lastname); // 根据 lastname 和 firstname 查询 equal 并且忽略大小写 List&lt;User&gt; findByLastnameAndFirstnameAllIgnoreCase(String lastname, String firstname); // 对查询结果根据 lastname 排序，正序 List&lt;User&gt; findByLastnameOrderByFirstnameAsc(String lastname); // 对查询结果根据 lastname 排序，倒序 List&lt;User&gt; findByLastnameOrderByFirstnameDesc(String lastname);&#125; 常用的关键字列表 Keyword Sample JPQL snippet And findByLastnameAndFirstname … where x.lastname = ?1 and x.firstname = ?2 Or findByLastnameOrFirstname … where x.lastname = ?1 or x.firstname = ?2 Is, Equals findByFirstname findByFirstnameIs findByFirstnameEquals … where x.firstname = ?1 Between findByStartDateBetween … where x.startDate between ?1 and ?2 LessThan findByAgeLessThan … where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual … where x.age &lt;= ?1 GreaterThan findByAgeGreaterThan … where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual … where x.age &gt;= ?1 After findByStartDateAfter … where x.startDate &gt; ?1 Before findByStartDateBefore … where x.startDate &lt; ?1 IsNull, Null findByAge(Is)Null … where x.age is null IsNotNull, NotNull findByAge(Is)NotNull … where x.age not null Like findByFirstnameLike … where x.firstname like ?1 NotLike findByFirstnameNotLike … where x.firstname not like ?1 StartingWith findByFirstnameStartingWith … where x.firstname like ?1 (parameter bound with appended %) EndingWith findByFirstnameEndingWith … where x.firstname like ?1 (parameter bound with prepended %) Containing findByFirstnameContaining … where x.firstname like ?1 (parameter bound wrapped in %) OrderBy findByAgeOrderByLastnameDesc … where x.age = ?1 order by x.lastname desc Not findByLastnameNot … where x.lastname &lt;&gt; ?1 In findByAgeIn(Collection&lt;Age&gt; ages) … where x.age in ?1 NotIn findByAgeNotIn(Collection&lt;Age&gt; ages) … where x.age not in ?1 True findByActiveTrue() … where x.active = true False findByActiveFalse() … where x.active = false IgnoreCase findByFirstnameIgnoreCase … where UPPER(x.firstame) = UPPER(?1) 综上，总结 3 点经验： 方法名的表达式通常是实体属性连接运算符的组合，如 And、or、Between、LessThan、GreaterThan、Like 等属性连接运算表达式，不同的数据库（NoSQL、MySQL）可能产生的效果不一样，如果遇到问题，可以打开 SQL 日志观察。 IgnoreCase 可以针对单个属性（如 findByLastnameIgnoreCase(…)），也可以针对查询条件里面所有的实体属性忽略大小写（所有属性必须在 String 情况下，如 findByLastnameAndFirstnameAllIgnoreCase(…)）。 OrderBy 可以在某些属性的排序上提供方向（Asc 或 Desc），称为静态排序，也可以通过一个方便的参数 Sort 实现指定字段的动态排序的查询方法（如 repository.findAll(Sort.by(Sort.Direction.ASC, “myField”))）。 上述表格虽然大多是 find 开头的方法，但其实 JPA 还支持 read、get、query、stream、count、exists、delete、remove 等前缀，如字面意思一样。我们来看看 count、delete、remove 的例子，其他前缀可以举一反三。实例代码如下： 12345678interface UserRepository extends CrudRepository&lt;User, Long&gt; &#123; // 查询总数 long countByLastname(String lastname); // 根据一个字段进行删除操作，并返回删除行数 long deleteByLastname(String lastname); // 根据 Lastname 删除一堆 User，并返回删除的 User List&lt;User&gt; removeByLastname(String lastname);&#125; 关键字都定义在下面两个类里面 12org.springframework.data.repository.query.parser.PartTreeorg.springframework.data.repository.query.parser.Part 看源码就可以知道框架支持了哪些逻辑关键字，比如 NotIn、Like、In、Exists 等，有的时候比查文档和任何人写的博客都准确、还快。 Sort 排序和 Pageable 分页Sort 在查询的时候可以实现动态排序 123public Sort(Direction direction, String... properties) &#123; this(direction, properties == null ? new ArrayList&lt;&gt;() : Arrays.asList(properties));&#125; Sort 里面决定了我们哪些字段的排序方向（ASC 正序、DESC 倒序） Pageable 在查询的时候可以实现分页效果和动态排序双重效果 Pageable 的 Structure，如下图所示： 方法一：Page允许将 org.springframework.data.domain.Pageable 实例传递给查询方法，将分页参数添加到静态定义的查询中，通过 Page 返回的结果得知可用的元素和页面的总数。 这种分页查询方法可能是昂贵的（会默认执行一条 count 的 SQL 语句），所以用的时候要考虑一下使用场景。 1Page&lt;User&gt; findByLastname(String lastname, Pageable pageable); 方法二：Slice返回结果是 Slice，因为只知道是否有下一个 Slice 可用，而不知道 count，所以当查询较大的结果集时，只知道数据是足够的，也就是说用在业务场景中时不用关心一共有多少页。 1Slice&lt;User&gt; findByLastname(String lastname, Pageable pageable); 方法三：List如果只需要排序，需在 org.springframework.data.domain.Sort 参数中添加一个参数，正如上面看到的，只需返回一个 List 也是有可能的。 1List&lt;User&gt; findByLastname(String lastname, Sort sort); 排序选项也通过 Pageable 实例处理，在这种情况下，Page 将不会创建构建实际实例所需的附加元数据（即不需要计算和查询分页相关数据），而仅仅用来做限制查询给定范围的实体。 1List&lt;User&gt; findByLastname(String lastname, Pageable pageable); PageRequest 里面提供的几个 of 静态方法（多态），分别构建页码、页面大小、排序等。如下所示： 12345678//查询user里面的lastname=jk的第一页，每页大小是20条；并会返回一共有多少页的信息Page&lt;User&gt; users = userRepository.findByLastname("jk", PageRequest.of(1, 20));//查询user里面的lastname=jk的第一页的20条数据，不知道一共多少条Slice&lt;User&gt; users = userRepository.findByLastname("jk", PageRequest.of(1, 20));//查询出来所有的user里面的lastname=jk的User数据，并按照name正序返回ListList&lt;User&gt; users = userRepository.findByLastname("jk", new Sort(Sort.Direction.ASC, "name"))//按照createdAt倒序，查询前一百条User数据List&lt;User&gt; users = userRepository.findByLastname("jk", PageRequest.of(0, 100, Sort.Direction.DESC, "createdAt")); 限制查询结果 First 和 Top有的时候我们想直接查询前几条数据，也不需要动态排序，那么就可以简单地在方法名字中使用 First 和 Top 关键字，来限制返回条数。 我们可以在 userRepository 里面定义的一些限制返回结果的使用。在查询方法上加限制查询结果的关键字 First 和 Top。 12345User findFirstByOrderByLastnameAsc();User findTopByOrderByAgeDesc();List&lt;User&gt; findDistinctUserTop3ByLastname(String lastname, Pageable pageable);List&lt;User&gt; findFirst10ByLastname(String lastname, Sort sort);List&lt;User&gt; findTop10ByLastname(String lastname, Pageable pageable); 其中： 查询方法在使用 First 或 Top 时，数值可以追加到 First 或 Top 后面，指定返回最大结果的大小； 如果数字被省略，则假设结果大小为 1； 限制表达式也支持 Distinct 关键字； 如果将 Pageable 作为参数，以 Top 和 First 后面的数字为准，即分页将在限制结果中应用。 @NonNull、@NonNullApi、@Nullable从 Spring Data 2.0 开始，JPA 新增了@NonNull @NonNullApi @Nullable，是对 null 的参数和返回结果做的支持。 @NonNullApi：在包级别用于声明参数，以及返回值的默认行为是不接受或产生空值的。 @NonNull：用于不能为空的参数或返回值（在 @NonNullApi 适用的参数和返回值上不需要）。 @Nullable：用于可以为空的参数或返回值。 我在自己的 Repository 所在 package 的 package-info.java 类里面做如下声明： 12@org.springframework.lang.NonNullApipackage com.myrespository; myRepository 下面的 UserRepository 实现如下： 1234package com.myrespository; interface UserRepository extends Repository&lt;User, Long&gt; &#123; User getByEmailAddress(EmailAddress emailAddress); &#125; 这个时候当 emailAddress 参数为 null 的时候就会抛异常，当返回结果为 null 的时候也会抛异常。因为我们在package 的 package-info.java里面指定了 NonNullApi，所有返回结果和参数不能为 Null。 12345//当我们添加 @Nullable 注解之后，参数和返回结果这个时候就都会允许为 null 了；@NullableUser findByEmailAddress(@Nullable EmailAddress emailAdress);//返回结果允许为 null，参数不允许为 null 的情况Optional&lt;User&gt; findOptionalByEmailAddress(EmailAddress emailAddress); 思考在实际工作中，也可以将方法名（非常语义化的 repository 里面所定义方法命名规范）的强制约定规范运用到 controller 和 service 层，这样全部统一后，可以减少很多的沟通成本。Spring Data Common 里面的 repository 基类，我们是否可以应用推广到 service 层呢？能否也建立一个自己的 baseService？我们来看下面的实战例子： 123456789101112131415161718192021222324252627public interface BaseService&lt;T, ID&gt; &#123; Class&lt;T&gt; getDomainClass(); &lt;S extends T&gt; S save(S entity); &lt;S extends T&gt; List&lt;S&gt; saveAll(Iterable&lt;S&gt; entities); void delete(T entity); void deleteById(ID id); void deleteAll(); void deleteAll(Iterable&lt;? extends T&gt; entities); void deleteInBatch(Iterable&lt;T&gt; entities); void deleteAllInBatch(); T getOne(ID id); &lt;S extends T&gt; Optional&lt;S&gt; findOne(Example&lt;S&gt; example); Optional&lt;T&gt; findById(ID id); List&lt;T&gt; findAll(); List&lt;T&gt; findAll(Sort sort); Page&lt;T&gt; findAll(Pageable pageable); &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; example); &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; example, Sort sort); &lt;S extends T&gt; Page&lt;S&gt; findAll(Example&lt;S&gt; example, Pageable pageable); List&lt;T&gt; findAllById(Iterable&lt;ID&gt; ids); long count(); &lt;S extends T&gt; long count(Example&lt;S&gt; example); &lt;S extends T&gt; boolean exists(Example&lt;S&gt; example); boolean existsById(ID id); void flush(); &lt;S extends T&gt; S saveAndFlush(S entity);&#125; 我们模仿 JpaRepository 接口也自定义了一个自己的 BaseService，声明了常用的 CRUD操作。当然了我们也可以建立自己的 PagingAndSortingService、ComplexityService、SampleService 等来划分不同的 service接口，供不同目的 Service 子类继承。 我们再来模仿一个 SimpleJpaRepository，来实现自己的 BaseService 的实现类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116public class BaseServiceImpl&lt;T, ID, R extends JpaRepository&lt;T, ID&gt;&gt; implements BaseService&lt;T, ID&gt; &#123; private static final Map&lt;Class, Class&gt; DOMAIN_CLASS_CACHE = new ConcurrentHashMap&lt;&gt;(); private final R repository; public BaseServiceImpl(R repository) &#123; this.repository = repository; &#125; @Override public Class&lt;T&gt; getDomainClass() &#123; Class thisClass = getClass(); Class&lt;T&gt; domainClass = DOMAIN_CLASS_CACHE.get(thisClass); if (Objects.isNull(domainClass)) &#123; domainClass = GenericsUtils.getGenericClass(thisClass, 0); DOMAIN_CLASS_CACHE.putIfAbsent(thisClass, domainClass); &#125; return domainClass; &#125; protected R getRepository() &#123; return repository; &#125; @Override public &lt;S extends T&gt; S save(S entity) &#123; return repository.save(entity); &#125; @Override public &lt;S extends T&gt; List&lt;S&gt; saveAll(Iterable&lt;S&gt; entities) &#123; return repository.saveAll(entities); &#125; @Override public void delete(T entity) &#123; repository.delete(entity); &#125; @Override public void deleteById(ID id) &#123; repository.deleteById(id); &#125; @Override public void deleteAll() &#123; repository.deleteAll(); &#125; @Override public void deleteAll(Iterable&lt;? extends T&gt; entities) &#123; repository.deleteAll(entities); &#125; @Override public void deleteInBatch(Iterable&lt;T&gt; entities) &#123; repository.deleteInBatch(entities); &#125; @Override public void deleteAllInBatch() &#123; repository.deleteAllInBatch(); &#125; @Override public T getOne(ID id) &#123; return repository.getOne(id); &#125; @Override public &lt;S extends T&gt; Optional&lt;S&gt; findOne(Example&lt;S&gt; example) &#123; return repository.findOne(example); &#125; @Override public Optional&lt;T&gt; findById(ID id) &#123; return repository.findById(id); &#125; @Override public List&lt;T&gt; findAll() &#123; return repository.findAll(); &#125; @Override public List&lt;T&gt; findAll(Sort sort) &#123; return repository.findAll(sort); &#125; @Override public Page&lt;T&gt; findAll(Pageable pageable) &#123; return repository.findAll(pageable); &#125; @Override public &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; example) &#123; return repository.findAll(example); &#125; @Override public &lt;S extends T&gt; List&lt;S&gt; findAll(Example&lt;S&gt; example, Sort sort) &#123; return repository.findAll(example, sort); &#125; @Override public &lt;S extends T&gt; Page&lt;S&gt; findAll(Example&lt;S&gt; example, Pageable pageable) &#123; return repository.findAll(example, pageable); &#125; @Override public List&lt;T&gt; findAllById(Iterable&lt;ID&gt; ids) &#123; return repository.findAllById(ids); &#125; @Override public long count() &#123; return repository.count(); &#125; @Override public &lt;S extends T&gt; long count(Example&lt;S&gt; example) &#123; return repository.count(example); &#125; @Override public &lt;S extends T&gt; boolean exists(Example&lt;S&gt; example) &#123; return repository.exists(example); &#125; @Override public boolean existsById(ID id) &#123; return repository.existsById(id); &#125; @Override public void flush() &#123; repository.flush(); &#125; @Override public &lt;S extends T&gt; S saveAndFlush(S entity) &#123; return repository.saveAndFlush(entity); &#125;&#125; 以上代码就是 BaseService 常用的 CURL 实现代码，我们这里面大部分也是直接调用 Repository 提供的方法。需要注意的是，当继承 BaseServiceImpl 的时候需要传递自己的 Repository，如下面实例代码： 1234567@Servicepublic class UserServiceImpl extends BaseServiceImpl&lt;User, Long, UserRepository&gt; implements UserService &#123; public UserServiceImpl(UserRepository repository) &#123; super(repository); &#125; .....&#125;]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>Defining Query Methods</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PlantUML语法]]></title>
    <url>%2F2020%2F09%2F21%2FPlantUML%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[PlantUML语法安装官方站点 需要 java 环境。 需要安装 Graphviz 依赖 下载 plantuml 使用 IDEA 来创建和预览 下载 IDEA 编辑器 打开 Plugins 搜索 “plantuml”，选中 PlantUML integration，点击安装即可 配置 PlantUML 脚本语法puml 是脚本语言，是按照换行来识别语法的。 一般的表达语句为: 节点 连接符号 节点 关键字 节点或内容 123开始关键字 内容结束关键字 注意空格等非字符作为节点内容的使用，有些多关键字语句中需要使用双引号来包含带空格的语句 活动图 ( Activity Diagram )Simple Activity语法是 : 节点 ; 结束 12345@startumltitle act_new_1:步骤一;:此处有个支持简单的 mark 语法 **加粗字体**;@enduml Start / Stop 开始和结束开始位置为 start 开始，stop 停止，end 结束 1234567@startumltitle act_new_2start:步骤一;:mark 语法 **加粗字体**;stop@enduml Conditional 条件语句控制语句的使用，注意内容 () 的使用 1234567891011@startumltitle act_new_3startif (是否已下载 graphviz?) then (是) :执行 \ndiagrams;else (否) :需要下载 graphviz 后才能执行 __activity__ diagrams;endifstop@enduml Repeat loop 循环处理（控制语句）使用关键字 repeat，repeat while () 来实现 123456789@startumltitle act_new_4startrepeat :读取文件; :编译 diagrams;repeat while (更多，执行下一个?)stop@enduml While loop 循环控制使用 while endwhile 循环控制 123456789@startumltitle act_new_5startwhile (执行条件允许文件存在?) :读取文件; :编译 diagrams;endwhilestop@enduml 可以追加标签 1234567891011@startumltitle act_new_6start:打开文件;while (检查文件大小 ?) is (有内容) :读取文件; :编译 dia;endwhile (空):关闭文件;end@enduml Parallel processing 并行处理使用 fork 关键字创建并行处理 1234567891011121314@startumltitle act_new_7startif (multiprocessor?) then (yes) fork :Treatment 1; fork again :Treatment 2; end forkelse (monoproc) :Treatment 1; :Treatment 2;endif@enduml Notes 注释 单行注释使用 note right|left 内容，注意后面没有分号 多行注释使用 note right|left 换行 内容 换行 end note的方式处理 12345678910111213141516@startumltitle act_new_8start: 步骤1;note left: This is a __NOTE__: 步骤2;note right 支持 mark 编辑语法 This note is on several //lines// and can contain &lt;b&gt;HTML&lt;/b&gt; ==== * Calling the method &quot;&quot;foo()&quot;&quot; is prohibitedend notestop@enduml 注意 mark 并非 Markdown 语法，以案例为 b Color 颜色在冒号钱可以追加 #十六进制 的颜色编号或者名称 123456789@startumltitle act_new_9start:进程开始;#red:试着读取文件文件必须有可写权限!;#AAAAAA:结束进程;stop@enduml Complete example 案例1234567891011121314151617181920212223242526272829303132@startumltitle act_new_10start:ClickServlet.handleRequest();:new page;if (Page.onSecurityCheck) then (true) #7c7c7c:Page.onInit(); if (isForward?) then (no) #red:Process controls; if (continue processing?) then (no) stop endif if (isPost?) then (yes) :Page.onPost(); else (no) :Page.onGet(); endif :Page.onRender(); endifelse (false)endifif (do redirect?) then (yes) :redirect process;else if (do forward?) then (yes) :Forward request; else (no) :Render page template; endifendifstop@enduml 时序图 ( Sequence Diagram )基本语法 使用 节点 -&gt; 节点: 描述 -&gt; 为实线 如果使用虚线则 用 --&gt; 箭头的方向可以是双向的 节点 &lt;-- 节点 1234567@startumltitle seq_1用户 -&gt; 平台: 发送登录请求平台 --&gt; 用户: 验证通过用户 -&gt; 平台: 另一个请求用户 &lt;-- 平台: 返回验证消息@enduml Comments 注释内部注释使用 &#39; 单引号来开始 单行注释， 多行注释则使用 /&#39; 和 &#39;/ 包括 Declaring participant （角色）提供了的声明参加者 (角色)，来丰富图形，默认情况下在箭头两边的已经隐式声明了( participant )，也可以使用 participant 节点 来显示声明角色，这种状况主要是为了声明角色的顺序。 actor boundary control entity database 123456789101112@startumltitle seq_2actor Foo1boundary Foo2control Foo3entity Foo4database Foo5Foo1 -&gt; Foo2 : To boundaryFoo1 -&gt; Foo3 : To controlFoo1 -&gt; Foo4 : To entityFoo1 -&gt; Foo5 : To database@enduml 追加色彩案例 1234567891011121314151617@startumltitle seq_3actor 客户 #red&apos; 声明角色的时候在后面可以追加颜色&apos; 注释内容并不会显示&apos; The only difference between actor&apos; and participant is the drawingparticipant 服务端participant &quot;I have a really\nlong name&quot; as L #99FF99/&apos; 这里使用 `as` 关键字声明了 L 来作为别名 你也可以这样写: participant L as &quot;I have a really\nlong name&quot; #99FF99 &apos;/服务端-&gt;客户: 请求认证客户-&gt;服务端: Authentication Response客户-&gt;L: Log transaction@enduml Use non-letters in participants 非字符处理在声明非字母的角色的时候使用双引号处理 (包含空格) 12345678@startumltitle seq_4服务端 -&gt; &quot;客户()&quot; : Hello&quot;客户()&quot; -&gt; &quot;This is very\nlong&quot; as Long&apos; You can also declare:&apos; &quot;客户()&quot; -&gt; Long as &quot;This is very\nlong&quot;Long --&gt; &quot;客户()&quot; : ok@enduml Message to Self在介绍过长的内容的时候使用 \n 来换行 1234@startumltitle seq_5服务端-&gt;服务端: This is a signal to self.\nIt also demonstrates\nmultiline \ntext@enduml Change arrow style 箭头样式在时序图中可以使用多种样式，基于下面的案例，来替换默认的箭头 总体来说 - 表示实线， -- 标识虚线而在在两边的标识箭头的方向和样式 123456789101112@startumltitle seq_6客户 -&gt; 服务端客户 -&gt;&gt; 服务端客户 -\ 服务端客户 \\- 服务端客户 //-- 服务端客户 -&gt;o 服务端客户 o\\-- 服务端客户 &lt;-&gt; 服务端客户 &lt;-&gt;o 服务端@enduml Change arrow color 箭头色彩使用关键字 autonumber 来自动为消息生成索引数字 123456@startumltitle seq_8autonumber客户 -&gt; 服务端 : 请求认证客户 &lt;- 服务端 : Authentication Response@enduml 在 autonumber 后可以设定 开始位置 梯度 参数在默认情况下 autonumber 自动追加了 start 参数， 在使用 stop 参数可以停止计数 1234567891011121314@startumltitle seq_9autonumber客户 -&gt; 服务端 : 请求认证客户 &lt;- 服务端 : Authentication Responseautonumber stop客户 &lt;- 服务端 : 停止autonumber 15客户 -&gt; 服务端 : Another 请求认证客户 &lt;- 服务端 : Another authentication Responseautonumber 40 10客户 -&gt; 服务端 : Yet another 请求认证客户 &lt;- 服务端 : Yet another authentication Response@enduml 你也可以对数字进行格式化输出，为 autonumber 追加格式化使用单引号 &quot;格式&quot; ，内部可以使用 HTML 标记，可以参看后面的 Formatting using HTML 123456789101112@startumltitle seq_10autonumber &quot;&lt;b&gt;[000]&lt;/b&gt;&quot;客户 -&gt; 服务端 : 请求认证客户 &lt;- 服务端 : Authentication Responseautonumber 15 &quot;&lt;b&gt;(&lt;u&gt;###&lt;/u&gt;)&quot;客户 -&gt; 服务端 : Another 请求认证客户 &lt;- 服务端 : Another authentication Responseautonumber 40 10 &quot;&lt;font color=red&gt;&lt;b&gt;Message 0 &quot;客户 -&gt; 服务端 : Yet another 请求认证客户 &lt;- 服务端 : Yet another authentication Response@enduml Splitting diagrams 分割为多个图形关键字 newpage 可以将同一张图形分割为多个图，可以追加 标题参数 1234567891011@startumltitle seq_11服务端 -&gt; 客户 : message 1服务端 -&gt; 客户 : message 2newpage服务端 -&gt; 客户 : message 3服务端 -&gt; 客户 : message 4newpage 第三章的标题\n最后一页服务端 -&gt; 客户 : message 5服务端 -&gt; 客户 : message 6@enduml Grouping message 分组消息提供了一下几组关键词 alt/else opt loop par break critical group, followed by a text to be displayed 关键词后可以追加文本，来作为副标题使用 end 关键词来表示结束 123456789101112131415161718@startumltitle seq_12服务端 -&gt; 客户: 请求认证alt 成功 客户 -&gt; 服务端: 许可认证else 可识别的正常失败 客户 -&gt; 服务端: 认证失败 group 组名称 服务端 -&gt; 日志: attack start loop 1000 times 服务端 -&gt; 客户: DNS Attack end 服务端 -&gt; 日志: Log attack end endelse 其他失败 客户 -&gt; 服务端: 重新请求end@enduml Notes on messages单行注释12note right: 右侧的内容note left: 右侧的内容 多行注释123note right:多行内容end note 12345678910111213@startumltitle seq_13服务端-&gt;客户 : hellonote left: this is a first note客户-&gt;服务端 : oknote right: this is another note客户-&gt;客户 : I am thinkingnote left a note can also be defined on several linesend note@enduml Some other notesof，over 后跟上角色（参与者），可以管理，其中 over 可以覆盖在线上 1234567891011121314151617@startumltitle seq_14participant 服务端participant 客户note left of 服务端 #aqua 显示 左侧 of 服务端.end notenote right of 服务端: 显示在右侧 of 服务端.note over 服务端: over 注释覆盖在服务端.note over 服务端, 客户 #FFAAAA: 注释\n 覆盖在 客户 和 服务端.note over 客户, 服务端 This is yet another example of a long note.end note@enduml Formatting using HTML使用类 html-tag： &lt;b&gt; 来加粗字体 &lt;u&gt; or &lt;u:#AAAAAA&gt; or &lt;u:colorName&gt; 输出下划线 &lt;i&gt; 斜体字输出 &lt;s&gt; or &lt;s:#AAAAAA&gt; or &lt;s:colorName&gt; for strike text &lt;w&gt; or&lt;w:#AAAAAA&gt;or&lt;w:colorName&gt;` for wave underline text &lt;color:#AAAAAA&gt; or &lt;color:colorName&gt; &lt;back:#AAAAAA&gt; or &lt;back:colorName&gt; for background color `size:nn to change font size &lt;img src=&quot;file&quot;&gt; or &lt;img:file&gt; : the file must be accessible by the filesystem &lt;img src=&quot;http://url&quot;&gt; or &lt;img:http://url&gt; : the URL must be available from the Internet example 1234567891011121314151617@startumltitle seq_15participant 服务端participant &quot;The &lt;b&gt;Famous&lt;/b&gt; 客户&quot; as 客户服务端 -&gt; 客户 : A &lt;i&gt;well formated&lt;/i&gt; messagenote right of 服务端 This is &lt;back:cadetblue&gt;&lt;size:18&gt;displayed&lt;/size&gt;&lt;/back&gt; &lt;u&gt;left of&lt;/u&gt; 服务端.end notenote left of 客户 &lt;u:red&gt;This&lt;/u&gt; is &lt;color #118888&gt;displayed&lt;/color&gt; &lt;b&gt;&lt;color purple&gt;left of&lt;/color&gt; &lt;s:red&gt;服务端&lt;/strike&gt; 客户&lt;/b&gt;.end notenote over 服务端, 客户 &lt;w:#FF33FF&gt;This is hosted&lt;/w&gt; by &lt;img http://s.plantuml.com/logoc.png&gt;end note@enduml DividerIf you want, you can split a diagram using == separator to divide your diagram into logical steps 123456789@startumltitle seq_16== Initialisation ==服务端 -&gt; 客户: 请求认证客户 --&gt; 服务端: Authentication Response== Repetition ==服务端 -&gt; 客户: Another 请求认证服务端 &lt;-- 客户: another authentication Response@enduml ReferenceYou can use reference in a diagram, using the keyword ref over 1234567891011@startumltitle seq_17participant 服务端actor 客户ref over 服务端, 客户 : init服务端 -&gt; 客户 : helloref over 客户 This can be on several linesend ref@enduml DelayYou can use … to indicate a delay in the diagram. And it is also possible to put a message with this delay. 12345678@startumltitle seq_18服务端 -&gt; 客户: 请求认证...客户 --&gt; 服务端: Authentication Response...5 minutes latter...客户 --&gt; 服务端: Bye !@enduml Space对消息使用空行 |||，也可以自己定义像素高度 ||数字|| 1234567891011@startumltitle seq_19服务端 -&gt; 客户: message 1客户 --&gt; 服务端: ok|||服务端 -&gt; 客户: message 2客户 --&gt; 服务端: ok||45||服务端 -&gt; 客户: message 3客户 --&gt; 服务端: ok@enduml Lifeline Activation and Destruction 生命周期使用 activate 角色 表示开始生命周期，使用 destory 角色 标识结束周期 开始和结束必须成对的存在 12345678910111213141516@startumltitle seq_20participant UserUser -&gt; A: DoWorkactivate AA -&gt; B: &lt;&lt; createRequest &gt;&gt;activate BB -&gt; C: DoWorkactivate CC --&gt; B: WorkDonedestroy CB --&gt; A: RequestCreateddeactivate BA -&gt; User: Donedeactivate A@enduml 为进程的生命周期进行标记色彩 ，可以追加色彩参数， 在 plantuml 中色彩使用 #十六进制的色彩值 来表示 123456789101112131415@startumltitle seq_21participant UserUser -&gt; A: DoWorkactivate A #FFBBBBA -&gt; A: Internal callactivate A #DarkSalmonA -&gt; B: &lt;&lt; createRequest &gt;&gt;activate BB --&gt; A: RequestCreateddeactivate Bdeactivate AA -&gt; User: Donedeactivate A@enduml Participant creationYou can use the create keyword just before the first reception of a message to emphasize the fact that this message is actually creating this new object 123456789@startuml客户 -&gt; 服务端 : hellocreate Other服务端 -&gt; Other : newcreate control String服务端 -&gt; Stringnote right : You can also put notes!服务端 --&gt; 客户 : ok@enduml Incoming and outgoing messagesYou can use incoming or outgoing arrows if you want to focus on a part of the diagram. Use square brackets to denotate the left “[“ or the right “]” side of the diagram. 1234567891011@startuml[-&gt; A: DoWorkactivate AA -&gt; A: Internal callactivate AA -&gt;] : &lt;&lt; createRequest &gt;&gt;A&lt;--] : RequestCreateddeactivate A[&lt;- A: Donedeactivate A@enduml Stereotypes and SpotsIt is possible to add stereotypes to participants using &lt;&lt; and &gt;&gt;. In the stereotype, you can add a spotted character in a colored circle using the syntax (X,color) 12345@startumlparticipant &quot;Famous 客户&quot; as 客户 &lt;&lt; Generated &gt;&gt;participant 服务端 &lt;&lt; (C,#ADD1B2) Testable &gt;&gt;客户-&gt;服务端: First message@enduml 或者 12345@startumlparticipant 客户 &lt;&lt; (C,#ADD1B2) &gt;&gt;participant 服务端 &lt;&lt; (C,#ADD1B2) &gt;&gt;客户-&gt;服务端: First message@enduml More information on titlesYou can use some HTML tags in the title. 12345@startumltitle &lt;u&gt;Simple&lt;/u&gt; communication example服务端 -&gt; 客户: 请求认证客户 -&gt; 服务端: Authentication Response@enduml You can add newline using \n in the title description. 12345@startumltitle &lt;u&gt;Simple&lt;/u&gt; communication example\non several lines服务端 -&gt; 客户: 请求认证客户 -&gt; 服务端: Authentication Response@enduml You can also define title on several lines using title and end title keywords. 123456789101112@startumltitle &lt;u&gt;Simple&lt;/u&gt; communication example on &lt;i&gt;several&lt;/i&gt; lines and using &lt;font color=red&gt;html&lt;/font&gt; This is hosted by &lt;img:sourceforge.jpg&gt;end title服务端 -&gt; 客户: 请求认证客户 -&gt; 服务端: Authentication Response@enduml 7.22. Participants engloberIt is possible to draw a box arround some participants, using box and end box commands. You can add an optional title or a optional background color, after the box keyword. 123456789@startumlbox &quot;Internal Service&quot; #LightBlue participant 客户 participant 服务端end boxparticipant Other客户 -&gt; 服务端 : hello服务端 -&gt; Other : hello@enduml 7.23. Removing FooterYou can use the hide footbox keywords to remove the footer of the diagram. 12345@startumlhide footboxtitle Footer removed服务端 -&gt; 客户: 请求认证客户 --&gt; 服务端: Authentication Response @enduml Skinparam You can use the skinparam command to change colors and fonts for the drawing. You can use this command : In the diagram definition, like any other commands, In an included file, In a configuration file, provided in the command line or the ANT task. 1234567891011121314151617181920212223242526272829303132333435363738394041424344@startumlskinparam backgroundColor #EEEBDCskinparam sequence &#123; ArrowColor DeepSkyBlue ActorBorderColor DeepSkyBlue LifeLineBorderColor blue LifeLineBackgroundColor #A9DCDF ParticipantBorderColor DeepSkyBlue ParticipantBackgroundColor DodgerBlue ParticipantFontName Impact ParticipantFontSize 17 ParticipantFontColor #A9DCDF ActorBackgroundColor aqua ActorFontColor DeepSkyBlue ActorFontSize 17 ActorFontName Aapex&#125;actor Userparticipant &quot;First Class&quot; as Aparticipant &quot;Second Class&quot; as Bparticipant &quot;Last Class&quot; as CUser -&gt; A: DoWorkactivate AA -&gt; B: Create Requestactivate BB -&gt; C: DoWorkactivate CC --&gt; B: WorkDonedestroy CB --&gt; A: Request Createddeactivate BA --&gt; User: Donedeactivate A@enduml 参考资料PlantUML语法 PlantUML语法]]></content>
      <categories>
        <category>UML</category>
      </categories>
      <tags>
        <tag>UML</tag>
        <tag>plantuml</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统一建模语言(UML)]]></title>
    <url>%2F2020%2F09%2F19%2FUML%2F</url>
    <content type="text"><![CDATA[统一建模语言(UML)什么是模型模型是一个系统的完整的抽象。人们对某个领域特定问题的求解及解决方案，对它们的理解和认识都蕴含在模型中。通常，开发一个计算机系统是为了解决某个领域特定问题，问题的求解过程，就是从领域问题到计算机系统的映射。 为什么要建造模型 建造传统模型的目的 为了证明某件事物能否工作 前提：建造模型的成本远远低于建造实物的成本 造飞机 造高楼 建造软件模型的目的 为了与他人沟通 为了保存软件设计的最终成果 前提：除非模型比代码更说明问题 通用模型元素模型元素是UML构造系统的各种元素，是UML构建模型的基本单位。 基元素是由UML定义的模型元素，如：类、结点、构件、注释、关联、依赖和泛化(继承)等 关联关系分类： 泛化(继承) - 类与类，接口与接口的关系 实现 - 是一种类与接口的关系 组合 - 是整体与部分，是“has-a”关系，但部分不能离开整体而单独存在 公司和部门是整体和部分的关系，没有公司就不存在部门 也是一种关联关系，比聚合更强关联 聚合 - 是整体与部分，是“contains-a”关系，且部分可以离开整体而单独存在 车和轮胎是整体和部分的关系，轮胎离开车仍然可以存在 也是一种关联关系，是强关联 关联 - 是一种拥有的关系，它使一个类知道另一个类的属性和方法， [代码表现] 成员变量 依赖 - 是一种使用的关系，即一个类的实现需要另一个类的协助，尽量不要双向依赖 [代码表现] 局部变量、方法的参数或者对静态方法的调用 各种关系的强弱顺序： 泛化 = 实现 &gt; 组合 &gt; 聚合 &gt; 关联 &gt; 依赖 构造型元素在基元素的基础上增加了新的定义而构造的新的模型元素，允许用户自定义 构造型元素用括在双尖括号《 》中的字符串表示 目前UML提供了40多个预定义的构造型元素，如包含《include》、扩展《 Extend 》 什么是UML是一种对软件系统(架构)进行==可视化建模==的语言，以图形的方式描述了软件的概念。 UML有什么作用 描述某个问题领域 描述构思中的软件设计 描述已经完成的软件实现 UML图的分类静态图 通过描述类、对象和数据结构以及它们之间存在的关系，来描述软件要素中不变的逻辑结构。 用例图（Use Case Diagrams）对象图（Object Diagrams）类图（Class Diagrams）组件图（Component Diagrams）包图（Package Diagrams）部署图（Deployment Diagrams）动态图 通过描绘执行流程或者实体状态变化的方式，来展示软件实体在执行过程中的变化过程。 协作图（Collaboration Diagrams）序列图（Sequence Diagrams）活动图（Activity Diagrams）状态图（State Diagrams）]]></content>
      <categories>
        <category>架构</category>
        <category>UML</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>UML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面向对象设计原则]]></title>
    <url>%2F2020%2F09%2F16%2F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[面向对象设计原则面向对象设计（OOD：Object Oriented Design）的六大设计原则 缩写 英文名称 中文名称 SRP Single Responsibility Principle 单一职责原则 OCP Open Close Principle 开闭原则 LSP Liskov Substitution Principle 里氏替换原则 ISP Interface Segregation Principle 接口分离原则 DIP Dependency Inversion Principle 依赖倒置原则 LoD Law of Demeter （ Least Knowledge Principle） 迪米特法则（最少知道原则） 前五个设计原则就是通常所说的SOLID（上方表格缩写的首字母，从上到下） 开闭原则 ( OCP )定义 📌 Software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification. 即：一个软件实体如类、模块和函数等应该对扩展开放，对修改关闭。 不需要修改软件实体（类、模块、函数等），就应该能实现功能的扩展 优点可以在不改动原有代码的前提下给程序扩展功能。增加了程序的可扩展性，同时也降低了程序的维护成本。 实现的方法关键是抽象，通过实现预先设置好的抽象的接口（抽象类）来实现新的功能。 策略模式 ( Strategy Pattern )定义该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。 优点 使用策略模式可以避免使用多重条件语句 策略模式提供了一系列的可供重用的算法族，恰当使用继承可以把算法族的公共代码转移到父类里面，从而避免重复的代码 策略模式可以提供相同行为的不同实现，客户可以根据不同时间或空间要求选择不同的 策略模式提供了对开闭原则的完美支持，可以在不修改原代码的情况下，灵活增加新算法 策略模式把算法的使用放到环境类中，而算法的实现移到具体策略类中，实现了二者的分离 缺点 客户端必须理解所有策略算法的区别，以便适时选择恰当的算法类 策略模式造成很多的策略类 实现继承实现定义一个抽象类，强制子类实现该方法 这种实现，对父类对子类不一定是透明的，如果父类提供了默认的实现，子类需要了解实现的细节，再决定是否重写 123456789101112131415161718public abstract class Action &#123; // 也可以将该方法设置成抽象方法， 强迫子类来实现该方法 public void execute() &#123; // 提供一个默认的实现 &#125;&#125;public class ActionModelA extends Action &#123; public void execute() &#123; aStyleBrake();// A 风格的行为 &#125;&#125;public class ActionModelB extends Action &#123; public void execute() &#123; bStyleBrake(); // B 风格的行为 &#125;&#125; 组合实现这种实现是透明的，只需要改变对象的引用就可以改变其行为。 123456789101112131415161718192021222324252627public interface IBrakeBehavior &#123; void execute();&#125;public class AStyleBrake implements IBrakeBehavior &#123; public void execute() &#123; aStyleBrake(); // A 风格的行为 &#125;&#125;public class BStyleBrake implements IBrakeBehavior &#123; public void execute() &#123; bStyleBrake(); // B 风格的行为 &#125;&#125;public class Action &#123; protected IBrakeBehavior brakeBehavior; public void execute() &#123; brakeBehavior.execute(); &#125; public void setBrakeBehavior(final IBrakeBehavior brakeType) &#123; this.brakeBehavior = brakeType; &#125;&#125; 最后通过工厂类，根据运行的参数选择出对应的实现 1Action action = ActionFactory.createAction(String parameters); action.execute(); 只有在策略选择里有条件选择语句，其他地方不出现。 如上述的createAction()方法 适配器模式 ( Adapter Pattern )定义将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。分为两种模式： 类适配器模式：适配器与适配者之间是继承（或实现）关系 对象适配器模式：适配器与适配者之间是关联关系 前者的耦合度比后者高，且要求开发了解现有组件库中的相关组件的内部结构，应用相对较少些。 优点 将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，而无须修改原有代码。 增加了类的透明性和复用性，将具体的实现封装在适配者类中，对于客户端类来说是透明的，而且提高了适配者的复用性。 灵活性和扩展性都非常好，通过使用配置文件，可以很方便地更换适配器，也可以在不修改原有代码的基础上增加新的适配器类，完全符合“开闭原则”。 类适配器模式优点： 由于适配器类是适配者类的子类，因此可以在适配器类中置换一些适配者的方法，使得适配器的灵活性更强。 对象适配器模式优点： 同一个适配器可以把适配者类和它的子类都适配到目标接口。 缺点类适配器模式缺点： 对于 Java 等不支持多重继承的语言，一次最多只能适配一个适配者类，而且目标抽象类只能为抽象类，不能为具体类，其使用有一定的局限性，不能将一个适配者类和它的子类都适配到目标接口。 对象适配器模式缺点： 与类适配器模式相比，要想置换适配者类的方法就不容易。如果一定要置换掉适配者类的一个或多个方法，就只好先做一个适配者类的子类，将适配者类的方法置换掉，然后再把适配者类的子类当做真正的适配者进行适配，实现过程较为复杂。 实现角色 Target（目标抽象类）：目标抽象类定义客户所需接口，可以是一个抽象类或接口，也可以是具体类。 Adapter（适配器类）：适配器可以调用另一个接口，作为一个转换器，对 Adaptee 和 Target 进行适配，适配器类是适配器模式的核心，在对象适配器中，它通过继承 Target 并关联一个 Adaptee 对象使二者产生联系。 Adaptee（适配者类）：适配者即被适配的角色，它定义了一个已经存在的接口，这个接口需要适配，适配者类一般是一个具体类，包含了客户希望使用的业务方法，在某些情况下可能没有适配者类的源代码。 类适配器首先有一个已存在的将被适配的类： 12345public class Adaptee &#123; public void adapteeRequest() &#123; System.out.println("被适配者的方法"); &#125;&#125; 定义一个目标接口： 123public interface Target &#123; void request();&#125; 如果通过一个适配器类，实现 Target 接口，同时继承了 Adaptee 类，然后在实现的 request() 方法中调用父类的 adapteeRequest() 即可实现 12345678public class Adapter extends Adaptee implements Target&#123; @Override public void request() &#123; //...一些操作... super.adapteeRequest(); //...一些操作... &#125;&#125; 对象适配器对象适配器与类适配器不同之处在于，类适配器通过继承来完成适配，对象适配器则是通过关联来完成，这里稍微修改一下 Adapter 类即可将转变为对象适配器 1234567891011public class Adapter implements Target&#123; // 适配者是对象适配器的一个属性 private Adaptee adaptee = new Adaptee(); @Override public void request() &#123; //... adaptee.adapteeRequest(); //... &#125;&#125; 观察者模式 （ Observer Pattern ）依赖倒置原则（ Dependency Inversion Principle ）依赖倒置原则定义 📌High level modules should not depend upon low level modules, Both should depend upon abstractions. Abstractions should not depend upon details. Details should depend upon abstractions 翻译过来： 高层模块不依赖低层模块，两者都应该依赖其抽象 抽象不应该依赖细节 细节应该依赖抽象 按照Java的理解，是面向接口编程 依赖倒置原则作用降低类间的耦合性类间的依赖都是通过接口完成的，某个类的实现变了，也不影响上游代码。 减少并行开发引起的风险各层级，模块间都是通过接口定义的约束，只要都遵循这些约束，能相应减少了并行开发的冲突。 依赖倒置的实现方法构造函数传递依赖对象1234567891011public class Driver &#123; private Car car; public Driver(Car car) &#123; this.car = car; &#125; public void drive() &#123; this.car.run(); &#125;&#125; Setter方法传递依赖对象1234567891011public class Driver &#123; private Car car; public void setCar(Car car) &#123; this.car = car; &#125; public void drive() &#123; this.car.run(); &#125;&#125; 接口声明依赖12345public class Driver &#123; public void drive(Car car) &#123; car.run(); &#125;&#125; 好莱坞原则 don’t call us, we’ll call you 因为在好莱坞，把简历递交给演艺公司后就只有回家等待。由演艺公司对整个娱乐项的完全控制，演员只能被动式的接受公司的差使，在需要的环节中，完成自己的演出。 为什么有时候依赖倒置原则又被称为好莱坞原则好莱坞有一个角色，找演员来演 好莱坞相当于高层，演员们相当于低层，他们间的联系是通过某个角色， 这个角色由哪个演员来演都可以，并不依赖于具体某个演员， 整个流程有点像面向接口编程。 📌两个原则强调的点应该是不一样的，查到的资料给我的感觉是： 好莱坞原则：强调的是高层对底层的主动调用 依赖倒置原则：强调的是面向接口编程 有点类似，但又不全是。 里氏替换原则（）单一职责原则（）接口隔离原则（）]]></content>
      <categories>
        <category>架构</category>
        <category>面向对象设计原则</category>
      </categories>
      <tags>
        <tag>面向对象设计原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库索引]]></title>
    <url>%2F2020%2F09%2F14%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[数据库索引什么是索引？索引的类型数据结构角度 B-Tree 索引 hash 索引 Full-test 索引 R-Tree 索引 物理存储角度cluster 聚簇索引（聚集索引）：物理索引，与基表的物理顺序相同，数据值的顺序总是按照顺序排列 CREATE CLUSTERED INDEX mycolumn_cindex ON mytable(mycolumn) WITH ALLOW_DUP_ROW(允许有重复记录的聚簇索引) 非聚簇索引（群集索引）：CREATE UNCLUSTERED INDEX mycolumn_cindex ON mytable(mycolumn) 逻辑角度 唯一索引 CREATE UNIQUE INDEX myclumn_uindex ON mytable(mycolumn) 主键索引（主索引）primary key (复)组合索引（多列索引）：CREATE INDEX mycolumn_index ON mytable (mycolumn1, mycolumn2) 普通索引（单列索引）：CREATE INDEX mycolumn_index ON mytable (mycolumn) 空间索引： 索引的优缺点优点 创建唯一性索引，保证数据库表中每一行数据的唯一性 大大加快数据的检索速度 – 最主要的原因 加速表和表之间的连接 在使用分组和排序子句进行数据检索时，可以显著减少查询中分组和排序的时间 通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能 缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加 索引需要占用数据表以外的物理存储空间，如果要建立聚簇索引，那么需要的空间就会更大 当对表中的数据进行增加、删除和修改的时候，索引需要被重建，降低了数据的维护速度 索引的使用 当字段的数据更新频率较低，查询使用频率较高并且存在大量重复值时，建议使用聚簇索引 经常同时存取多列，且每列都含有重复值可考虑建立组合索引 组合索引的前导列一定好控制好，否则无法起到索引的效果。如果查询时前导列不在查询条件中则该组合索引不会被使用，前导列一定是使用最频繁的列。 数据库优化(ORACLE)SQL使用索引的例子123456789INDEX_COLUMN = ?INDEX_COLUMN &gt; ?INDEX_COLUMN &gt;= ?INDEX_COLUMN &lt; ?INDEX_COLUMN &lt;= ?INDEX_COLUMN between ? and ?INDEX_COLUMN in (?,?,...,?)INDEX_COLUMN like ?||'%' --（后导模糊查询）T1.INDEX_COLUMN=T2.COLUMN1 --（两个表通过索引字段关联） SQL不使用索引的例子1234567891011121314151617181920-- 不等于操作不能使用索引INDEX_COLUMN &lt;&gt; ?INDEX_COLUMN not in (?,?,...,?)-- 经过普通运算或函数运算后的索引字段不能使用索引function(INDEX_COLUMN) = ?INDEX_COLUMN + 1 = ?INDEX_COLUMN || 'a' = ?-- 含前导模糊查询的Like语法不能使用索引INDEX_COLUMN like '%'||?INDEX_COLUMN like '%'||?||'%'-- B-TREE索引里不保存字段为NULL值记录，因此IS NULL不能使用索引INDEX_COLUMN is null-- Oracle在做数值比较时需要将两边的数据转换成同一种数据类型，如果两边数据类型不同时会对字段值隐式转换，相当于加了一层函数处理，所以不能使用索引。NUMBER_INDEX_COLUMN='12345'CHAR_INDEX_COLUMN=12345a.INDEX_COLUMN=a.COLUMN_1 常用优化方法（ORACLE）where子句中的连接顺序 WHERE 子句是从下往上执行，筛选掉内容最多的条件必须写在 WHERE 子句的末尾，尤其是主键ID=?这样的条件。 select子句中避免使用 ‘ * ‘ oracle解析过程中，会把*依次转换为所有列名，这个工作是通过查询数据字典来完成的， 这意味着将耗费更多的时间 避免在索引列上使用函数WHERE子句中，如果索引列是函数的一部分。优化器将不使用索引而使用全表扫描 避免在索引列上使用NOTNOT 会产生在和在索引列上使用函数相同的效果。 避免在索引列上使用IS NULL和IS NOT NULL避免在索引中使用任何可以为空的列，oracle 将无法使用该索引 对于单列索引，如果列包含空值，索引中将不存在此记录 对于复合索引，如果每个列都为空，索引中同样不存在此记录 注意通配符%的影响前导模糊查询的Like语法不能使用索引INDEX_COLUMN like &#39;%&#39;||? 后导模糊查询的Like语法可以使用索引INDEX_COLUMN like ?||&#39;%&#39; 避免改变索引列的类型在做数值比较时需要将两边的数据转换成同一种数据类型，如果两边数据类型不同时会对字段值隐式转换，相当于加了一层函数处理，导致不能使用索引。 用(UNION)UNION ALL替换OR (适用于索引列)通常情况下， 用UNION替换WHERE子句中的OR将会起到较好的效果，对索引列使用OR将造成全表扫描。 注意：以上规则只针对多个索引列有效。如果有column没有被索引，查询效率可能会因为你没有选择OR而降低 用UNION-ALL 替换UNION ( 如果有可能的话)当 SQL语句需要UNION两个查询结果集合时，这两个结果集合会以UNION-ALL的方式被合并， 然后在输出最终结果前进行排序。 如果用UNION ALL替代UNION， 这样排序就不是必要了， 效率就会因此得到提高。 注意：UNION ALL 将重复输出两个结果集合中相同记录 EXISTS替换DISTINCT带有DISTINCT、UNION、MINUS、INTERSECT的SQL语句会启动SQL引擎执行耗费资源的排序(SORT)功能。 DISTINCT 需要一次排序操作， 而其他的至少需要执行两次排序。 通常， 带有UNION、MINUS 、INTERSECT的SQL语句都可以用其他方式重写。 合理的使用EXISTS和INEXISTS：首先检查主查询，然后运行子查询直到它找到第一个匹配项，这就节省了时间。IN 子查询：首先系统先将主查询挂起，然后执行子查询，并将获得的结果列表存放在一个加了索引的临时表中。待子查询执行完毕，再执行主查询。这也就是使用EXISTS比使用IN通常查询速度快的原因。 12select * from A where exists(select 1 from B where A.id = B.id) ;select * from A where A.id in (select B.id from B) ; 当子查询的表大，exists 优于 in； 补充 EXISTS(subquery) 只返回 TRUE or FALSE，实际实行时会优化子查询的select清单，因此子查询里写 select * / select 1 / select ‘X’，没有任何区别 如果两个表的大小相当，exists 和 in 差别不大 无论哪个表大，not exists 都比 not in 要快，因为 not in 内外表都不走索引；而 not exists的子查询依然可以用表的索引]]></content>
      <categories>
        <category>索引</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String.join()]]></title>
    <url>%2F2020%2F09%2F10%2FStringJoiner%2F</url>
    <content type="text"><![CDATA[String.join() JDK8 重载了join方法 SpringJoiner构造函数详细信息public StringJoiner (CharSequence delimiter, CharSequence prefix，CharSequence suffix ) 参数： delimiter - 添加到的每个元素之间要使用的字符序列 prefix - 开头使用的字符序列 suffix - 最后使用的字符序列 如果 prefix，delimiter 或 suffix 是 null, 则抛出空指针异常。 举例：字符串&quot;[George:Sally:Fred]&quot;可以构造如下: 123StringJoiner sj = new StringJoiner(":", "[", "]");sj.add("George").add("Sally").add("Fred");String desiredString = sj.toString();]]></content>
      <categories>
        <category>JDK</category>
        <category>JDK8</category>
      </categories>
      <tags>
        <tag>String</tag>
        <tag>Join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Transaction]]></title>
    <url>%2F2020%2F09%2F08%2FSpring%20Transaction%2F</url>
    <content type="text"><![CDATA[只读事务设置 JDBC 指定只读事务的设置：connection.setReadOnly(true) Hibernate 只读事务的设置：session.setFlushMode(FlushMode.NEVER) Spring 只读事务设置： @Transactional(readOnly = true) &lt;tx:method name=&quot;search*&quot; read-only=&quot;true&quot; /&gt; read-only 并不是所有数据库都支持的，不同的数据库下会有不同的结果（例如：对 Mysql 生效，对 Oracle 不生效）。 设置了read-only 后，connection 都会被赋予 read-only（希望数据库驱动开启只读优化，不一定生效） ，不应该把 read-only作为打开只读事务的判断。 在ORM框架中，设置了read-only会赋予一些额外的优化，例如在 Hibernate 中，会被禁止 flush 等。 Oracle 11.2 的文档中表明Oracle JDBC Driver 不支持 read-only connections]]></content>
      <categories>
        <category>Transaction</category>
      </categories>
      <tags>
        <tag>Transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK 内置注解]]></title>
    <url>%2F2020%2F09%2F08%2FJDK%20%E5%86%85%E7%BD%AE%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[@SuppressWarnings范围：类、字段、方法、参数、构造方法，以及局部变量上。 作用：告诉编译器忽略指定的警告，不用在编译完成后出现警告信息。 参数： deprecation：忽略过时 rawtypes：忽略类型安全 unused：忽略不使用 unchecked：忽略安全检查 null：忽略空指针 all：忽略所有 例子：@SuppressWarnings(&quot;unchecked&quot;, &quot;deprecation&quot;)]]></content>
      <categories>
        <category>注解</category>
      </categories>
      <tags>
        <tag>注解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CAS]]></title>
    <url>%2F2020%2F09%2F06%2FCAS%E6%80%9D%E6%83%B3%2F</url>
    <content type="text"><![CDATA[CAS什么是CAS CAS 全称 compare and swap (比较并替换)，是用在线程并发算法里常见的一种思想 CAS 是原子操作，保证并发安全，但不保证并发同步 CAS原理CAS 就是将内存值更新为需要的值时，设置一个条件，内存值必须与期望值相同 期望值 E、内存值M、更新值U，当E == M的时候将M更新为U CAS应用 数据库常见的乐观锁实现方式，使用版本号version JAVA里的原子类 CAS优缺点优点用来实现非阻塞的轻量级的乐观锁，通过CPU指令实现，在资源竞争不激烈的情况下性能高 缺点 ABA 问题：线程C、D，线程D将A修改为B后又修改为A，此时C线程以为A没有改变过 JAVA的原子类AtomicStampedReference，通过控制变量值的版本来保证CAS的正确性（其他情况也可以通过增加版本号来解决ABA问题） 自旋时间过长(一直不能更新)，消耗CPU资源， 如果资源竞争激烈，多线程自旋长时间消耗资源]]></content>
      <categories>
        <category>锁</category>
        <category>乐观锁</category>
      </categories>
      <tags>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPA用法]]></title>
    <url>%2F2020%2F09%2F06%2FJPA%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[查询用法实体12345678910111213141516@Entity@Datapublic class Employee implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; // 拥有级联维护的一方，参考http://westerly-lzh.github.io/cn/2014/12/JPA-CascadeType-Explaining/ @OneToOne(cascade = CascadeType.ALL) @JoinColumn(foreignKey = @ForeignKey(name = "none", value = ConstraintMode.NO_CONSTRAINT)) private EmployeeDetail detail; @ManyToOne(fetch = FetchType.LAZY) // 设置外键的问题，参考http://mario1412.github.io/2016/06/27/JPA%E4%B8%AD%E5%B1%8F%E8%94%BD%E5%AE%9E%E4%BD%93%E9%97%B4%E5%A4%96%E9%94%AE/ @JoinColumn(name = "jobId", foreignKey = @ForeignKey(name = "none", value = ConstraintMode.NO_CONSTRAINT)) private Job job;&#125; 12345678910@Entity@Datapublic class EmployeeDetail implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String phone; private Integer age;&#125; 123456789101112131415@Entity@Datapublic class Job implements Serializable &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; // mappedBy 只有在双向关联的时候设置，表示关系维护的一端，否则会生成中间表A_B @OneToMany(targetEntity = Employee.class, mappedBy = "job") // 注意这里不能使用 @JoinColumn 中的 @ForeignKey 不然会生成外键 @org.hibernate.annotations.ForeignKey(name = "none") private Set&lt;Employee&gt; employees;&#125; Specification 复杂查询JPA复杂查询接口 JpaSpecificationExecutor 接口，可以完成各种复杂查询，配合 JAVA 8的新特性，使用起来特别的方便，一般用于单表的复杂查询。 1234567891011121314151617181920212223242526272829/* * @param search 查询属性 * @param pageable 分页和排序 * @return 分页数据 */@Overridepublic Page&lt;Employee&gt; pageBySearch(EmployeeSearch search, Pageable pageable) &#123; return employeeDao.findAll((Specification&lt;Employee&gt;) (root, criteriaQuery, criteriaBuilder) -&gt; &#123; List&lt;Predicate&gt; predicates = new LinkedList&lt;&gt;(); Optional&lt;EmployeeSearch&gt; optional = Optional.ofNullable(search); // 根据 employee id 查询 optional.map(EmployeeSearch::getEmployeeId).ifPresent(id -&gt; &#123; predicates.add(criteriaBuilder.equal(root.get(Employee_.id), id)); &#125;); // 根据 employee detail name 模糊查询 optional.map(EmployeeSearch::getEmployeeName).ifPresent(name -&gt; &#123; Join&lt;Employee, EmployeeDetail&gt; join = root.join(Employee_.detail, JoinType.LEFT); predicates.add(criteriaBuilder.like(join.get(EmployeeDetail_.name), "%" + name + "%")); &#125;); // 根据职位名查询 optional.map(EmployeeSearch::getJobName).ifPresent(name -&gt; &#123; Join&lt;Employee, Job&gt; join = root.join(Employee_.job, JoinType.LEFT); predicates.add(criteriaBuilder.equal(join.get(Job_.name), name)); &#125;); Predicate[] array = new Predicate[predicates.size()]; return criteriaBuilder.and(predicates.toArray(array)); &#125;, pageable);&#125; 注意： 针对有外键关联的表的查询条件，需要使用左联接；如上面的 employee detail name 模糊查询 如果直接 get(Employee_.detail).get(EmployeeDetail_.name) ，就是无条件内联，相当于 cross join，会产生笛卡尔积 Criteria 查询普通的查询返回结果类型：entity / Object[] / tuple / DTO 12345678910111213141516171819202122232425@PersistenceContextprivate EntityManager em;/** * Search age gt or eq the parameter * * @param age * @return */@Overridepublic List&lt;Employee&gt; listByAge(Integer age) &#123; CriteriaBuilder cb = em.getCriteriaBuilder(); CriteriaQuery&lt;Employee&gt; query = cb.createQuery(Employee.class); // 设置查询根，可以根据查询的类型设置不同的 就是 Form 语句 后面的 entity Root&lt;Employee&gt; root = query.from(Employee.class); List&lt;Predicate&gt; predicates = new LinkedList&lt;&gt;(); // 连表查询使用左连接 Join&lt;Employee, EmployeeDetail&gt; join = root.join(Employee_.detail, JoinType.LEFT); predicates.add(cb.gt(join.get(EmployeeDetail_.age), age)); predicates.add(cb.equal(join.get(EmployeeDetail_.age), age)); // 设置排序规则 query.orderBy(cb.asc(root.get(Employee_.id))); query.where(predicates.toArray(new Predicate[predicates.size()])); return em.createQuery(query).getResultList();&#125; 返回DTO对象因为不是一个Entity，则需要做一些特殊的操作 12345678@Getter@Setter@AllArgsConstructor@NoArgsConstructorpublic class EmployeeResult &#123; private String name; private Integer age;&#125; 1234567891011121314151617181920/** * 使用 构造函数 装载查询出来的数据 * * @return */@Overridepublic List&lt;EmployeeResult&gt; findEmployee() &#123; CriteriaBuilder cb = em.getCriteriaBuilder(); CriteriaQuery&lt;EmployeeResult&gt; query = cb.createQuery(EmployeeResult.class); // 设置查询根，可以根据查询的类型设置不同的 Root&lt;Employee&gt; root = query.from(Employee.class); Join&lt;Employee, EmployeeDetail&gt; join = root.join(Employee_.detail, JoinType.LEFT); // 使用构造函数 CriteriaBuilder.construct 来完成装载数据 query.select(cb.construct(EmployeeResult.class, join.get(EmployeeDetail_.name), join.get(EmployeeDetail_.age))); // 设置排序规则 Order order = cb.asc(root.get(Employee_.id)); query.orderBy(order); TypedQuery typedQuery = em.createQuery(query); // TypedQuery执行查询与获取元模型实例 return typedQuery.getResultList();&#125; query.select(CriteriaBuilder.construct()) 这种方式 就相当于 1234&gt; query.mutipleSelect(&gt; join.get(EmployeeDetail_.name), &gt; join.get(EmployeeDetail_.age));&gt; 而且顺序必须和DTO的构造方法的参数顺序一致才行 返回Object[]1criteriaQuery.select(criteriaBuilder.array(root.get(xxx),join.get(xxx))); 元模型查询元模型实例通过调用 EntityManager.getMetamodel 方法获得，EntityType&lt;Employee&gt;的元模型实例通过调用Metamodel.entity(Employee.class)而获得，其被传入 CriteriaQuery.from 获得查询根 123Metamodel metamodel = em.getMetamodel();EntityType&lt;Employee&gt; Employee_ = metamodel.entity(Employee.class);Root&lt;Employee&gt; empRoot = criteriaQuery.from(Employee_); Tuple查询123456789101112131415161718/** * 分组统计重名数量 * @param name * @return */@Overridepublic List&lt;Tuple&gt; groupByName(String name) &#123; CriteriaBuilder cb = em.getCriteriaBuilder(); CriteriaQuery&lt;Tuple&gt; query = cb.createTupleQuery(); Root&lt;Employee&gt; root = query.from(Employee.class); Join&lt;Employee, EmployeeDetail&gt; join = root.join(Employee_.detail, JoinType.LEFT); query.groupBy(join.get(EmployeeDetail_.name)); if (name != null) &#123; query.having(cb.like(join.get(EmployeeDetail_.name), "%" + name + "%")); &#125; query.select(cb.tuple(join.get(EmployeeDetail_.name), cb.count(root))); return em.createQuery(query).getResultList();&#125; 描述：对于BigDecimal这种对象在HQL里没有构造方法的。可以使用查询Object再强转 12345678public BigDecimal findRemainRefundableAmount(String orderNo) &#123; CriteriaBuilder builder = entityManager.getCriteriaBuilder(); // 返回值类型设置为Object CriteriaQuery&lt;Object&gt; query = builder.createQuery(Object.class); Root&lt;Account&gt; root = query.from(Account.class); // 查询的时候再进行强转 return (BigDecimal) entityManager.createQuery(query).getSingleResult(); &#125; 排序写法1：这种写法优点是，可以排其他表的字段（userRoot -&gt; accountRoot ） 1criteriaQuery.orderBy(builder.desc(userRoot.get("createTime"))); 写法2：这种优点，可以将直接将参数的Pageable里面的sort对象之间排序 1criteriaQuery.orderBy(QueryUtils.toOrders(pageable.getSort(), userRoot, builder)); 写法3：最常用的写法，对多个字段排序List&lt;Order&gt; 1criteriaQuery.orderBy(orderList); 更新参考资料JPA 使用 Specification 复杂查询和 Criteria 查询]]></content>
      <categories>
        <category>JPA</category>
      </categories>
      <tags>
        <tag>JPA</tag>
        <tag>Spring Data JPA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事务]]></title>
    <url>%2F2020%2F09%2F06%2F%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[事务什么是事务 所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，是一个不可分割的工作单位。 事务是数据库维护数据一致性的单位，在每个事务结束时，都能保持数据一致性。 事务的四个特性Atomic 原子性不同对象之间对于原子性定义不同： 事务性数据库(ACID语境下)：原子性是指一组对数据库的改变，要么最终成功执行完成，要不就全部回滚。这就要求数据库系统要实现某种==回滚的机制==，比如redo/undo log。 对于支持并发的编程语言(Java、C++等)：原子性是指一组指令被执行时，不受其他指令的干扰。比如”给一个整型变量赋值是原子的“等等。 一些NoSQL的数据库(Redis)： 事务的原子性的意思更接近于“一组指令被执行时，不受其他指令的干扰”，而不是“可以回滚”。 Consistency 一致性不同的环境下有着不同的含义： 多副本的一致性 一致性hash CAP理论的一致性 ACID里的一致性 ACID语境下的一致性AID 都是数据库的特征，也就是依赖数据库的具体实现。但是唯独C不一样，它需要依赖于应用层，也就是依赖于开发者的具体实现。 这里的一致性是指系统从一个正确的状态，迁移到另一个正确的状态。 正确的状态：就是当前的状态满足预定的约束（业务上的要求）就叫做正确的状态。 业务的数据一致性一般用正确的业务代码 + 定时任务 + 数据库自身的简单合法性防护（事务的AID、锁等）一起实现。 Isolation 隔离性一组对数据库的并发修改互相不影响。 并发修改的是互不相干的数据，本身就满足隔离的条件 并发修改的是相关联的，或者就是同一份数据，就必然会相互影响。 保证隔离性的主要问题不在于隔离本身：而在于如果将读取作为对数据修改的前提条件，之后在对数据进行修改的一刹那，读取时的前提条件还是否满足。毕竟读取和写入是两个分开的指令，而在这两个指令中间可能夹杂其他事务对数据的修改。（如：找到可用的库存，有则扣减，没有则提示缺货） 保持隔离性做法：保证对关联数据的修改串行化(SERIALIZABLE) ，如：加锁，但是锁对数据库并发的性能负面影响很大，所以出现了几种弱一些的隔离性保证 READ UNCOMMITTED READ COMMITTED REPEATABLE READ Durability 持久性是指对数据的修改，一旦完成，该结果就应当永远不丢失。 在现实当中， 一般通过持久性存储设备（比如磁盘/SSD）写入并刷新来保证数据的持久性。 如果觉得一个节点不靠谱，可以增加多个副本（replica）一起来保证持久； 如果觉得这样还不够靠谱，可以在不同的地理位置的另一个数据中心做备份。 实际上绝对的持久性是不存在的，因为整个存储层面有很多不确定因素，比如文件系统本身fsync指令实现有bug，磁盘的固件有bug，供电出现问题造成数据错乱，异步的数据复制没有生效等等。 所以在现实当中的数据库，只能在当前成本和技术限制的约束下，尽量维持一定程度的持久性。 总结：事务性数据库实现的是 支持未完成的数据修改回滚的机制，对应“原子性” 力所能及的数据合法性检查，对应“一致性” 保证数据并发的修改的规则，对应“隔离性” 使用基于持久化存储（磁盘、SSD）的方式对数据进行存储，对应“持久性” 事务隔离级别 Read uncommitted：即使一个事务的更新语句没有提交，但是别的事务可以读到这个改变，鸡肋一般的存在。 Read committed：一个事务能看到另外一个事务对一条数据记录已经提交的修改。 Repeatable read：一个事务中进行两次或多次同样的对于数据内容的查询，得到的结果是一样的，但不保证对于数据条数的查询是一样的。 Serializable：事务串行执行，不允许并发执行。 Repeatable read 示例Repeatable Read 的直观感觉仿佛是给事务做一个整个数据库做了一个快照， 所以很多时候这种隔离级别又被称为Snapshot Isolation。 事务A 事务B get x (得到0) set x = 1 commit get x (得到0) commit “快照”的功能在一些场景下非常重要，如： 数据备份：例如数据库S从数据库M中复制数据，但是同时M数据库又被持续修改。S需要拿到一个M的数据快照，但是又不能真的把M给停了。 数据合法性检查：例如有两张数据表，一张记录了当时的交易总额，另外一张表记录了每个交易的金额。 ==那么在读取数据时，如果没有快照的存在，交易金额的总和可能与当时的交易总额对不上，== ==因为随着检查事务的进行，新的交易记录数据会被提交。这些新的提交会被检查事务看到。== 注意： 如果是基于MVCC的实现，Repeatable Read可以完全避免幻读。 无论MySQL还是PostgreSQL在Repeatable Read隔离级别都不会出现幻读。 数据库并发操作产生的问题丢失更新第一类丢失更新问题此种更新丢失是因为回滚的原因，所以也叫==回滚丢失==。 时间 取款事务A 转账事务B T1 开始事务 T2 开始事务 T3 查询账户余额为1000元 T4 查询账户余额为1000元 T5 汇入100元把余额改为1100元 T6 提交事务 T7 取出100元把余额改为900元 T8 撤销事务，余额恢复为1000 元（丢失更新） 注意： A事务在撤销时，将B事务已经转入账户的金额给抹去了，此时余额为1000元。 第二类丢失更新问题此种更新丢失是因为更新被其他事务给覆盖了，也可以叫==覆盖丢失==。 时间 转账事务A 取款事务B T1 开始事务 T2 开始事务 T3 查询账户余额为1000元 T4 查询账户余额为1000元 T5 取出100元把余额改为900元 T6 提交事务 T7 汇入100元 T8 提交事务，把余额改为1100 元（丢失更新） 注意：A事务把B事务的金额修改了，此时余额为1100元。 脏读A事务读取B事务尚未提交的更改数据，并在这个数据的基础上进行操作。 时间 员工事务A 老板事务B T1 开始事务 T2 开始事务 T3 给员工账户转账5000元 T4 查询账户余额为5000元(脏读) T5 提交事务 T6 撤销事务 T7 开始事务 T8 给员工账户转账2000元 T9 提交事务 解释 ​ 公司发工资了，领导把5000元打到员工的账号上，但是该事务并未提交，而员工正好去查看账户，发现5000元工资到账。但是不幸的是，领导发现发给员工的发的工资金额不对，是2000元，于是回滚了事务，修改为2000后，将事务提交，最后员工实际的工资是只有 2000元。 不可重复读是指在A事务内，多次读同一数据，但是两次读到的数据内容不一样。（即不能读到相同的数据内容） 时间 员工事务A 员工老婆事务B T1 开始事务 T2 开始事务 T3 准备付款，查询账户余额为2000元 T4 网上转账2000元 T5 提交事务 T6 付款，查询账户余额为0元，付款失败 T7 提交事务 解释 员工拿着工资卡去消费，系统读取到卡里确实有2000元，而此时他老婆也正好在网上转账，把工资卡的2000元转到另一账户， 并在员工之前提交了事务，当员工进行扣款时，系统检查到员工的工资卡已经没有钱，扣款失败。 幻读是指当事务不是独立执行时发生的一种现象，A事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，B事务也修改这个表中的数据，这种修改是向表中插入或删除一行数据。那么，操作A事务的用户会发现表中还有尚未被修改的数据行，就好像发生了幻觉一样。 时间 员工老婆事务A 员工事务B T1 开始事务 T2 开始事务 T3 查询信用卡的消费记录总额为80元 T4 客户应酬，信用卡付款1000元 T5 提交事务 T6 打印信用卡账单，总额为1080元（幻读） T7 提交事务 解释 员工老婆时常查看员工的信用卡消费记录。有一天，她正在查询到员工当月信用卡的总消费金额 （select sum(amount) from transaction where month = 本月）为80元，而员工此时正好应酬完后在收银台买单，消费了1000元，即新增了一条1000元的消费记录（insert transaction ... ），并提交事务，随后员工老婆打印当月信用卡章法，却发现消费总额为1080元，员工老婆很诧异，以为出现了幻觉，幻读就这样产生了。 幻读和不可重复读强调的维度不一样: 不可重复读是指A操作数据时，B可以修改该行数据。 幻读是指A操作数据时，B可以新增/删除一条数据（是对总记录数有影响的）。 隔离级别对异常的控制能力 隔离级别\并发问题 第一类更新丢失 脏读 不可重复读 第二类丢失更新 幻读 Read uncommitted Y Y Y Y Y Read committed N N Y Y Y（MVVC-&gt; N） Repeatable read N N N Y Y（MVVC-&gt; N） Serializable N N N N N 写前提困境业务代码一般是这样的： 先读取一段现有的数据； 在这个数据的基础上做逻辑判断或者计算； 将计算的结果写回数据库； 第三步的写入会依赖第一步的读取。而且在1和3之间，不管业务代码离得有多近，都无法避免其他事务的并发修改。即：在当前事务读取这段数据后，又有另一个事务修改了这个数据，而当前事务将结果写回数据库，会造成另一个事务的修改丢失。 这个问题就是在修改的事务在提交时，无法确保这个修改的前提是否还可靠。这种问题称之为写前提困境。 Solution 事务隔离级别用Serializable； 加悲观锁；写入较多的情况 加乐观锁；读取较多的情况]]></content>
      <categories>
        <category>事务</category>
      </categories>
      <tags>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库锁]]></title>
    <url>%2F2020%2F09%2F06%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%94%81%2F</url>
    <content type="text"><![CDATA[数据库锁锁的分类按获取锁的方式： 排他锁 共享锁 按系统并发： 悲观锁 乐观锁 按保护对象： DML锁： TX行锁 TM表锁 DDL锁： DDL排他锁 DDL共享锁 DDL可分解解析锁 系统锁： 闩锁Latch 互斥锁Mutexes 内部锁： 字典缓存锁 文件和日志管理锁 表空间和撤销段锁 乐观锁 和 悲观锁 是一种思想 实现形式可能不一样：如 Java的原子类，数据库的version字段 悲观锁顾名思义，就是很悲观，它对于数据被外界修改持保守态度，认为数据随时会修改，所以整个数据处理中需要将数据加锁。悲观锁一般都是依靠关系数据库提供的锁机制，事实上关系数据库中的行锁，表锁不论是读写锁都是悲观锁。 悲观锁按照使用性质划分共享锁(S锁) 多个事务可以同时读，但不能有写操作 读锁，事务A对对象T加S锁，其他事务也只能对T加S，直到A释放S锁。 排它锁(X锁) 只有一个事务可以操作，其他事务等待 写锁，事务A对对象T加X锁以后，其他事务不能对T加任何锁，只有事务A可以读写对象T直到A释放X锁。 更新锁(U锁)用来预定要对此对象施加X锁，它允许其他事务读，但不允许再施加U锁或X锁；当被读取的对象将要被更新时，则升级为X锁，主要是用来防止死锁的。因为使用共享锁时，修改数据的操作分为两步，首先获得一个共享锁，读取数据，然后将共享锁升级为排它锁，然后再执行修改操作。这样如果同时有两个或多个事务同时对一个对象申请了共享锁，在修改数据的时候，这些事务都要将共享锁升级为排它锁。这些事务都不会释放共享锁而是一直等待对方释放，这样就造成了死锁。如果一个数据在修改前直接申请更新锁，在数据修改的时候再升级为排它锁，就可以避免死锁。 排它锁和共享锁的相容矩阵 T1\T2 X S - X N N Y S N Y Y - Y Y Y 悲观锁按照作用范围划分行锁 作用范围：行级别 数据库能够确定那些行需要锁的情况下使用行锁，如果不知道会影响哪些行的时候就会使用表锁。 举例： ​ 一个用户表user，有主键id和用户生日birthday当你使用update … where id=?这样的语句数据库明确知道会影响哪一行，它就会使用行锁，当你使用update … where birthday=?这样的的语句的时候因为事先不知道会影响哪些行就可能会使用表锁。 表锁 作用范围：整张表 乐观锁顾名思义，就是很乐观，每次自己操作数据的时候认为没有人回来修改它，所以不去加锁，但是在更新的时候会去判断在此期间数据有没有被修改，需要用户自己去实现。 两种锁的使用场景 两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 乐观锁实现方式版本号 实现：实体上增加版本标识，数据库上表增加一个version字段；每次更新把这个字段加1 原理：读数据时把version读出来，在更新数据时比较version，如果读的version没有变化可以更新了，如果DB的version比读的version大，说明有其他事务更新了该数据。这时候得到一个无法更新的通知，用户自行根据这个通知来决定怎么处理，比如重新开始一遍。 关键：判断version和更新两个动作需要作为一个原子单元执行，否则在你判断可以更新，但是在正式更新前，又有别的数据修改了version，这个时候你的更新就会覆盖别的事务的更新，造成第二类丢失更新问题。 update … , version = version + 1 where … and version=&#39;old version&#39;;，根据返回结果是0还是非0来判断是否更新成功。 时间戳 和版本号基本一样，只是通过时间戳来判断而已，注意时间戳要使用数据库服务器的时间戳不能是业务系统的时间 待更新字段 实现：和版本号方式相似，只是不增加额外字段，直接使用有效数据字段做版本控制信息，因为有时候我们可能无法改变旧系统的数据库表结构。 原理：假如待更新字段叫count，先去读取这个count，更新时去比较数据库中count的值是不是原来读取的值，如果是就更新，否则更新失败。JAVA原子类型对象：如AtomicInteger就是这种思想。 所有字段 和待更新字段类似，只是使用所有字段做版本控制信息，只有所有字段都没变化才会执行更新。]]></content>
      <categories>
        <category>锁</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue Mixin]]></title>
    <url>%2F2020%2F08%2F25%2FVue%20Mixin%2F</url>
    <content type="text"><![CDATA[参考连接 局部 mixin当多个组件都需要用到相同得到方法或者data有相同的属性，就可以使用mixin进行封装 需要封装的方法和属性 –&gt; mixin.js （名字根据业务定义） 1234567891011121314151617181920212223export default &#123; name: 'Mixin', data() &#123; return &#123; message: 'hello' &#125; &#125;, //可以添加钩子函数 created() &#123; console.log('我是 mixins 中的 created') &#125;, methods: &#123; show(num) &#123; console.log(num) // mixins的函数可以接收组件传的参数。 &#125;, foo() &#123; console.log('foo') &#125;, conflicting() &#123; console.log('from mixin') &#125; &#125;&#125; 需要使用mixin.js里面的方法和属性的vue文件 –&gt; test.vue 1234567891011121314151617181920212223242526272829303132333435&lt;template&gt; &lt;div/&gt;&lt;/template&gt;&lt;script&gt;import mixin from &apos;../mixins/mixin&apos; export default &#123; name: &apos;Test&apos;, mixins: [mixin], // 这里引入mixin.js data() &#123; return &#123; title: &apos;def&apos;, message: &apos;goodbye&apos; &#125; &#125;, created() &#123; console.log(&apos;我是Vue中的created&apos;) console.log(this.$data) this.show(50); //可通过函数传参,把组件中需要的参数传给mixins进行使用。 &#125;, methods: &#123; bar() &#123; console.log(&apos;bar&apos;) &#125;, conflicting() &#123; console.log(&apos;from self&apos;) &#125; &#125;&#125;&lt;/script&gt;&lt;style lang=&quot;scss&quot; scoped&gt;&lt;/style&gt; 注意 组件 –&gt; test.vue 混入对象 –&gt; mixin.js 当组件和混入对象含有同名选项（属性/方法）时，这些选项将以恰当的方式混合。 比如，数据对象在内部会进行浅合并 (一层属性深度)，在和组件的数据发生冲突时以组件数据优先。 上述例子： 123456789101112131415161718&gt; data: &#123;&gt; return: &#123;&gt; title: 'def', // mixin.js属性装进组件里面&gt; message: 'goodbye' // 与mixin.js同名，组件优先&gt; &#125;&gt; &#125;,&gt; methods: &#123;&gt; show(num) &#123; // mixin.js方法装进组件里面&gt; console.log(num) &gt; &#125;, &gt; bar() &#123; // 与mixin.js同名，组件优先&gt; console.log('bar')&gt; &#125;,&gt; conflicting() &#123; // 与mixin.js同名，组件优先&gt; console.log('from self')&gt; &#125;, &gt; &#125;&gt; 同名钩子函数将混合为一个数组，都会被调用。 混入对象的钩子将在组件自身钩子之前调用。 值为对象的选项，例如 methods, components 和 directives，将被混合为同一个对象。 两个对象键名冲突时，取组件对象的键值对。 全局 mixin 慎用: 一旦使用全局混入对象，将会影响到 所有 之后创建的 Vue 实例 1Vue.mixin(&#123;&#125;)]]></content>
      <categories>
        <category>Vue</category>
      </categories>
      <tags>
        <tag>Vue</tag>
        <tag>Mixin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[防抖动]]></title>
    <url>%2F2020%2F08%2F20%2F%E9%98%B2%E6%8A%96%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[参考的连接 什么是防抖所谓防抖，就是指触发事件后在n秒内只能执行一次，如果在n秒内触发事件的，则会重新计算函数执行事件 防抖的两种状态立即执行触发事件后函数立即执行 非立即执行触发事件后函数不会立即执行，而是在n秒后开始执行 防抖使用场景 防抖函数常用来进行处理某些频繁触发的请求事件（或者前端计算的事件） window滚动条事件 scroll window的resize 鼠标的mouse move 下拉框的远程模糊搜索 表单组件输入内容验证 防止多次点击导致表单多次提交 目的：限制函数触发频率；对性能优化，减少服务器负担； 实现核心：维护一个setTimeout，在冷却时间内再次触发函数则清空原setTimeout重新计算一个setTimeout推入队列等待执行 lodash的方法1_.debounce(func, [wait=0], [options=&#123;&#125;]) 参数 func (Function): 要防抖动的函数。 [wait=0] (number): 需要延迟的毫秒数。 [options={}] (Object): 选项对象。 [options.leading=false] (boolean): 指定在延迟开始前调用。 [options.maxWait] (number): 设置 func 允许被延迟的最大值。 [options.trailing=true] (boolean): 指定在延迟结束后调用。 返回(Function): 返回新的 debounced（防抖动）函数。 例子12345678910111213141516// 避免窗口在变动时出现昂贵的计算开销。jQuery(window).on('resize', _.debounce(calculateLayout, 150)); // 当点击时 `sendMail` 随后就被调用。jQuery(element).on('click', _.debounce(sendMail, 300, &#123; 'leading': true, 'trailing': false&#125;)); // 确保 `batchLog` 调用1次之后，1秒内会被触发。var debounced = _.debounce(batchLog, 250, &#123; 'maxWait': 1000 &#125;);var source = new EventSource('/stream');jQuery(source).on('message', debounced); // 取消一个 trailing 的防抖动调用jQuery(window).on('popstate', debounced.cancel);]]></content>
      <categories>
        <category>前端</category>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>lodash</tag>
        <tag>防止多次提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pageable分页]]></title>
    <url>%2F2020%2F08%2F19%2FPageable%2F</url>
    <content type="text"><![CDATA[JpaRepository提供了两个和分页和排序有关的查询 List findAll(Sort sort): 按照指定顺序排序返回所有实体 List findAll(Pageable pageable): 分页查询 SortSort 对象用来指示排序，默认采用升序排序 Sort sort = new Sort(&quot;id&quot;); // id 是 属性名 Sort sort = new Sort(Direction.DESC,&quot;id&quot;); PagePage接口可以获得 当前页面的记录 总页数 总记录数 是否有上一页或下一页等 123int getTotalPages() 总的页数long getTotalElements() 返回总数List getContent() 返回此次查询的结果集 Spring Data 分页查询总是返回Page对象 PageablePageable 接口用于构造翻页查询，PageRequest 是其实现类，它不仅仅支持分页，还支持排序(单字段/多字段均支持) public static PageRequest of(int page, int size); public static PageRequest of(int page, int size, Sort sort); page: 起始页，从0开始 size: 页大小，一页的行数 @PageableDefaultController层可以直接使用Pageable接收参数，如果要设置默认参数，可以使用 @PageableDefault注解 参数例子： 1@PageableDefault(page = 0, value = 6, sort = &#123;"createdTime"&#125;, direction = Sort.Direction.DESC) Pageable pageable RESTful接口的例子： 1http://localhost:8882/api/v1/questionBanks/byEnterprise?sort=id%2Cdesc&amp;page=0&amp;size=2 解释： api接口排序字段后面传的是[,asc/desc] 例如要排序的是id属性，则 id,desc 如果是升序的话，可以直接写属性名。 Order和Sort的转换用EntityManager查询的时候，排序的类型是javax.persistence.criteria.Order 所以需要将Sort对象转成Order对象: 1List&lt;javax.persistence.criteria.Order&gt; QueryUtils.toOrders(Sort, Root, CriteriaBuilder) 注意Spring的web Pageable分页，页大小记录默认只支持2000，如果要超过这个限制，需要在配置文件中配置： 12345spring: data: web: pageable: max-page-size: 10000 # 设置页大小]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
        <tag>pageable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Wake On Lan 配置远程启动电脑]]></title>
    <url>%2F2020%2F07%2F04%2FWOL%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[前提远程控制电脑开机。 本人笔记本是合起来放在书架后面的，每次开机都要把它拿出来打开，非常麻烦，所以想要通过其他方式唤醒电脑。 解决思路 使用Wake On Lan通过网卡控制电脑开机。 什么是Wake On Lan Wake-On-LAN 简称 WOL，是一种电源管理功能；它是由IBM公司提出的网络唤醒标准，目前该标准已被大多数主板厂商支持。支持该标准的主板允许从远程通过网络唤醒计算机，也就是远程开机。 如何实现远程开机打开你的网络设置方式1：我的电脑-&gt;右键-&gt;管理-&gt;设备管理器-&gt;网络适配器-&gt;找到自己的网卡-&gt;右键-&gt;属性 方式2：打开网络和Internet设置-&gt;网络共享中心-&gt;更改适配器设置-&gt;找到自己的网卡-&gt;右键-&gt;属性-&gt;配置 选择网卡唤醒 设置防火墙策略防火墙和网络保护-&gt; 高级设置 在路由器设置里将PC的MAC地址和网线的IP绑定 PC安装测试软件https://www.depicus.com/wake-on-lan/wake-on-lan-monitor 下载完成后打开，设置UDP端口为刚才设置的端口号9，点击Start 手机安装WOL的软件这里我Iphone安装的是Wolow 点击后，如果刚才打开的monitor软件，出现以下的内容，理论上已经代表能够联通，关机后如果还是没有唤醒可能需要找下其他原因。 Wake On Lan原理Wake-On-LAN的实现，主要是向目标主机发送特殊格式的数据包，俗称魔术包（Magic Packet）。 Magic Packet 格式的数据包是由 AMD 公司开发推广的技术，虽然其并非世界公认的标准，但是仍然受到很多网卡制造商的支持，因此许多具有网络唤醒功能的网卡都能与之兼容。 Magic Packet魔法数据包（Magic Packet）是一个广播性的帧（frame），通过端口7或端口9进行发送，且可以用无连接（Connectionless protocol）的通信协议（如UDP）来传递。 在魔法数据包内，每次都会先有连续6个”FF”（十六进制，换算成二进制即：11111111）的数据，即：FF FF FF FF FF FF，在连续6个”FF”后则开始带出MAC地址信息（MAC地址重复16次），有时还会带出4字节或6字节的密码，一旦经由网卡侦测、解读、研判（广播）魔法数据包的内容，内容中的MAC地址、密码若与电脑自身的地址、密码吻合，就会引导唤醒、开机的程序。 Magic Packet 魔术数据包的格式一般看上去像下面这个样子 假设MAC地址为：00-00-00-00-00 序号 MagicPacket 1 FF FF FF FF FF FF 2 00 00 00 00 00 00 3 00 00 00 00 00 00 4 00 00 00 00 00 00 5 00 00 00 00 00 00 6 00 00 00 00 00 00 7 00 00 00 00 00 00 8 00 00 00 00 00 00 9 00 00 00 00 00 00 10 00 00 00 00 00 00 11 00 00 00 00 00 00 12 00 00 00 00 00 00 13 00 00 00 00 00 00 14 00 00 00 00 00 00 15 00 00 00 00 00 00 16 00 00 00 00 00 00 17 00 00 00 00 00 00 魔法数据包（Magic Packet）结构上非常简单。 其他C#语言去实现一个WakeOnLan软件]]></content>
      <categories>
        <category>WOL</category>
      </categories>
      <tags>
        <tag>WOL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security (一) 架构框架-Component、Service、Filter分析]]></title>
    <url>%2F2020%2F05%2F30%2FSpring%20Security%20(%E4%B8%80)%20%E6%9E%B6%E6%9E%84%E6%A1%86%E6%9E%B6-Component%E3%80%81Service%E3%80%81Filter%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[【转载】 作者：Ccww 链接：https://juejin.im/post/5d074dc1f265da1bce3dd10f Spring security （一）架构框架-Component、Service、Filter分析 想要深入spring security的authentication （身份验证）和access-control（访问权限控制）工作流程，必须清楚spring security的主要技术点包括关键接口、类以及抽象类如何协同工作进行authentication 和access-control的实现。 Spring Security 认证和授权流程常见认证和授权流程可以分成： A user is prompted to log in with a username and password （用户用账密码登录） The system (successfully) verifies that the password is correct for the username（校验密码正确性） The context information for that user is obtained (their list of roles and so on).（获取用户信息context，如权限） A security context is established for the user（为用户创建security context） The user proceeds, potentially to perform some operation which is potentially protected by an access control mechanism which checks the required permissions for the operation against the current security context information.（访问权限控制，是否具有访问权限） Spring Security 认证上述前三点为spring security认证验证环节： 通常通过AbstractAuthenticationProcessingFilter过滤器将账号密码组装成Authentication实现类UsernamePasswordAuthenticationToken； 将token传递给AuthenticationManager验证是否有效，而AuthenticationManager通常使用ProviderManager实现类来检验； AuthenticationManager认证成功后将返回一个拥有详细信息的Authentication object（包括权限信息，身份信息，细节信息，但密码通常会被移除）； 通过SecurityContextHolder.getContext().getAuthentication().getPrincipal()将Authentication设置到security context中。 Spring Security 访问授权 通过FilterSecurityInterceptor过滤器入口进入； FilterSecurityInterceptor通过其继承的抽象类AbstractSecurityInterceptor的beforeInvocation(Object object)方法进行访问授权，其中涉及了类AuthenticationManager、AccessDecisionManager、SecurityMetadataSource等。 根据上述描述的过程，我们接下来主要去分析其中涉及的一下Component、Service、Filter。 核心组件（Core Component ）SecurityContextHolder SecurityContextHolder提供对SecurityContext的访问，存储security context（用户信息、角色权限等），而且其具有下列储存策略即工作模式： SecurityContextHolder.MODE_THREADLOCAL（默认）：使用ThreadLocal，信息可供此线程下的所有的方法使用，一种与线程绑定的策略，此天然很适合Servlet Web应用。 SecurityContextHolder.MODE_GLOBAL：使用于独立应用 SecurityContextHolder.MODE_INHERITABLETHREADLOCAL：具有相同安全标示的线程 修改SecurityContextHolder的工作模式有两种方法 : 设置一个系统属性(system.properties) : spring.security.strategy; 调用SecurityContextHolder静态方法setStrategyName() 在默认ThreadLocal策略中，SecurityContextHolder为静态方法获取用户信息为: 123456Object principal = SecurityContextHolder.getContext().getAuthentication().getPrincipal(); if (principal instanceof UserDetails) &#123; String username = ((UserDetails)principal).getUsername(); &#125; else &#123; String username = principal.toString(); &#125; 但是一般不需要自身去获取。 其中getAuthentication()返回一个Authentication认证主体，接下来分析Authentication、UserDetails细节。 Authentication Spring Security使用一个Authentication对象来描述当前用户的相关信息,其包含用户拥有的权限信息列表、用户细节信息（身份信息、认证信息）。Authentication为认证主体在spring security中时最高级别身份/认证的抽象，常见的实现类UsernamePasswordAuthenticationToken。Authentication接口源码： 123456789101112public interface Authentication extends Principal, Serializable &#123; //权限信息列表,默认GrantedAuthority接口的一些实现类 Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); //密码信息 Object getCredentials(); //细节信息，web应用中的实现接口通常为 WebAuthenticationDetails，它记录了访问者的ip地址和sessionId的值 Object getDetails(); //通常返回值为UserDetails实现类 Object getPrincipal(); boolean isAuthenticated(); void setAuthenticated(boolean var1) throws IllegalArgumentException;&#125; 前面两个组件都涉及了UserDetails，以及GrantedAuthority其到底是什么呢？2.3小节分析。 UserDetails&amp;GrantedAuthority UserDetails提供从应用程序的DAO或其他安全数据源构建Authentication对象所需的信息，包含GrantedAuthority。其官方实现类为User，开发者可以实现其接口自定义UserDetails实现类。其接口源码： 12345678910111213141516 public interface UserDetails extends Serializable &#123; Collection&lt;? extends GrantedAuthority&gt; getAuthorities(); String getPassword(); String getUsername(); boolean isAccountNonExpired(); boolean isAccountNonLocked(); boolean isCredentialsNonExpired(); boolean isEnabled();&#125; UserDetails与Authentication接口功能类似，其含义是Authentication为用户提交的认证凭证（账号密码）； ​ UserDetails为系统中用户正确认证凭证； ​ UserDetailsService中的loadUserByUsername方法获取正确的认证凭证； 其中在getAuthorities()方法中获取到GrantedAuthority列表是代表用户访问应用程序权限范围，此类权限通常是“role(角色）”，例如ROLE_ADMINISTRATOR或ROLE_HR_SUPERVISOR； ​ GrantedAuthority接口常见的实现类SimpleGrantedAuthority。 核心服务类（Core Services）AuthenticationManager、ProviderManager以及AuthenticationProvider AuthenticationManager是认证相关的核心接口，是认证一切的起点。 ​ 常见的认证流程都是AuthenticationManager实现类ProviderManager处理，而且ProviderManager实现类基于委托者模式维护AuthenticationProvider 列表用于不同的认证方式。例如： 使用账号密码认证方式DaoAuthenticationProvider实现类（继承了AbstractUserDetailsAuthenticationProvide抽象类），其为默认认证方式，进行数据库库获取认证数据信息。 游客身份登录认证方式AnonymousAuthenticationProvider实现类 从cookies获取认证方式RememberMeAuthenticationProvider实现类 ProviderManager源码分析： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public Authentication authenticate(Authentication authentication) throws AuthenticationException &#123; Class&lt;? extends Authentication&gt; toTest = authentication.getClass(); AuthenticationException lastException = null; Authentication result = null; //AuthenticationProvider列表依次认证 for (AuthenticationProvider provider : getProviders()) &#123; if (!provider.supports(toTest)) &#123; continue; &#125; try &#123; //每个AuthenticationProvider进行认证 result = provider.authenticate(authentication) if (result != null) &#123; copyDetails(authentication, result); break; &#125; &#125; .... catch (AuthenticationException e) &#123; lastException = e; &#125; &#125; //进行父类AuthenticationProvider进行认证 if (result == null &amp;&amp; parent != null) &#123; // Allow the parent to try. try &#123; result = parent.authenticate(authentication); &#125; catch (AuthenticationException e) &#123; lastException = e; &#125; &#125; // 如果有Authentication信息，则直接返回 if (result != null) &#123; if (eraseCredentialsAfterAuthentication &amp;&amp; (result instanceof CredentialsContainer)) &#123; //清除密码 ((CredentialsContainer) result).eraseCredentials(); &#125; //发布登录成功事件 eventPublisher.publishAuthenticationSuccess(result); return result; &#125; //如果都没认证成功，抛出异常 if (lastException == null) &#123; lastException = new ProviderNotFoundException(messages.getMessage( "ProviderManager.providerNotFound", new Object[] &#123; toTest.getName() &#125;, "No AuthenticationProvider found for &#123;0&#125;")); &#125; prepareException(lastException, authentication); throw lastException;&#125; ProviderManager 中的List&lt;AuthenticationProvider&gt;，会依照次序去认证； ​ 默认策略下，只需要通过一个AuthenticationProvider的认证，即可被认为是登录成功，而且认证成功后返回一个Authentication实体，并为了安全会进行清除密码。 ​ 如果所有认证器都无法认证成功，则ProviderManager 会抛出一个ProviderNotFoundException异常。 UserDetailsService UserDetailsService接口作用是从特定的地方获取认证的数据源（账号、密码）。如何获取到系统中正确的认证凭证，通过loadUserByUsername(String username)获取认证信息，而且其只有一个方法： 1UserDetails loadUserByUsername(String username) throws UsernameNotFoundException; ​ UserDetailsService常见的实现类 从数据库获取的JdbcDaoImpl实现类， 从内存中获取的InMemoryUserDetailsManager实现类， …. 自定义UserDetailsService实现类，如下： 1234567891011121314151617181920212223242526272829public class CustomUserService implements UserDetailsService &#123; @Autowired //用户mapper private UserInfoMapper userInfoMapper; @Autowired //用户权限mapper private PermissionInfoMapper permissionInfoMapper; @Override public UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; UserInfoDTO userInfo = userInfoMapper.getUserInfoByUserName(username); if (userInfo != null) &#123; List&lt;PermissionInfoDTO&gt; permissionInfoDTOS = permissionInfoMapper.findByAdminUserId(userInfo.getId()); List&lt;GrantedAuthority&gt; grantedAuthorityList = new ArrayList&lt;&gt;(); //组装权限GrantedAuthority object for (PermissionInfoDTO permissionInfoDTO : permissionInfoDTOS) &#123; if (permissionInfoDTO != null &amp;&amp; permissionInfoDTO.getPermissionName() != null) &#123; GrantedAuthority grantedAuthority = new SimpleGrantedAuthority( permissionInfoDTO.getPermissionName()); grantedAuthorityList.add(grantedAuthority); &#125; &#125; //返回用户信息 return new User(userInfo.getUserName(), userInfo.getPasswaord(), grantedAuthorityList); &#125;else &#123; //抛出用户不存在异常 throw new UsernameNotFoundException("admin" + username + "do not exist"); &#125; &#125;&#125; AccessDecisionManager&amp;SecurityMetadataSource AccessDecisionManager是由AbstractSecurityInterceptor调用，负责做出最终的访问控制决策。 AccessDecisionManager接口源码： 123456//访问控制决策 void decide(Authentication authentication, Object secureObject,Collection&lt;ConfigAttribute&gt; attrs) throws AccessDeniedException; //是否支持处理传递的ConfigAttribute boolean supports(ConfigAttribute attribute); //确认class是否为AccessDecisionManager boolean supports(Class clazz); SecurityMetadataSource包含着AbstractSecurityInterceptor访问授权所需的元数据（动态url、动态授权所需的数据）；在AbstractSecurityInterceptor授权模块中结合AccessDecisionManager进行访问授权，其涉及了ConfigAttribute。 SecurityMetadataSource接口： 123456Collection&lt;ConfigAttribute&gt; getAttributes(Object object) throws IllegalArgumentException;Collection&lt;ConfigAttribute&gt; getAllConfigAttributes();boolean supports(Class&lt;?&gt; clazz); 我们还可以自定义SecurityMetadataSource数据源，实现接口FilterInvocationSecurityMetadataSource。例： 123456789101112131415161718192021public class MyFilterSecurityMetadataSource implements FilterInvocationSecurityMetadataSource &#123; public List&lt;ConfigAttribute&gt; getAttributes(Object object) &#123; FilterInvocation fi = (FilterInvocation) object; String url = fi.getRequestUrl(); String httpMethod = fi.getRequest().getMethod(); List&lt;ConfigAttribute&gt; attributes = new ArrayList&lt;ConfigAttribute&gt;(); // Lookup your database (or other source) using this information and populate the // list of attributes return attributes; &#125; public Collection&lt;ConfigAttribute&gt; getAllConfigAttributes() &#123; return null; &#125; public boolean supports(Class&lt;?&gt; clazz) &#123; return FilterInvocation.class.isAssignableFrom(clazz); &#125;&#125; PasswordEncoder 为了存储安全，一般要对密码进行算法加密。 Spring Security提供了加密PasswordEncoder接口。其实现类有： BCrypt hash算法实现的BCryptPasswordEncoder； SCrypt hashing 算法实现的SCryptPasswordEncoder； PasswordEncoder接口只有两个方法： 123456public interface PasswordEncoder &#123; //密码加密 String encode(CharSequence rawPassword); //密码配对 boolean matches(CharSequence rawPassword, String encodedPassword);&#125; 核心 Security 过滤器（Core Security Filters）FilterSecurityInterceptor FilterSecurityInterceptor是Spring Security授权模块入口，该类根据访问的用户的角色，权限授权访问那些资源（访问特定路径应该具备的权限）。 FilterSecurityInterceptor封装FilterInvocation对象进行操作； ​ 所有的请求到了FilterSecurityInterceptor，如果这个filter之前没有执行过的话，那么首先执行其父类AbstractSecurityInterceptor提供的InterceptorStatusToken token = super.beforeInvocation(fi)，该方法会： 使用AuthenticationManager获取Authentication中用户详情； 使用ConfigAttribute封装已定义好访问权限详情； 使用AccessDecisionManager.decide()方法进行访问权限控制。 FilterSecurityInterceptor源码分析： 123456789101112131415161718192021222324public void invoke(FilterInvocation fi) throws IOException, ServletException &#123; if ((fi.getRequest() != null) &amp;&amp; (fi.getRequest().getAttribute(FILTER_APPLIED) != null) &amp;&amp; observeOncePerRequest) &#123; fi.getChain().doFilter(fi.getRequest(), fi.getResponse()); &#125; else &#123; // first time this request being called, so perform security checking if (fi.getRequest() != null &amp;&amp; observeOncePerRequest) &#123; fi.getRequest().setAttribute(FILTER_APPLIED, Boolean.TRUE); &#125; //回调其继承的抽象类AbstractSecurityInterceptor的方法 InterceptorStatusToken token = super.beforeInvocation(fi); try &#123; fi.getChain().doFilter(fi.getRequest(), fi.getResponse()); &#125; finally &#123; super.finallyInvocation(token); &#125; super.afterInvocation(token, null); &#125;&#125; AbstractSecurityInterceptor源码分析： 12345678910111213141516protected InterceptorStatusToken beforeInvocation(Object object) &#123; .... //获取所有访问权限（url-role）属性列表（已定义在数据库或者其他地方） Collection&lt;ConfigAttribute&gt; attributes = this.obtainSecurityMetadataSource() .getAttributes(object); .... //获取该用户访问信息（包括url，访问权限） Authentication authenticated = authenticateIfRequired(); // Attempt authorization try &#123; //进行授权访问 this.accessDecisionManager.decide(authenticated, object, attributes); &#125;catch ....&#125; UsernamePasswordAuthenticationFilter UsernamePasswordAuthenticationFilter使用username和password表单登录使用的过滤器，也是最常用的过滤器。其源码： 123456789101112131415public Authentication attemptAuthentication(HttpServletRequest request, HttpServletResponse response) throws AuthenticationException &#123; //获取表单中的用户名和密码 String username = obtainUsername(request); String password = obtainPassword(request); ... username = username.trim(); //组装成username+password形式的token UsernamePasswordAuthenticationToken authRequest = new UsernamePasswordAuthenticationToken( username, password); // Allow subclasses to set the "details" property setDetails(request, authRequest); //交给内部的AuthenticationManager去认证，并返回认证信息 return this.getAuthenticationManager().authenticate(authRequest);&#125; 其主要代码为创建UsernamePasswordAuthenticationToken的Authentication实体以及调用AuthenticationManager进行authenticate认证，根据认证结果执行successfulAuthentication或者unsuccessfulAuthentication，无论成功失败，一般的实现都是转发或者重定向等处理，不再细究AuthenticationSuccessHandler和AuthenticationFailureHandle。若有兴趣，可以研究一下其父类AbstractAuthenticationProcessingFilter过滤器。 AnonymousAuthenticationFilterAnonymousAuthenticationFilter是匿名登录过滤器，它位于常用的身份认证过滤器（如UsernamePasswordAuthenticationFilter、BasicAuthenticationFilter、RememberMeAuthenticationFilter）之后，意味着只有在上述身份过滤器执行完毕后，SecurityContext依旧没有用户信息，AnonymousAuthenticationFilter该过滤器才会有意义——给当前请求一个匿名身份。 AnonymousAuthenticationFilter源码分析： 123456789101112131415161718192021222324252627public class AnonymousAuthenticationFilter extends GenericFilterBean implements InitializingBean &#123; ... public AnonymousAuthenticationFilter(String key) &#123; this(key, "anonymousUser", AuthorityUtils.createAuthorityList("ROLE_ANONYMOUS")); &#125; ... public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; if (SecurityContextHolder.getContext().getAuthentication() == null) &#123; //创建匿名登录Authentication的信息 SecurityContextHolder.getContext().setAuthentication( createAuthentication((HttpServletRequest) req)); ... &#125; chain.doFilter(req, res); &#125; //创建匿名登录Authentication的信息方法 protected Authentication createAuthentication(HttpServletRequest request) &#123; AnonymousAuthenticationToken auth = new AnonymousAuthenticationToken(key, principal, authorities); auth.setDetails(authenticationDetailsSource.buildDetails(request)); return auth; &#125;&#125; SecurityContextPersistenceFilter SecurityContextPersistenceFilter的两个主要作用： 当Request过来时，创建SecurityContext安全上下文信息 当Request结束时，清空SecurityContextHolder。 小节总结：. AbstractAuthenticationProcessingFilter:主要处理登录. FilterSecurityInterceptor:主要处理鉴权]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Security (二) WebSecurityConfigurer和filter的配置]]></title>
    <url>%2F2020%2F05%2F30%2FSpring%20Security%20(%E4%BA%8C)%20WebSecurityConfigurer%E5%92%8Cfilter%E7%9A%84%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[【转载】 作者：Ccww 链接：https://juejin.im/post/5d0b1eb35188252f921b1535 Spring Security（二）–WebSecurityConfigurer配置以及filter顺序 在认证过程和访问授权前必须了解spring Security如何知道我们要求所有用户都经过身份验证？ Spring Security如何知道我们想要支持基于表单的身份验证？因此必须了解WebSecurityConfigurerAdapter配置类如何工作的。而且也必须了解清楚filter的顺序，才能更好了解其调用工作流程。 WebSecurityConfigurerAdapter 在使用 WebSecurityConfigurerAdapter 前，先了解 Spring security config。 Spring security config 具有三个模块，一共有3个 builder，认证相关的 AuthenticationManagerBuilder 和web相关的 WebSecurity、HttpSecurity。 AuthenticationManagerBuilder：用来配置全局的认证相关的信息，其实就是AuthenticationProvider和UserDetailsService，前者是认证服务提供商，后者是用户详情查询服务； WebSecurity： 全局请求忽略规则配置（比如说静态文件，比如说注册页面）、全局HttpFirewall配置、是否debug配置、全局SecurityFilterChain配置、privilegeEvaluator、expressionHandler、securityInterceptor； HttpSecurity：具体的权限控制规则配置。一个这个配置相当于xml配置中的一个标签。各种具体的认证机制的相关配置，OpenIDLoginConfigurer、AnonymousConfigurer、FormLoginConfigurer、HttpBasicConfigurer等。 WebSecurityConfigurerAdapter提供了简洁方式来创建WebSecurityConfigurer，其作为基类，可通过实现该类自定义配置类，主要重写这三个方法： 123protected void configure(AuthenticationManagerBuilder auth) throws Exception &#123;&#125;public void configure(WebSecurity web) throws Exception &#123;&#125;protected void configure(HttpSecurity httpSecurity) throws Exception &#123;&#125; 而且其自动从SpringFactoriesLoader查找AbstractHttpConfigurer让我们去扩展，想要实现必须创建一个AbstractHttpConfigurer的扩展类，并在classpath路径下创建一个文件META-INF/spring.factories。例如： 1org.springframework.security.config.annotation.web.configurers.AbstractHttpConfigurer = sample.MyClassThatExtendsAbstractHttpConfigurer 其源码分析： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980//1.init初始化：获取HttpSecurity和配置FilterSecurityInterceptor拦截器到WebSecuritypublic void init(final WebSecurity web) throws Exception &#123; //获取HttpSecurity final HttpSecurity http = getHttp(); //配置FilterSecurityInterceptor拦截器到WebSecurity web.addSecurityFilterChainBuilder(http).postBuildAction(new Runnable() &#123; public void run() &#123; FilterSecurityInterceptor securityInterceptor = http .getSharedObject(FilterSecurityInterceptor.class); web.securityInterceptor(securityInterceptor); &#125; &#125;); &#125; .....//2.获取HttpSecurity的过程 protected final HttpSecurity getHttp() throws Exception &#123; if (http != null) &#123; return http; &#125; DefaultAuthenticationEventPublisher eventPublisher = objectPostProcessor .postProcess(new DefaultAuthenticationEventPublisher()); localConfigureAuthenticationBldr.authenticationEventPublisher(eventPublisher); AuthenticationManager authenticationManager = authenticationManager(); authenticationBuilder.parentAuthenticationManager(authenticationManager); Map&lt;Class&lt;? extends Object&gt;, Object&gt; sharedObjects = createSharedObjects(); http = new HttpSecurity(objectPostProcessor, authenticationBuilder, sharedObjects); if (!disableDefaults) &#123; // 默认的HttpSecurity的配置 http //添加 CSRF 支持，使用WebSecurityConfigurerAdapter时，默认启用，禁用csrf().disable() .csrf().and() //添加WebAsyncManagerIntegrationFilter .addFilter(new WebAsyncManagerIntegrationFilter()) //允许配置异常处理 .exceptionHandling().and() //将安全标头添加到响应 .headers().and() //允许配置会话管理 .sessionManagement().and() //HttpServletRequest之间的SecurityContextHolder创建securityContext管理 .securityContext().and() //允许配置请求缓存 .requestCache().and() //允许配置匿名用户 .anonymous().and() //HttpServletRequestd的方法和属性注册在SecurityContext中 .servletApi().and() //使用默认登录页面 .apply(new DefaultLoginPageConfigurer&lt;&gt;()).and() //提供注销支持 .logout(); // @formatter:on ClassLoader classLoader = this.context.getClassLoader(); List&lt;AbstractHttpConfigurer&gt; defaultHttpConfigurers = SpringFactoriesLoader.loadFactories(AbstractHttpConfigurer.class, classLoader); for(AbstractHttpConfigurer configurer : defaultHttpConfigurers) &#123; http.apply(configurer); &#125; &#125; configure(http); return http; &#125; ..... //3.可重写方法实现自定义的HttpSecurity protected void configure(HttpSecurity http) throws Exception &#123; logger.debug("Using default configure(HttpSecurity). If subclassed this will potentially override subclass configure(HttpSecurity)."); http .authorizeRequests() .anyRequest().authenticated() .and() .formLogin().and() .httpBasic(); &#125; ..... 从源码init初始化模块中的“获取HttpSecurity”和“配置FilterSecurityInterceptor拦截器到WebSecurity”中可以看出，想要spring Security如何知道我们要求所有用户都经过身份验证？ Spring Security如何知道我们想要支持基于表单的身份验证？只要重写protected void configure(HttpSecurity http) throws Exception方法即可。因此我们需要理解HttpSecurity的方法的作用，如何进行配置。下一节来讨论HttpSecurity。 HttpSecurity HttpSecurity基于Web的安全性允许为特定的http请求进行配置。其有很多方法，列举一些常用的如下表： 方法 说明 使用案例 csrf() 添加 CSRF 支持，使用WebSecurityConfigurerAdapter时，默认启用 禁用：csrf().disable() openidLogin() 用于基于 OpenId 的验证 openidLogin().permitAll(); authorizeRequests() 开启使用HttpServletRequest请求的访问限制 authorizeRequests().anyRequest().authenticated() formLogin() 开启表单的身份验证，如果未指定FormLoginConfigurer#loginPage(String)，则将生成默认登录页面 formLogin().loginPage(“/authentication/login”).failureUrl(“/authentication/login?failed”) oauth2Login() 开启OAuth 2.0或OpenID Connect 1.0身份验证 authorizeRequests()..anyRequest().authenticated()..and().oauth2Login() rememberMe() 开启配置“记住我”的验证 authorizeRequests().antMatchers(“/**”).hasRole(“USER”).and().formLogin().permitAll().and().rememberMe() addFilter() 添加自定义的filter addFilter(new CustomFilter()) addFilterAt() 在指定filter相同位置上添加自定义filter addFilterAt(new CustomFilter(), UsernamePasswordAuthenticationFilter.class) addFilterAfter() 在指定filter位置后添加自定义filter addFilterAfter(new CustomFilter(), UsernamePasswordAuthenticationFilter.class) requestMatchers() 开启配置HttpSecurity，仅当RequestMatcher相匹配时开启 requestMatchers().antMatchers(“/api/**”) antMatchers() 其可以与authorizeRequests()、RequestMatcher匹配，如：requestMatchers().antMatchers(“/api/**”) logout() 添加退出登录支持。当使用WebSecurityConfigurerAdapter时，这将自动应用。默认情况是，访问URL”/ logout”，使HTTP Session无效来清除用户，清除已配置的任何#rememberMe()身份验证，清除SecurityContextHolder，然后重定向到”/login?success” logout().deleteCookies(“remove”).invalidateHttpSession(false).logoutUrl(“/custom-logout”).logoutSuccessUrl(“/logout-success”); HttpSecurity还有很多方法供我们使用，去配置HttpSecurity。由于太多这边就不一一说明，有兴趣可去研究。 WebSecurityConfigurerAdapter 使用WebSecurityConfigurerAdapter示例： 12345678910111213141516171819202122232425262728@Configuration@EnableWebSecuritypublic class WebSecurityConfig extends WebSecurityConfigurerAdapter &#123; @Autowired private MyFilterSecurityInterceptor myFilterSecurityInterceptor; protected void configure(HttpSecurity http) throws Exception &#123; http //request 设置 .authorizeRequests() //http.authorizeRequests() 方法中的自定义匹配 .antMatchers("/resources/**", "/signup", "/about").permitAll() // 指定所有用户进行访问指定的url .antMatchers("/admin/**").hasRole("ADMIN") //指定具有特定权限的用户才能访问特定目录，hasRole()方法指定用户权限，且不需前缀 “ROLE_“ .antMatchers("/db/**").access("hasRole('ADMIN') and hasRole('DBA')")// .anyRequest().authenticated() //任何请求没匹配的都需要进行验证 .and() //login设置 自定义登录页面且允许所有用户登录 .formLogin() .loginPage("/login") //The updated configuration specifies the location of the log in page 指定自定义登录页面 .permitAll(); // 允许所有用户访问登录页面. The formLogin().permitAll() 方法 .and .logout() //logouts 设置 .logoutUrl("/my/logout") // 指定注销路径 .logoutSuccessUrl("/my/index") //指定成功注销后跳转到指定的页面 .logoutSuccessHandler(logoutSuccessHandler) //指定成功注销后处理类 如果使用了logoutSuccessHandler()的话， logoutSuccessUrl()就会失效 .invalidateHttpSession(true) // httpSession是否有效时间，如果使用了 SecurityContextLogoutHandler，其将被覆盖 .addLogoutHandler(logoutHandler) //在最后增加默认的注销处理类LogoutHandler .deleteCookies(cookieNamesToClear);//指定注销成功后remove cookies //增加在FilterSecurityInterceptor前添加自定义的myFilterSecurityInterceptor http.addFilterBefore(myFilterSecurityInterceptor, FilterSecurityInterceptor.class); &#125; NOTE：此示例只供参考 filter 顺序Spring Security filter顺序： Filter Class 说明 ChannelProcessingFilter 访问协议控制过滤器，可能会将我们重新定向到另外一种协议,从http转换成https SecurityContextPersistenceFilter 创建SecurityContext安全上下文信息和request结束时清空SecurityContextHolder ConcurrentSessionFilter 并发访问控制过滤器,主要功能：SessionRegistry中获取SessionInformation来判断session是否过期，从而实现并发访问控制。 HeaderWriterFilter 给http response添加一些Header CsrfFilter 跨域过滤器，跨站请求伪造保护Filter LogoutFilter 处理退出登录的Filter X509AuthenticationFilter 添加X509预授权处理机制支持 CasAuthenticationFilter 认证filter，经过这些过滤器后SecurityContextHolder中将包含一个完全组装好的Authentication对象，从而使后续鉴权能正常执行 UsernamePasswordAuthenticationFilter 认证的filter，经过这些过滤器后SecurityContextHolder中将包含一个完全组装好的Authentication对象，从而使后续鉴权能正常执行。表单认证是最常用的一个认证方式。 BasicAuthenticationFilter 认证filter，经过这些过滤器后SecurityContextHolder中将包含一个完全组装好的Authentication对象，从而使后续鉴权能正常执行 SecurityContextHolderAwareRequestFilter 此过滤器对ServletRequest进行了一次包装，使得request具有更加丰富的API JaasApiIntegrationFilter (JAAS)认证方式filter RememberMeAuthenticationFilter 记忆认证处理过滤器，即是如果前面认证过滤器没有对当前的请求进行处理，启用了RememberMe功能，会从cookie中解析出用户，并进行认证处理，之后在SecurityContextHolder中存入一个Authentication对象。 AnonymousAuthenticationFilter 匿名认证处理过滤器，当SecurityContextHolder中认证信息为空,则会创建一个匿名用户存入到SecurityContextHolder中 SessionManagementFilter 会话管理Filter，持久化用户登录信息，可以保存到session中，也可以保存到cookie或者redis中 ExceptionTranslationFilter 异常处理过滤器，主要拦截后续过滤器（FilterSecurityInterceptor）操作中抛出的异常。 FilterSecurityInterceptor 安全拦截过滤器类，获取当前请求url对应的ConfigAttribute，并调用accessDecisionManager进行访问授权决策。 spring security的默认filter链: SecurityContextPersistenceFilter-&gt;HeaderWriterFilter-&gt;LogoutFilter-&gt;UsernamePasswordAuthenticationFilter-&gt;RequestCacheAwareFilter-&gt;SecurityContextHolderAwareRequestFilter-&gt;SessionManagementFilter-&gt;ExceptionTranslationFilter-&gt;FilterSecurityInterceptor 在上节我们已分析了核心的filter源码以及功能。可回看上节源码分析更加深入的了解各个filter工作原理。 总结： 在认证和访问授权过程前，首先必须进行WebSecurityConfigurer符合自身应用的security Configurer，也要清楚filter链的先后顺序，才能更好理解spring security的工作原理以及在项目中出现的问题定位。]]></content>
      <categories>
        <category>Spring Security</category>
      </categories>
      <tags>
        <tag>Spring Security</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同源策略]]></title>
    <url>%2F2020%2F05%2F29%2F%E5%90%8C%E6%BA%90%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[【转载】 作者：laixiangran 链接：https://juejin.im/post/5ba1d4fe6fb9a05ce873d4ad 什么是浏览器同源策略同源策略（Same origin policy）是一种约定，它是浏览器最核心也最基本的安全功能，如果缺少了同源策略，则浏览器的正常功能可能都会受到影响。可以说 Web 是构建在同源策略基础之上的，浏览器只是针对同源策略的一种实现。 它的核心就在于它认为自任何站点装载的信赖内容是不安全的。当被浏览器半信半疑的脚本运行在沙箱时，它们应该只被允许访问来自同一站点的资源，而不是那些来自其它站点可能怀有恶意的资源。 所谓同源是指：域名、协议、端口相同。 下表是相对于 http://www.laixiangran.cn/home/index.html 的同源检测结果： URL 结果 原因 http://www.laixiangran.cn/home/index.html 成功 域名、协议、端口均不同 https://www.laixiangran.cn/home/index.html 失败 协议不同 http://www.laixiangran.cn:8080/home/index.html 失败 端口不同 http://www.laixiangran.cn/other/index.html 失败 域名不同 另外，同源策略又分为以下两种： DOM 同源策略：禁止对不同源页面 DOM 进行操作。这里主要场景是 iframe 跨域的情况，不同域名的 iframe 是限制互相访问的。 XMLHttpRequest 同源策略：禁止使用 XHR 对象向不同源的服务器地址发起 HTTP 请求。 为什么要有跨域限制因为存在浏览器同源策略，所以才会有跨域问题。那么浏览器是出于何种原因会有跨域的限制呢。其实不难想到，跨域限制主要的目的就是为了用户的上网安全。 如果浏览器没有同源策略，会存在什么样的安全问题呢。下面从 DOM 同源策略和 XMLHttpRequest 同源策略来举例说明： 如果没有 DOM 同源策略，也就是说不同域的 iframe 之间可以相互访问，那么黑客可以这样进行攻击： 做一个假网站，里面用 iframe 嵌套一个银行网站 http://mybank.com。 把 iframe 宽高啥的调整到页面全部，这样用户进来除了域名，别的部分和银行的网站没有任何差别。 这时如果用户输入账号密码，我们的主网站可以跨域访问到 http://mybank.com 的 dom 节点，就可以拿到用户的账户密码了。 如果没有 XMLHttpRequest 同源策略，那么黑客可以进行 CSRF（跨站请求伪造） 攻击： 用户登录了自己的银行页面 http://mybank.com，http://mybank.com 向用户的 cookie 中添加用户标识。 用户浏览了恶意页面 http://evil.com，执行了页面中的恶意 AJAX 请求代码。 http://evil.com 向 http://mybank.com 发起 AJAX HTTP 请求，请求会默认把 http://mybank.com 对应 cookie 也同时发送过去。 银行页面从发送的 cookie 中提取用户标识，验证用户无误，response 中返回请求数据。此时数据就泄露了。 而且由于 Ajax 在后台执行，用户无法感知这一过程。 因此，有了浏览器同源策略，我们才能更安全的上网。 跨域的解决方法从上面我们了解到了浏览器同源策略的作用，也正是有了跨域限制，才使我们能安全的上网。但是在实际中，有时候我们需要突破这样的限制，因此下面将介绍几种跨域的解决方法。 CORS（跨域资源共享）CORS（Cross-origin resource sharing，跨域资源共享）是一个 W3C 标准，定义了在必须访问跨域资源时，浏览器与服务器应该如何沟通。CORS 背后的基本思想，就是使用自定义的 HTTP 头部让浏览器与服务器进行沟通，从而决定请求或响应是应该成功，还是应该失败。 CORS 需要浏览器和服务器同时支持。目前，所有浏览器都支持该功能，IE 浏览器不能低于 IE10。 整个 CORS 通信过程，都是浏览器自动完成，不需要用户参与。对于开发者来说，CORS 通信与同源的 AJAX 通信没有差别，代码完全一样。浏览器一旦发现 AJAX 请求跨源，就会自动添加一些附加的头信息，有时还会多出一次附加的请求，但用户不会有感觉。 因此，实现 CORS 通信的关键是服务器。只要服务器实现了 CORS 接口，就可以跨源通信。 浏览器将CORS请求分成两类：简单请求（simple request）和非简单请求（not-so-simple request）。 只要同时满足以下两大条件，就属于简单请求。 请求方法是以下三种方法之一： HEAD GET POST HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值 application/x-www-form-urlencoded、multipart/form-data、text/plain 凡是不同时满足上面两个条件，就属于非简单请求。 浏览器对这两种请求的处理，是不一样的。 简单请求 在请求中需要附加一个额外的 Origin 头部，其中包含请求页面的源信息（协议、域名和端口），以便服务器根据这个头部信息来决定是否给予响应。例如：Origin: http://www.laixiangran.cn 如果服务器认为这个请求可以接受，就在 Access-Control-Allow-Origin 头部中回发相同的源信息（如果是公共资源，可以回发 * ）。例如：Access-Control-Allow-Origin：http://www.laixiangran.cn 没有这个头部或者有这个头部但源信息不匹配，浏览器就会驳回请求。正常情况下，浏览器会处理请求。注意，请求和响应都不包含 cookie 信息。 如果需要包含 cookie 信息，ajax 请求需要设置 xhr 的属性 withCredentials 为 true，服务器需要设置响应头部 Access-Control-Allow-Credentials: true。 非简单请求浏览器在发送真正的请求之前，会先发送一个 Preflight 请求给服务器，这种请求使用 OPTIONS 方法，发送下列头部： Origin：与简单的请求相同。 Access-Control-Request-Method: 请求自身使用的方法。 Access-Control-Request-Headers: （可选）自定义的头部信息，多个头部以逗号分隔。 例如： 123Origin: http://www.laixiangran.cnAccess-Control-Request-Method: POSTAccess-Control-Request-Headers: NCZ 发送这个请求后，服务器可以决定是否允许这种类型的请求。服务器通过在响应中发送如下头部与浏览器进行沟通： Access-Control-Allow-Origin：与简单的请求相同。 Access-Control-Allow-Methods: 允许的方法，多个方法以逗号分隔。 Access-Control-Allow-Headers: 允许的头部，多个方法以逗号分隔。 Access-Control-Max-Age: 应该将这个 Preflight 请求缓存多长时间（以秒表示）。 例如： 1234Access-Control-Allow-Origin: http://www.laixiangran.cnAccess-Control-Allow-Methods: GET, POSTAccess-Control-Allow-Headers: NCZAccess-Control-Max-Age: 1728000 一旦服务器通过 Preflight 请求允许该请求之后，以后每次浏览器正常的 CORS 请求，就都跟简单请求一样了。 优点 CORS 通信与同源的 AJAX 通信没有差别，代码完全一样，容易维护。 支持所有类型的 HTTP 请求。 缺点 存在兼容性问题，特别是 IE10 以下的浏览器。 第一次发送非简单请求时会多一次请求。 JSONP 跨域由于 script 标签不受浏览器同源策略的影响，允许跨域引用资源。因此可以通过动态创建 script 标签，然后利用 src 属性进行跨域，这也就是 JSONP 跨域的基本原理。 直接通过下面的例子来说明 JSONP 实现跨域的流程： 1234567891011121314// 1. 定义一个 回调函数 handleResponse 用来接收返回的数据function handleResponse(data) &#123; console.log(data);&#125;;// 2. 动态创建一个 script 标签，并且告诉后端回调函数名叫 handleResponsevar body = document.getElementsByTagName('body')[0];var script = document.gerElement('script');script.src = 'http://www.laixiangran.cn/json?callback=handleResponse';body.appendChild(script);// 3. 通过 script.src 请求 `http://www.laixiangran.cn/json?callback=handleResponse`，// 4. 后端能够识别这样的 URL 格式并处理该请求，然后返回 handleResponse(&#123;"name": "laixiangran"&#125;) 给浏览器// 5. 浏览器在接收到 handleResponse(&#123;"name": "laixiangran"&#125;) 之后立即执行 ，也就是执行 handleResponse 方法，获得后端返回的数据，这样就完成一次跨域请求了。 优点 使用简便，没有兼容性问题，目前最流行的一种跨域方法。 缺点 只支持 GET 请求。 由于是从其它域中加载代码执行，因此如果其他域不安全，很可能会在响应中夹带一些恶意代码。 要确定 JSONP 请求是否失败并不容易。虽然 HTML5 给 script 标签新增了一个 onerror 事件处理程序，但是存在兼容性问题。 图像 Ping 跨域由于 img 标签不受浏览器同源策略的影响，允许跨域引用资源。因此可以通过 img 标签的 src 属性进行跨域，这也就是图像 Ping 跨域的基本原理。 直接通过下面的例子来说明图像 Ping 实现跨域的流程： 123456789var img = new Image();// 通过 onload 及 onerror 事件可以知道响应是什么时候接收到的，但是不能获取响应文本img.onload = img.onerror = function() &#123; console.log("Done!");&#125;// 请求数据通过查询字符串形式发送img.src = 'http://www.laixiangran.cn/test?name=laixiangran'; 优点 用于实现跟踪用户点击页面或动态广告曝光次数有较大的优势。 缺点 只支持 GET 请求。 只能浏览器与服务器的单向通信，因为浏览器不能访问服务器的响应文本。 服务器代理浏览器有跨域限制，但是服务器不存在跨域问题，所以可以由服务器请求所有域的资源再返回给客户端。 服务器代理是万能的。 document.domain 跨域对于主域名相同，而子域名不同的情况，可以使用 document.domain 来跨域。这种方式非常适用于 iframe 跨域的情况。 比如，有一个页面，它的地址是 http://www.laixiangran.cn/a.html，在这个页面里面有一个 iframe，它的 src 是 http://laixiangran.cn/b.html。很显然，这个页面与它里面的 iframe 框架是不同域的，所以我们是无法通过在页面中书写 js 代码来获取 iframe 中的东西的。 这个时候，document.domain 就可以派上用场了，我们只要把 http://www.laixiangran.cn/a.html 和 http://laixiangran.cn/b.html 这两个页面的 document.domain 都设成相同的域名就可以了。但要注意的是，document.domain 的设置是有限制的，我们只能把 document.domain 设置成自身或更高一级的父域，且主域必须相同。例如：a.b.laixiangran.cn 中某个文档的 document.domain 可以设成 a.b.laixiangran.cn、b.laixiangran.cn 、laixiangran.cn 中的任意一个，但是不可以设成 c.a.b.laixiangran.cn ，因为这是当前域的子域，也不可以设成 baidu.com，因为主域已经不相同了。 例如，在页面 http://www.laixiangran.cn/a.html 中设置document.domain： 1234567&lt;iframe src="http://laixiangran.cn/b.html" id="myIframe" onload="test()"&gt;&lt;script&gt; document.domain = 'laixiangran.cn'; // 设置成主域 function test() &#123; console.log(document.getElementById('myIframe').contentWindow); &#125;&lt;/script&gt; 在页面 http://laixiangran.cn/b.html 中也设置 document.domain，而且这也是必须的，虽然这个文档的 domain 就是 laixiangran.cn，但是还是必须显式地设置 document.domain 的值： 123&lt;script&gt; document.domain = 'laixiangran.cn'; // document.domain 设置成与主页面相同&lt;/script&gt; 这样，http://www.laixiangran.cn/a.html 就可以通过 js 访问到 http://laixiangran.cn/b.html 中的各种属性和对象了。 window.name 跨域window 对象有个 name 属性，该属性有个特征：即在一个窗口（window）的生命周期内，窗口载入的所有的页面（不管是相同域的页面还是不同域的页面）都是共享一个 window.name 的，每个页面对 window.name 都有读写的权限，window.name 是持久存在一个窗口载入过的所有页面中的，并不会因新页面的载入而进行重置。 通过下面的例子介绍如何通过 window.name 来跨域获取数据的。 页面 http://www.laixiangran.cn/a.html 的代码： 123456789101112131415161718&lt;iframe src="http://laixiangran.cn/b.html" id="myIframe" onload="test()" style="display: none;"&gt;&lt;script&gt; // 2. iframe载入 "http://laixiangran.cn/b.html 页面后会执行该函数 function test() &#123; var iframe = document.getElementById('myIframe'); // 重置 iframe 的 onload 事件程序， // 此时经过后面代码重置 src 之后， // http://www.laixiangran.cn/a.html 页面与该 iframe 在同一个源了，可以相互访问了 iframe.onload = function() &#123; var data = iframe.contentWindow.name; // 4. 获取 iframe 里的 window.name console.log(data); // hello world! &#125;; // 3. 重置一个与 http://www.laixiangran.cn/a.html 页面同源的页面 iframe.src = 'http://www.laixiangran.cn/c.html'; &#125;&lt;/script&gt; 页面 http://laixiangran.cn/b.html 的代码： 1234&lt;script type="text/javascript"&gt; // 1. 给当前的 window.name 设置一个 http://www.laixiangran.cn/a.html 页面想要得到的数据值 window.name = "hello world!";&lt;/script&gt; location.hash 跨域location.hash 方式跨域，是子框架修改父框架 src 的 hash 值，通过这个属性进行传递数据，且更改 hash 值，页面不会刷新。但是传递的数据的字节数是有限的。 页面 http://www.laixiangran.cn/a.html 的代码： 123456789&lt;iframe src="http://laixiangran.cn/b.html" id="myIframe" onload="test()" style="display: none;"&gt;&lt;script&gt; // 2. iframe载入 "http://laixiangran.cn/b.html 页面后会执行该函数 function test() &#123; // 3. 获取通过 http://laixiangran.cn/b.html 页面设置 hash 值 var data = window.location.hash; console.log(data); &#125;&lt;/script&gt; 页面 http://laixiangran.cn/b.html 的代码： 1234&lt;script type="text/javascript"&gt; // 1. 设置父页面的 hash 值 parent.location.hash = "world";&lt;/script&gt; postMessage 跨域window.postMessage(message，targetOrigin) 方法是 HTML5 新引进的特性，可以使用它来向其它的 window 对象发送消息，无论这个 window 对象是属于同源或不同源。这个应该就是以后解决 dom 跨域通用方法了。 调用 postMessage 方法的 window 对象是指要接收消息的那一个 window 对象，该方法的第一个参数 message 为要发送的消息，类型只能为字符串；第二个参数 targetOrigin 用来限定接收消息的那个 window 对象所在的域，如果不想限定域，可以使用通配符 *。 需要接收消息的 window 对象，可是通过监听自身的 message 事件来获取传过来的消息，消息内容储存在该事件对象的 data 属性中。 页面 http://www.laixiangran.cn/a.html 的代码： 1234567891011&lt;iframe src="http://laixiangran.cn/b.html" id="myIframe" onload="test()" style="display: none;"&gt;&lt;script&gt; // 1. iframe载入 "http://laixiangran.cn/b.html 页面后会执行该函数 function test() &#123; // 2. 获取 http://laixiangran.cn/b.html 页面的 window 对象， // 然后通过 postMessage 向 http://laixiangran.cn/b.html 页面发送消息 var iframe = document.getElementById('myIframe'); var win = iframe.contentWindow; win.postMessage('我是来自 http://www.laixiangran.cn/a.html 页面的消息', '*'); &#125;&lt;/script&gt; 页面 http://laixiangran.cn/b.html 的代码： 1234567&lt;script type="text/javascript"&gt; // 注册 message 事件用来接收消息 window.onmessage = function(e) &#123; e = e || event; // 获取事件对象 console.log(e.data); // 通过 data 属性得到发送来的消息 &#125;&lt;/script&gt;]]></content>
      <categories>
        <category>同源策略</category>
      </categories>
      <tags>
        <tag>同源策略</tag>
        <tag>浏览器</tag>
        <tag>CORF</tag>
        <tag>跨域</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring data jpa 如何实现动态部分更新]]></title>
    <url>%2F2019%2F12%2F08%2FSpringDataJpa%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E9%83%A8%E5%88%86%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[创建表 contact 12345678CREATE TABLE `contact` ( `id` int(32) NOT NULL AUTO_INCREMENT, `name` varchar(32) NOT NULL, `mobile` varchar(20) NOT NULL, `address` varchar(255), `create_time` datetime, PRIMARY KEY (`id`)); 创建实体 Contact 12345678910111213141516@Data@Entity(name = "contact")public class Contact &#123; @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Integer id; private String name; private String mobile; private String address; @Column(name = "create_time") private Date createTime;&#125; 假设前端修改了某个用户的联系电话，需要更新数据库中某个用户的联系电话： 前端的 json ： 1234567&gt; &#123;&gt; "id": 1,&gt; "name": "张三",&gt; "mobile": "13522222222",&gt; "address": "广东省"&gt; &#125;&gt; 后台执行的代码： 1234&gt; if(contact.getId() != null)&#123;&gt; contactRepository.save(contact);&gt; &#125;&gt; 如果直接执行这段代码，会把之前的create_time信息置为null 原因：Spring Data Jpa 对Entity的更新是对主键以外的所有字段更新，而不是传的参数更新，执行的sql是： 1update contact set name=?, mobile=?, address=?, create_time=? where id=? Spring Data Jpa save方法的源码： 1234567891011@Transactional @Override public &lt;S extends T&gt; S save(S entity) &#123; if (entityInformation.isNew(entity)) &#123; em.persist(entity); return entity; &#125; else &#123; return em.merge(entity); &#125; &#125; Spring Data Jpa 会依据主键先执行一边查询，如果不存在这条记录则执行插入，如果存在则执行覆盖操作。 但是这里就有问题了，Spring Data Jpa 无法理解开发者的意图， 即开发者到底是想用NULL值覆盖原值，还是想遇到NULL值而忽略。 对于部分字段更新，可以使用@Query和@Modify注解手写sql来进行部分更新， 但是动态更新无法做到。 什么是动态更新? 不是对数据记录的所有字段整体更新，而是直到运行时才确定哪个或者哪些字段会被更新 如： 只更改了mobile，执行的是：update contact set mobile=? where id=? 更改了mobile和address，执行的是：update contact set mobile=?, address=? where id=? 使用@DynamicUpdate 在实体类上使用注解@DynamicUpdate注解，这个注解是由hibernate提供的 这时就可以实现动态更新了，在运行时修改哪些字段，会动态生成对应的sql，但是仍然存在无法理解开发者对于NULL值的意图。默认是如果原值不为 NULL , 则用NULL值覆盖，如果想要实现不覆盖原值，需要自行实现接口或者工具类。]]></content>
      <categories>
        <category>Spring Data</category>
        <category>Spring Data Jpa</category>
      </categories>
      <tags>
        <tag>Spring Data Jpa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker安装常用软件]]></title>
    <url>%2F2019%2F12%2F07%2Fdocker%E5%AE%89%E8%A3%85%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[docker 安装常用软件 如果配置开机启动：后面加 --restart=always nginx 下载: docker pull nginx 运行: docker run --name nginx -p 80:80 -d docker.io/nginx 配置 nginx.conf 进入 nginx 容器: docker exec -it nginx /bin/bash 进入 /etc/nginx 目录: cd /etc/nginx vi或者vim编辑: vim nginx.conf PS：因为nginx里面没有自带vim或者vi，所以需要自行安装 在/etc/nginx目录下： apt-get update 更新 apt-get install vim 下载 vim redis单机版 下载: docker pull redis:latest 运行: docker run -itd --name redis -p 6379:6379 redis + --requirepass &quot;******&quot; (运行时就设置密码) 进入redis容器里面(可选): docker exec -it redis /bin/bash 连接redis(可选): redis-cli 查看是否有设置密码(可选): config get requirepass 设置密码(可选): config set requirepass **** mysql 下载: docker pull mysql:latest 运行: docker run -itd --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 mysql zookeeper单机版 下载: docker pull zookeeper:latest 运行: docker run -d --name zookeeper -p 2181:2181 zookeeper 进入zookeeper容器里面(可选): docker exec -it zookeeper /bin/bash kafuka单机版 下载: docker pull wurstmeister/kafka 运行: 先运行zookeeper docker run -d --name zookeeper -p 2181:2181 zookeeper 再启动kafuka 1docker run -d --name kafka -p 9092:9092 -e KAFKA_BROKER_ID=0 -e KAFKA_ZOOKEEPER_CONNECT=192.168.1.100:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.1.100:9092 -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 -t wurstmeister/kafka 解释： 上面的命令，主要设置四个参数 KAFKA_BROKER_ID=0KAFKA_ZOOKEEPER_CONNECT=192.168.1.100:2181KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.1.100:9092KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092中间两个参数的192.168.1.100改为宿主机器的IP地址，如果不这么设置，可能会导致在别的机器上访问不到kafka。 测试kafuka: 进入容器: docker exec -it kafka /bin/bash 进入kafka所在目录: cd opt/kafka_2.11-2.5.0/ 启动消息发送方: ./bin/kafka-console-producer.sh --broker-list localhost:9092 --topic mykafka 克隆会话 进入kafka所在目录: cd opt/kafka_2.11-2.5.0/ 启动消息接收方: ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mykafka --from-beginning mongo 下载并运行: docker run -itd --name mongo -v /data/mongo/data:/data/db -p 27017:27017 mongo --auth 注意：默认数据是存在容器系统的/data/db目录下的 –auth是开启认证 设置密码 进入容器: docker exec -it mongo bash 连接到mongo: mongo 查看当前版本: db.version() 切换到admin库: use admin 创建管理员权限用户: db.createUser({ user:&#39;admin&#39;,pwd:&#39;123456&#39;,roles:[ { role:&#39;userAdminAnyDatabase&#39;, db: &#39;admin&#39;}]}); 认证: db.auth(&#39;admin&#39;,&#39;123456&#39;); 创建普通读写 并指定库用户: db.createUser({ user:&#39;user&#39;,pwd:&#39;123456&#39;,roles:[ { role:&#39;readWrite&#39;, db: &#39;testdb&#39;}]}); 必须要在admin库认证: db.auth(&#39;user&#39;,&#39;123456&#39;) 切换到testdb库: use testdb 插入一条数据到person集合中: db.person.insert({name:&#39;libai-go&#39;,age:18}) 查找person集合所有数据: db.person.find({}) elasticsearch 创建一个网络，同一个网络的内的容器，可以通过localhost:port 通信，方便kibana访问es: docker network create somenetwork 下载并运行: docker run -d --name elasticsearch --net somenetwork -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; elasticsearch kibana 下载并运行: docker run -d --name kibana --net somenetwork -p 5601:5601 kibana:7.6.2]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[部署Springboot服务到云服务器上]]></title>
    <url>%2F2019%2F12%2F06%2F%E9%83%A8%E7%BD%B2%E6%9C%8D%E5%8A%A1%E5%88%B0%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[部署一个java服务器到自己的云服务器上 以腾讯云服务器为例 设置安全组，开放一些常用端口在菜单里选择安全组配置一条安全组规则，添加完后会多一行记录： 选择添加规则，设置入站规则和出站规则： 将这个安全组规则关联到自己的云主机上： 使用xshell访问云服务器当开放完22端口后，就可以去生成公私钥，把私钥下载后，使用xshell访问云服务器的公网IP，会出现输入账号，密码和密钥。把下载好的密钥加载进来，就可以使用xshell访问了。以后再详细写。。 下载必要的软件 PS: 暂时还不懂使用 docker 部署服务，所以用最原始的方式部署 我部署的是springboot项目，所以需要使用到java环境，先安装jdk。 安装 JDK查看是否已经安装了JDKwhereis javawhich java 如果有jdk会输出所在位置，如果没有不输出，或者输出找不到 卸载旧版本的JDK找到JDK的位置：rpm -qa | grep jdk 根据位置卸载： 安装想要的JDK版本我这里下载的是 jdk8 下载地址 ，下载 tar.gz 格式的文件 下载完后，使用 xftp 上载到云服务器上，我这里放到了/usr/local 。 解压到当前文件夹上： tar zxvf jdk8.tar.gz 将该jdk配置到环境变量上去： vi /etc/profile 文件末尾追加： 123456JAVA_HOME=/usr/local/jdk1.8.0_231JRE_HOME=/usr/local/jdk1.8.0_231/jrePATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binCLASSPATH=:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport JAVA_HOME JRE_HOME PATH CLASSPATH 保存退出后，使其立即生效： source /etc/profile 创建Test.java文件: 12345public class Test &#123; public static void main(String[] args) &#123; System.out.println("hello world !"); &#125; &#125; 编译并运行，如果输出hello wold则配置完成 javac Test.java java Test 安装 Docker因为我的云服务器安装的是CentOs7 ，所以使用yum下载 安装 Docker： yum -y install docker 启动 Docker：systemctl start docker 验证是否成功：docker run hello-world 如果输出hello world则成功 下载想要运行的软件我这里需要用到Mysql：docker pull mysql 运行 mysql，这里没有设置复杂的密码：docker run -itd --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 docker.io/mysql 进入mysql容器：docker exec -it mysql /bin/bash 输入账号：mysql -uroot -p 输入密码： 进入mysql后，创建数据库并使用：create database mini; 和 use mini; 退出mysql命令行模式：exit 退出 docker 容器：exit maven 打包使用命令或者ide进行打包 如果云上的配置和本地的配置不一样，可以打包后，解压打开，用云上的配置替换本地的配置 在 xshell 运行springboot进入到springboot项目存放的位置，运行java -jar命令 直接用云服务器的公网访问自己写的api]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>运维</tag>
        <tag>云服务器</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring缓存注解]]></title>
    <url>%2F2019%2F12%2F04%2FSpring%E7%BC%93%E5%AD%98%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Spring 3.1 引入了基于注解的缓存技术，本质是一个对缓存使用的抽象，通过在既有代码中添加少量它定义的各种注解，就能够达到缓存方法的返回对象的效果。 Spring 的缓存技术还具备相当的灵活性，不仅能够使用 SpEL（Spring Expression Language）来定义缓存的 key 和各种 condition，还提供开箱即用的缓存临时存储方案。 @Cacheable12]]></content>
      <categories>
        <category>注解</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>注解</tag>
        <tag>Spring</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo常用命令]]></title>
    <url>%2F2019%2F11%2F28%2Fhexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[安装hexonpm install hexo -g 更新版本 npm update hexo -g 初始化hexo init 命令&amp;简写新建文章hexo new &quot;&lt;title&gt;&quot; === hexo n &quot;&lt;title&gt;&quot; 新建草稿hexo new draft &quot;&lt;title&gt;&quot; === hexo n draft &quot;&lt;title&gt;&quot; 草稿相当于“私密文章”功能。 会在source/_drafts目录下生成一个草稿的markdown文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到_drafts目录之中。 如果你希望强行预览草稿，更改配置文件：render_drafts: true 或者本地预览的命令加参数：hexo server --drafts 将草稿发布成文章hexo publish &quot;&lt;title&gt;&quot; === hexo p &quot;&lt;title&gt;&quot; 生成静态网页 hexo generate === hexo g 清除页面缓存hexo clean 本地启动hexo服务预览，修改内容会更新页面hexo server === hexo s hexo server -s #静态模式 hexo server -p 5000 #更改端口 hexo server -i 192.168.1.1 #自定义 IP 部署到线上hexo deploy === hexo d 组合命令，可以配置在package.json上 12345&gt; "scripts": &#123;&gt; "localStart": "hexo clean &amp;&amp; hexo g &amp;&amp; hexo s ",&gt; "blogDeployee": "hexo clean &amp;&amp; hexo g &amp;&amp; hexo d "&gt; &#125;&gt;]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot的properties]]></title>
    <url>%2F2019%2F09%2F30%2FSpringboot%E7%9A%84properties%2F</url>
    <content type="text"><![CDATA[Springboot基本的配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741751761771781791801811821831841851861871881891901911921931941951961971981992002012022032042052062072082092102112122132142152162172182192202212222232242252262272282292302312322332342352362372382392402412422432442452462472482492502512522532542552562572582592602612622632642652662672682692702712722732742752762772782792802812822832842852862872882892902912922932942952962972982993003013023033043053063073083093103113123133143153163173183193203213223233243253263273283293303313323333343353363373383393403413423433443453463473483493503513523533543553563573583593603613623633643653663673683693703713723733743753763773783793803813823833843853863873883893903913923933943953963973983994004014024034044054064074084094104114124134144154164174184194204214224234244254264274284294304314324334344354364374384394404414424434444454464474484494504514524534544554564574584594604614624634644654664674684694704714724734744754764774784794804814824834844854864874884894904914924934944954964974984995005015025035045055065075085095105115125135145155165175185195205215225235245255265275285295305315325335345355365375385395405415425435445455465475485495505515525535545555565575585595605615625635645655665675685695705715725735745755765775785795805815825835845855865875885895905915925935945955965975985996006016026036046056066076086096106116126136146156166176186196206216226236246256266276286296306316326336346356366376386396406416426436446456466476486496506516526536546556566576586596606616626636646656666676686696706716726736746756766776786796806816826836846856866876886896906916926936946956966976986997007017027037047057067077087097107117127137147157167177187197207217227237247257267277287297307317327337347357367377387397407417427437447457467477487497507517527537547557567577587597607617627637647657667677687697707717727737747757767777787797807817827837847857867877887897907917927937947957967977987998008018028038048058068078088098108118128138148158168178188198208218228238248258268278288298308318328338348358368378388398408418428438448458468478488498508518528538548558568578588598608618628638648658668678688698708718728738748758768778788798808818828838848858868878888898908918928938948958968978988999009019029039049059069079089099109119129139149159169179189199209219229239249259269279289299309319329339349359369379389399409419429439449459469479489499509519529539549559569579589599609619629639649659669679689699709719729739749759769779789799809819829839849859869879889899909919929939949959969979989991000100110021003100410051006100710081009101010111012101310141015101610171018101910201021102210231024102510261027102810291030103110321033103410351036103710381039104010411042104310441045104610471048104910501051105210531054105510561057105810591060106110621063106410651066106710681069107010711072107310741075107610771078107910801081108210831084108510861087108810891090109110921093109410951096109710981099110011011102110311041105110611071108110911101111111211131114111511161117111811191120112111221123112411251126112711281129113011311132113311341135113611371138113911401141114211431144114511461147114811491150115111521153115411551156115711581159116011611162116311641165116611671168116911701171117211731174117511761177117811791180118111821183118411851186118711881189119011911192119311941195119611971198119912001201120212031204120512061207120812091210121112121213121412151216121712181219122012211222122312241225122612271228122912301231123212331234123512361237123812391240124112421243124412451246124712481249125012511252125312541255125612571258125912601261126212631264126512661267126812691270127112721273127412751276127712781279128012811282128312841285128612871288128912901291129212931294129512961297129812991300130113021303130413051306130713081309131013111312131313141315131613171318131913201321132213231324132513261327132813291330133113321333133413351336133713381339134013411342134313441345134613471348134913501351135213531354135513561357135813591360136113621363136413651366136713681369137013711372137313741375137613771378137913801381138213831384138513861387138813891390139113921393139413951396139713981399140014011402140314041405140614071408140914101411141214131414141514161417141814191420142114221423142414251426142714281429143014311432143314341435143614371438143914401441144214431444144514461447144814491450145114521453145414551456145714581459146014611462146314641465146614671468146914701471147214731474147514761477147814791480148114821483148414851486148714881489149014911492149314941495149614971498149915001501150215031504150515061507150815091510151115121513151415151516151715181519152015211522152315241525152615271528152915301531153215331534153515361537153815391540154115421543154415451546154715481549155015511552155315541555155615571558155915601561# ===================================================================# COMMON SPRING BOOT PROPERTIES## This sample file is provided as a guideline. Do NOT copy it in its# entirety to your own application. ^^^# ===================================================================# ----------------------------------------# CORE PROPERTIES# ----------------------------------------debug=false # Enable debug logs.trace=false # Enable trace logs.# LOGGINGlogging.config= # Location of the logging configuration file. For instance, `classpath:logback.xml` for Logback.logging.exception-conversion-word=%wEx # Conversion word used when logging exceptions.logging.file= # Log file name (for instance, `myapp.log`). Names can be an exact location or relative to the current directory.logging.file.max-history=0 # Maximum of archive log files to keep. Only supported with the default logback setup.logging.file.max-size=10MB # Maximum log file size. Only supported with the default logback setup.logging.group.*= # Log groups to quickly change multiple loggers at the same time. For instance, `logging.level.db=org.hibernate,org.springframework.jdbc`.logging.level.*= # Log levels severity mapping. For instance, `logging.level.org.springframework=DEBUG`.logging.path= # Location of the log file. For instance, `/var/log`.logging.pattern.console= # Appender pattern for output to the console. Supported only with the default Logback setup.logging.pattern.dateformat=yyyy-MM-dd HH:mm:ss.SSS # Appender pattern for log date format. Supported only with the default Logback setup.logging.pattern.file= # Appender pattern for output to a file. Supported only with the default Logback setup.logging.pattern.level=%5p # Appender pattern for log level. Supported only with the default Logback setup.logging.register-shutdown-hook=false # Register a shutdown hook for the logging system when it is initialized.# AOPspring.aop.auto=true # Add @EnableAspectJAutoProxy.spring.aop.proxy-target-class=true # Whether subclass-based (CGLIB) proxies are to be created (true), as opposed to standard Java interface-based proxies (false).# IDENTITY (ContextIdApplicationContextInitializer)spring.application.name= # Application name.# ADMIN (SpringApplicationAdminJmxAutoConfiguration)spring.application.admin.enabled=false # Whether to enable admin features for the application.spring.application.admin.jmx-name=org.springframework.boot:type=Admin,name=SpringApplication # JMX name of the application admin MBean.# AUTO-CONFIGURATIONspring.autoconfigure.exclude= # Auto-configuration classes to exclude.# BANNERspring.banner.charset=UTF-8 # Banner file encoding.spring.banner.location=classpath:banner.txt # Banner text resource location.spring.banner.image.location=classpath:banner.gif # Banner image file location (jpg or png can also be used).spring.banner.image.width=76 # Width of the banner image in chars.spring.banner.image.height= # Height of the banner image in chars (default based on image height).spring.banner.image.margin=2 # Left hand image margin in chars.spring.banner.image.invert=false # Whether images should be inverted for dark terminal themes.# SPRING COREspring.beaninfo.ignore=true # Whether to skip search of BeanInfo classes.# SPRING CACHE (CacheProperties)spring.cache.cache-names= # Comma-separated list of cache names to create if supported by the underlying cache manager.spring.cache.caffeine.spec= # The spec to use to create caches. See CaffeineSpec for more details on the spec format.spring.cache.couchbase.expiration= # Entry expiration. By default the entries never expire. Note that this value is ultimately converted to seconds.spring.cache.ehcache.config= # The location of the configuration file to use to initialize EhCache.spring.cache.infinispan.config= # The location of the configuration file to use to initialize Infinispan.spring.cache.jcache.config= # The location of the configuration file to use to initialize the cache manager.spring.cache.jcache.provider= # Fully qualified name of the CachingProvider implementation to use to retrieve the JSR-107 compliant cache manager. Needed only if more than one JSR-107 implementation is available on the classpath.spring.cache.redis.cache-null-values=true # Allow caching null values.spring.cache.redis.key-prefix= # Key prefix.spring.cache.redis.time-to-live= # Entry expiration. By default the entries never expire.spring.cache.redis.use-key-prefix=true # Whether to use the key prefix when writing to Redis.spring.cache.type= # Cache type. By default, auto-detected according to the environment.# SPRING CONFIG - using environment property only (ConfigFileApplicationListener)spring.config.additional-location= # Config file locations used in addition to the defaults.spring.config.location= # Config file locations that replace the defaults.spring.config.name=application # Config file name.# HAZELCAST (HazelcastProperties)spring.hazelcast.config= # The location of the configuration file to use to initialize Hazelcast.# PROJECT INFORMATION (ProjectInfoProperties)spring.info.build.encoding=UTF-8 # File encoding.spring.info.build.location=classpath:META-INF/build-info.properties # Location of the generated build-info.properties file.spring.info.git.encoding=UTF-8 # File encoding.spring.info.git.location=classpath:git.properties # Location of the generated git.properties file.# JMXspring.jmx.default-domain= # JMX domain name.spring.jmx.enabled=true # Expose management beans to the JMX domain.spring.jmx.server=mbeanServer # MBeanServer bean name.spring.jmx.unique-names=false # Whether unique runtime object names should be ensured.# Email (MailProperties)spring.mail.default-encoding=UTF-8 # Default MimeMessage encoding.spring.mail.host= # SMTP server host. For instance, `smtp.example.com`.spring.mail.jndi-name= # Session JNDI name. When set, takes precedence over other Session settings.spring.mail.password= # Login password of the SMTP server.spring.mail.port= # SMTP server port.spring.mail.properties.*= # Additional JavaMail Session properties.spring.mail.protocol=smtp # Protocol used by the SMTP server.spring.mail.test-connection=false # Whether to test that the mail server is available on startup.spring.mail.username= # Login user of the SMTP server.# APPLICATION SETTINGS (SpringApplication)spring.main.allow-bean-definition-overriding=false # Whether bean definition overriding, by registering a definition with the same name as an existing definition, is allowed.spring.main.banner-mode=console # Mode used to display the banner when the application runs.spring.main.sources= # Sources (class names, package names, or XML resource locations) to include in the ApplicationContext.spring.main.web-application-type= # Flag to explicitly request a specific type of web application. If not set, auto-detected based on the classpath.# FILE ENCODING (FileEncodingApplicationListener)spring.mandatory-file-encoding= # Expected character encoding the application must use.# INTERNATIONALIZATION (MessageSourceProperties)spring.messages.always-use-message-format=false # Whether to always apply the MessageFormat rules, parsing even messages without arguments.spring.messages.basename=messages # Comma-separated list of basenames (essentially a fully-qualified classpath location), each following the ResourceBundle convention with relaxed support for slash based locations.spring.messages.cache-duration= # Loaded resource bundle files cache duration. When not set, bundles are cached forever. If a duration suffix is not specified, seconds will be used.spring.messages.encoding=UTF-8 # Message bundles encoding.spring.messages.fallback-to-system-locale=true # Whether to fall back to the system Locale if no files for a specific Locale have been found.spring.messages.use-code-as-default-message=false # Whether to use the message code as the default message instead of throwing a &quot;NoSuchMessageException&quot;. Recommended during development only.# OUTPUTspring.output.ansi.enabled=detect # Configures the ANSI output.# PID FILE (ApplicationPidFileWriter)spring.pid.fail-on-write-error= # Fails if ApplicationPidFileWriter is used but it cannot write the PID file.spring.pid.file= # Location of the PID file to write (if ApplicationPidFileWriter is used).# PROFILESspring.profiles.active= # Comma-separated list of active profiles. Can be overridden by a command line switch.spring.profiles.include= # Unconditionally activate the specified comma-separated list of profiles (or list of profiles if using YAML).# QUARTZ SCHEDULER (QuartzProperties)spring.quartz.auto-startup=true # Whether to automatically start the scheduler after initialization.spring.quartz.jdbc.comment-prefix=-- # Prefix for single-line comments in SQL initialization scripts.spring.quartz.jdbc.initialize-schema=embedded # Database schema initialization mode.spring.quartz.jdbc.schema=classpath:org/quartz/impl/jdbcjobstore/tables_@@platform@@.sql # Path to the SQL file to use to initialize the database schema.spring.quartz.job-store-type=memory # Quartz job store type.spring.quartz.overwrite-existing-jobs=false # Whether configured jobs should overwrite existing job definitions.spring.quartz.properties.*= # Additional Quartz Scheduler properties.spring.quartz.scheduler-name=quartzScheduler # Name of the scheduler.spring.quartz.startup-delay=0s # Delay after which the scheduler is started once initialization completes.spring.quartz.wait-for-jobs-to-complete-on-shutdown=false # Whether to wait for running jobs to complete on shutdown.# REACTOR (ReactorCoreProperties)spring.reactor.stacktrace-mode.enabled=false # Whether Reactor should collect stacktrace information at runtime.# SENDGRID (SendGridAutoConfiguration)spring.sendgrid.api-key= # SendGrid API key.spring.sendgrid.proxy.host= # SendGrid proxy host.spring.sendgrid.proxy.port= # SendGrid proxy port.# TASK EXECUTION (TaskExecutionProperties)spring.task.execution.pool.allow-core-thread-timeout=true # Whether core threads are allowed to time out. This enables dynamic growing and shrinking of the pool.spring.task.execution.pool.core-size=8 # Core number of threads.spring.task.execution.pool.keep-alive=60s # Time limit for which threads may remain idle before being terminated.spring.task.execution.pool.max-size= # Maximum allowed number of threads. If tasks are filling up the queue, the pool can expand up to that size to accommodate the load. Ignored if the queue is unbounded.spring.task.execution.pool.queue-capacity= # Queue capacity. An unbounded capacity does not increase the pool and therefore ignores the &quot;max-size&quot; property.spring.task.execution.thread-name-prefix=task- # Prefix to use for the names of newly created threads.# TASK SCHEDULING (TaskSchedulingProperties)spring.task.scheduling.pool.size=1 # Maximum allowed number of threads.spring.task.scheduling.thread-name-prefix=scheduling- # Prefix to use for the names of newly created threads.# ----------------------------------------# WEB PROPERTIES# ----------------------------------------# EMBEDDED SERVER CONFIGURATION (ServerProperties)server.address= # Network address to which the server should bind.server.compression.enabled=false # Whether response compression is enabled.server.compression.excluded-user-agents= # Comma-separated list of user agents for which responses should not be compressed.server.compression.mime-types=text/html,text/xml,text/plain,text/css,text/javascript,application/javascript,application/json,application/xml # Comma-separated list of MIME types that should be compressed.server.compression.min-response-size=2KB # Minimum &quot;Content-Length&quot; value that is required for compression to be performed.server.connection-timeout= # Time that connectors wait for another HTTP request before closing the connection. When not set, the connector&apos;s container-specific default is used. Use a value of -1 to indicate no (that is, an infinite) timeout.server.error.include-exception=false # Include the &quot;exception&quot; attribute.server.error.include-stacktrace=never # When to include a &quot;stacktrace&quot; attribute.server.error.path=/error # Path of the error controller.server.error.whitelabel.enabled=true # Whether to enable the default error page displayed in browsers in case of a server error.server.http2.enabled=false # Whether to enable HTTP/2 support, if the current environment supports it.server.jetty.acceptors=-1 # Number of acceptor threads to use. When the value is -1, the default, the number of acceptors is derived from the operating environment.server.jetty.accesslog.append=false # Append to log.server.jetty.accesslog.date-format=dd/MMM/yyyy:HH:mm:ss Z # Timestamp format of the request log.server.jetty.accesslog.enabled=false # Enable access log.server.jetty.accesslog.extended-format=false # Enable extended NCSA format.server.jetty.accesslog.file-date-format= # Date format to place in log file name.server.jetty.accesslog.filename= # Log filename. If not specified, logs redirect to &quot;System.err&quot;.server.jetty.accesslog.locale= # Locale of the request log.server.jetty.accesslog.log-cookies=false # Enable logging of the request cookies.server.jetty.accesslog.log-latency=false # Enable logging of request processing time.server.jetty.accesslog.log-server=false # Enable logging of the request hostname.server.jetty.accesslog.retention-period=31 # Number of days before rotated log files are deleted.server.jetty.accesslog.time-zone=GMT # Timezone of the request log.server.jetty.max-http-post-size=200000B # Maximum size of the HTTP post or put content.server.jetty.selectors=-1 # Number of selector threads to use. When the value is -1, the default, the number of selectors is derived from the operating environment.server.max-http-header-size=8KB # Maximum size of the HTTP message header.server.port=8080 # Server HTTP port.server.server-header= # Value to use for the Server response header (if empty, no header is sent).server.use-forward-headers= # Whether X-Forwarded-* headers should be applied to the HttpRequest.server.servlet.context-parameters.*= # Servlet context init parameters.server.servlet.context-path= # Context path of the application.server.servlet.application-display-name=application # Display name of the application.server.servlet.jsp.class-name=org.apache.jasper.servlet.JspServlet # Class name of the servlet to use for JSPs.server.servlet.jsp.init-parameters.*= # Init parameters used to configure the JSP servlet.server.servlet.jsp.registered=true # Whether the JSP servlet is registered.server.servlet.session.cookie.comment= # Comment for the session cookie.server.servlet.session.cookie.domain= # Domain for the session cookie.server.servlet.session.cookie.http-only= # Whether to use &quot;HttpOnly&quot; cookies for session cookies.server.servlet.session.cookie.max-age= # Maximum age of the session cookie. If a duration suffix is not specified, seconds will be used.server.servlet.session.cookie.name= # Session cookie name.server.servlet.session.cookie.path= # Path of the session cookie.server.servlet.session.cookie.secure= # Whether to always mark the session cookie as secure.server.servlet.session.persistent=false # Whether to persist session data between restarts.server.servlet.session.store-dir= # Directory used to store session data.server.servlet.session.timeout=30m # Session timeout. If a duration suffix is not specified, seconds will be used.server.servlet.session.tracking-modes= # Session tracking modes.server.ssl.ciphers= # Supported SSL ciphers.server.ssl.client-auth= # Client authentication mode.server.ssl.enabled=true # Whether to enable SSL support.server.ssl.enabled-protocols= # Enabled SSL protocols.server.ssl.key-alias= # Alias that identifies the key in the key store.server.ssl.key-password= # Password used to access the key in the key store.server.ssl.key-store= # Path to the key store that holds the SSL certificate (typically a jks file).server.ssl.key-store-password= # Password used to access the key store.server.ssl.key-store-provider= # Provider for the key store.server.ssl.key-store-type= # Type of the key store.server.ssl.protocol=TLS # SSL protocol to use.server.ssl.trust-store= # Trust store that holds SSL certificates.server.ssl.trust-store-password= # Password used to access the trust store.server.ssl.trust-store-provider= # Provider for the trust store.server.ssl.trust-store-type= # Type of the trust store.server.tomcat.accept-count=100 # Maximum queue length for incoming connection requests when all possible request processing threads are in use.server.tomcat.accesslog.buffered=true # Whether to buffer output such that it is flushed only periodically.server.tomcat.accesslog.directory=logs # Directory in which log files are created. Can be absolute or relative to the Tomcat base dir.server.tomcat.accesslog.enabled=false # Enable access log.server.tomcat.accesslog.file-date-format=.yyyy-MM-dd # Date format to place in the log file name.server.tomcat.accesslog.pattern=common # Format pattern for access logs.server.tomcat.accesslog.prefix=access_log # Log file name prefix.server.tomcat.accesslog.rename-on-rotate=false # Whether to defer inclusion of the date stamp in the file name until rotate time.server.tomcat.accesslog.request-attributes-enabled=false # Set request attributes for the IP address, Hostname, protocol, and port used for the request.server.tomcat.accesslog.rotate=true # Whether to enable access log rotation.server.tomcat.accesslog.suffix=.log # Log file name suffix.server.tomcat.additional-tld-skip-patterns= # Comma-separated list of additional patterns that match jars to ignore for TLD scanning.server.tomcat.background-processor-delay=10s # Delay between the invocation of backgroundProcess methods. If a duration suffix is not specified, seconds will be used.server.tomcat.basedir= # Tomcat base directory. If not specified, a temporary directory is used.server.tomcat.internal-proxies=10\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;|\\ 192\\.168\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;|\\ 169\\.254\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;|\\ 127\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;|\\ 172\\.1[6-9]&#123;1&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;|\\ 172\\.2[0-9]&#123;1&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;|\\ 172\\.3[0-1]&#123;1&#125;\\.\\d&#123;1,3&#125;\\.\\d&#123;1,3&#125;\\ 0:0:0:0:0:0:0:1\\ ::1 # Regular expression that matches proxies that are to be trusted.server.tomcat.max-connections=10000 # Maximum number of connections that the server accepts and processes at any given time.server.tomcat.max-http-post-size=2MB # Maximum size of the HTTP post content.server.tomcat.max-swallow-size=2MB # Maximum amount of request body to swallow.server.tomcat.max-threads=200 # Maximum amount of worker threads.server.tomcat.min-spare-threads=10 # Minimum amount of worker threads.server.tomcat.port-header=X-Forwarded-Port # Name of the HTTP header used to override the original port value.server.tomcat.protocol-header= # Header that holds the incoming protocol, usually named &quot;X-Forwarded-Proto&quot;.server.tomcat.protocol-header-https-value=https # Value of the protocol header indicating whether the incoming request uses SSL.server.tomcat.redirect-context-root=true # Whether requests to the context root should be redirected by appending a / to the path.server.tomcat.remote-ip-header= # Name of the HTTP header from which the remote IP is extracted. For instance, `X-FORWARDED-FOR`.server.tomcat.resource.allow-caching=true # Whether static resource caching is permitted for this web application.server.tomcat.resource.cache-ttl= # Time-to-live of the static resource cache.server.tomcat.uri-encoding=UTF-8 # Character encoding to use to decode the URI.server.tomcat.use-relative-redirects= # Whether HTTP 1.1 and later location headers generated by a call to sendRedirect will use relative or absolute redirects.server.undertow.accesslog.dir= # Undertow access log directory.server.undertow.accesslog.enabled=false # Whether to enable the access log.server.undertow.accesslog.pattern=common # Format pattern for access logs.server.undertow.accesslog.prefix=access_log. # Log file name prefix.server.undertow.accesslog.rotate=true # Whether to enable access log rotation.server.undertow.accesslog.suffix=log # Log file name suffix.server.undertow.buffer-size= # Size of each buffer.server.undertow.direct-buffers= # Whether to allocate buffers outside the Java heap. The default is derived from the maximum amount of memory that is available to the JVM.server.undertow.eager-filter-init=true # Whether servlet filters should be initialized on startup.server.undertow.io-threads= # Number of I/O threads to create for the worker. The default is derived from the number of available processors.server.undertow.max-http-post-size=-1B # Maximum size of the HTTP post content. When the value is -1, the default, the size is unlimited.server.undertow.worker-threads= # Number of worker threads. The default is 8 times the number of I/O threads.# FREEMARKER (FreeMarkerProperties)spring.freemarker.allow-request-override=false # Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name.spring.freemarker.allow-session-override=false # Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name.spring.freemarker.cache=false # Whether to enable template caching.spring.freemarker.charset=UTF-8 # Template encoding.spring.freemarker.check-template-location=true # Whether to check that the templates location exists.spring.freemarker.content-type=text/html # Content-Type value.spring.freemarker.enabled=true # Whether to enable MVC view resolution for this technology.spring.freemarker.expose-request-attributes=false # Whether all request attributes should be added to the model prior to merging with the template.spring.freemarker.expose-session-attributes=false # Whether all HttpSession attributes should be added to the model prior to merging with the template.spring.freemarker.expose-spring-macro-helpers=true # Whether to expose a RequestContext for use by Spring&apos;s macro library, under the name &quot;springMacroRequestContext&quot;.spring.freemarker.prefer-file-system-access=true # Whether to prefer file system access for template loading. File system access enables hot detection of template changes.spring.freemarker.prefix= # Prefix that gets prepended to view names when building a URL.spring.freemarker.request-context-attribute= # Name of the RequestContext attribute for all views.spring.freemarker.settings.*= # Well-known FreeMarker keys which are passed to FreeMarker&apos;s Configuration.spring.freemarker.suffix=.ftl # Suffix that gets appended to view names when building a URL.spring.freemarker.template-loader-path=classpath:/templates/ # Comma-separated list of template paths.spring.freemarker.view-names= # White list of view names that can be resolved.# GROOVY TEMPLATES (GroovyTemplateProperties)spring.groovy.template.allow-request-override=false # Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name.spring.groovy.template.allow-session-override=false # Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name.spring.groovy.template.cache=false # Whether to enable template caching.spring.groovy.template.charset=UTF-8 # Template encoding.spring.groovy.template.check-template-location=true # Whether to check that the templates location exists.spring.groovy.template.configuration.*= # See GroovyMarkupConfigurerspring.groovy.template.content-type=text/html # Content-Type value.spring.groovy.template.enabled=true # Whether to enable MVC view resolution for this technology.spring.groovy.template.expose-request-attributes=false # Whether all request attributes should be added to the model prior to merging with the template.spring.groovy.template.expose-session-attributes=false # Whether all HttpSession attributes should be added to the model prior to merging with the template.spring.groovy.template.expose-spring-macro-helpers=true # Whether to expose a RequestContext for use by Spring&apos;s macro library, under the name &quot;springMacroRequestContext&quot;.spring.groovy.template.prefix= # Prefix that gets prepended to view names when building a URL.spring.groovy.template.request-context-attribute= # Name of the RequestContext attribute for all views.spring.groovy.template.resource-loader-path=classpath:/templates/ # Template path.spring.groovy.template.suffix=.tpl # Suffix that gets appended to view names when building a URL.spring.groovy.template.view-names= # White list of view names that can be resolved.# SPRING HATEOAS (HateoasProperties)spring.hateoas.use-hal-as-default-json-media-type=true # Whether application/hal+json responses should be sent to requests that accept application/json.# HTTP (HttpProperties)spring.http.converters.preferred-json-mapper= # Preferred JSON mapper to use for HTTP message conversion. By default, auto-detected according to the environment.spring.http.encoding.charset=UTF-8 # Charset of HTTP requests and responses. Added to the &quot;Content-Type&quot; header if not set explicitly.spring.http.encoding.enabled=true # Whether to enable http encoding support.spring.http.encoding.force= # Whether to force the encoding to the configured charset on HTTP requests and responses.spring.http.encoding.force-request= # Whether to force the encoding to the configured charset on HTTP requests. Defaults to true when &quot;force&quot; has not been specified.spring.http.encoding.force-response= # Whether to force the encoding to the configured charset on HTTP responses.spring.http.encoding.mapping= # Locale in which to encode mapping.spring.http.log-request-details=false # Whether logging of (potentially sensitive) request details at DEBUG and TRACE level is allowed.# MULTIPART (MultipartProperties)spring.servlet.multipart.enabled=true # Whether to enable support of multipart uploads.spring.servlet.multipart.file-size-threshold=0B # Threshold after which files are written to disk.spring.servlet.multipart.location= # Intermediate location of uploaded files.spring.servlet.multipart.max-file-size=1MB # Max file size.spring.servlet.multipart.max-request-size=10MB # Max request size.spring.servlet.multipart.resolve-lazily=false # Whether to resolve the multipart request lazily at the time of file or parameter access.# JACKSON (JacksonProperties)spring.jackson.date-format= # Date format string or a fully-qualified date format class name. For instance, `yyyy-MM-dd HH:mm:ss`.spring.jackson.default-property-inclusion= # Controls the inclusion of properties during serialization. Configured with one of the values in Jackson&apos;s JsonInclude.Include enumeration.spring.jackson.deserialization.*= # Jackson on/off features that affect the way Java objects are deserialized.spring.jackson.generator.*= # Jackson on/off features for generators.spring.jackson.joda-date-time-format= # Joda date time format string. If not configured, &quot;date-format&quot; is used as a fallback if it is configured with a format string.spring.jackson.locale= # Locale used for formatting.spring.jackson.mapper.*= # Jackson general purpose on/off features.spring.jackson.parser.*= # Jackson on/off features for parsers.spring.jackson.property-naming-strategy= # One of the constants on Jackson&apos;s PropertyNamingStrategy. Can also be a fully-qualified class name of a PropertyNamingStrategy subclass.spring.jackson.serialization.*= # Jackson on/off features that affect the way Java objects are serialized.spring.jackson.time-zone= # Time zone used when formatting dates. For instance, &quot;America/Los_Angeles&quot; or &quot;GMT+10&quot;.spring.jackson.visibility.*= # Jackson visibility thresholds that can be used to limit which methods (and fields) are auto-detected.# GSON (GsonProperties)spring.gson.date-format= # Format to use when serializing Date objects.spring.gson.disable-html-escaping= # Whether to disable the escaping of HTML characters such as &apos;&lt;&apos;, &apos;&gt;&apos;, etc.spring.gson.disable-inner-class-serialization= # Whether to exclude inner classes during serialization.spring.gson.enable-complex-map-key-serialization= # Whether to enable serialization of complex map keys (i.e. non-primitives).spring.gson.exclude-fields-without-expose-annotation= # Whether to exclude all fields from consideration for serialization or deserialization that do not have the &quot;Expose&quot; annotation.spring.gson.field-naming-policy= # Naming policy that should be applied to an object&apos;s field during serialization and deserialization.spring.gson.generate-non-executable-json= # Whether to generate non executable JSON by prefixing the output with some special text.spring.gson.lenient= # Whether to be lenient about parsing JSON that doesn&apos;t conform to RFC 4627.spring.gson.long-serialization-policy= # Serialization policy for Long and long types.spring.gson.pretty-printing= # Whether to output serialized JSON that fits in a page for pretty printing.spring.gson.serialize-nulls= # Whether to serialize null fields.# JERSEY (JerseyProperties)spring.jersey.application-path= # Path that serves as the base URI for the application. If specified, overrides the value of &quot;@ApplicationPath&quot;.spring.jersey.filter.order=0 # Jersey filter chain order.spring.jersey.init.*= # Init parameters to pass to Jersey through the servlet or filter.spring.jersey.servlet.load-on-startup=-1 # Load on startup priority of the Jersey servlet.spring.jersey.type=servlet # Jersey integration type.# SPRING LDAP (LdapProperties)spring.ldap.anonymous-read-only=false # Whether read-only operations should use an anonymous environment.spring.ldap.base= # Base suffix from which all operations should originate.spring.ldap.base-environment.*= # LDAP specification settings.spring.ldap.password= # Login password of the server.spring.ldap.urls= # LDAP URLs of the server.spring.ldap.username= # Login username of the server.# EMBEDDED LDAP (EmbeddedLdapProperties)spring.ldap.embedded.base-dn= # List of base DNs.spring.ldap.embedded.credential.username= # Embedded LDAP username.spring.ldap.embedded.credential.password= # Embedded LDAP password.spring.ldap.embedded.ldif=classpath:schema.ldif # Schema (LDIF) script resource reference.spring.ldap.embedded.port=0 # Embedded LDAP port.spring.ldap.embedded.validation.enabled=true # Whether to enable LDAP schema validation.spring.ldap.embedded.validation.schema= # Path to the custom schema.# MUSTACHE TEMPLATES (MustacheAutoConfiguration)spring.mustache.allow-request-override=false # Whether HttpServletRequest attributes are allowed to override (hide) controller generated model attributes of the same name.spring.mustache.allow-session-override=false # Whether HttpSession attributes are allowed to override (hide) controller generated model attributes of the same name.spring.mustache.cache=false # Whether to enable template caching.spring.mustache.charset=UTF-8 # Template encoding.spring.mustache.check-template-location=true # Whether to check that the templates location exists.spring.mustache.content-type=text/html # Content-Type value.spring.mustache.enabled=true # Whether to enable MVC view resolution for this technology.spring.mustache.expose-request-attributes=false # Whether all request attributes should be added to the model prior to merging with the template.spring.mustache.expose-session-attributes=false # Whether all HttpSession attributes should be added to the model prior to merging with the template.spring.mustache.expose-spring-macro-helpers=true # Whether to expose a RequestContext for use by Spring&apos;s macro library, under the name &quot;springMacroRequestContext&quot;.spring.mustache.prefix=classpath:/templates/ # Prefix to apply to template names.spring.mustache.request-context-attribute= # Name of the RequestContext attribute for all views.spring.mustache.suffix=.mustache # Suffix to apply to template names.spring.mustache.view-names= # White list of view names that can be resolved.# SPRING MVC (WebMvcProperties)spring.mvc.async.request-timeout= # Amount of time before asynchronous request handling times out.spring.mvc.contentnegotiation.favor-parameter=false # Whether a request parameter (&quot;format&quot; by default) should be used to determine the requested media type.spring.mvc.contentnegotiation.favor-path-extension=false # Whether the path extension in the URL path should be used to determine the requested media type.spring.mvc.contentnegotiation.media-types.*= # Map file extensions to media types for content negotiation. For instance, yml to text/yaml.spring.mvc.contentnegotiation.parameter-name= # Query parameter name to use when &quot;favor-parameter&quot; is enabled.spring.mvc.date-format= # Date format to use. For instance, `dd/MM/yyyy`.spring.mvc.dispatch-trace-request=false # Whether to dispatch TRACE requests to the FrameworkServlet doService method.spring.mvc.dispatch-options-request=true # Whether to dispatch OPTIONS requests to the FrameworkServlet doService method.spring.mvc.favicon.enabled=true # Whether to enable resolution of favicon.ico.spring.mvc.formcontent.filter.enabled=true # Whether to enable Spring&apos;s FormContentFilter.spring.mvc.hiddenmethod.filter.enabled=true # Whether to enable Spring&apos;s HiddenHttpMethodFilter.spring.mvc.ignore-default-model-on-redirect=true # Whether the content of the &quot;default&quot; model should be ignored during redirect scenarios.spring.mvc.locale= # Locale to use. By default, this locale is overridden by the &quot;Accept-Language&quot; header.spring.mvc.locale-resolver=accept-header # Define how the locale should be resolved.spring.mvc.log-resolved-exception=false # Whether to enable warn logging of exceptions resolved by a &quot;HandlerExceptionResolver&quot;, except for &quot;DefaultHandlerExceptionResolver&quot;.spring.mvc.message-codes-resolver-format= # Formatting strategy for message codes. For instance, `PREFIX_ERROR_CODE`.spring.mvc.pathmatch.use-registered-suffix-pattern=false # Whether suffix pattern matching should work only against extensions registered with &quot;spring.mvc.contentnegotiation.media-types.*&quot;.spring.mvc.pathmatch.use-suffix-pattern=false # Whether to use suffix pattern match (&quot;.*&quot;) when matching patterns to requests.spring.mvc.servlet.load-on-startup=-1 # Load on startup priority of the dispatcher servlet.spring.mvc.servlet.path=/ # Path of the dispatcher servlet.spring.mvc.static-path-pattern=/** # Path pattern used for static resources.spring.mvc.throw-exception-if-no-handler-found=false # Whether a &quot;NoHandlerFoundException&quot; should be thrown if no Handler was found to process a request.spring.mvc.view.prefix= # Spring MVC view prefix.spring.mvc.view.suffix= # Spring MVC view suffix.# SPRING RESOURCES HANDLING (ResourceProperties)spring.resources.add-mappings=true # Whether to enable default resource handling.spring.resources.cache.cachecontrol.cache-private= # Indicate that the response message is intended for a single user and must not be stored by a shared cache.spring.resources.cache.cachecontrol.cache-public= # Indicate that any cache may store the response.spring.resources.cache.cachecontrol.max-age= # Maximum time the response should be cached, in seconds if no duration suffix is not specified.spring.resources.cache.cachecontrol.must-revalidate= # Indicate that once it has become stale, a cache must not use the response without re-validating it with the server.spring.resources.cache.cachecontrol.no-cache= # Indicate that the cached response can be reused only if re-validated with the server.spring.resources.cache.cachecontrol.no-store= # Indicate to not cache the response in any case.spring.resources.cache.cachecontrol.no-transform= # Indicate intermediaries (caches and others) that they should not transform the response content.spring.resources.cache.cachecontrol.proxy-revalidate= # Same meaning as the &quot;must-revalidate&quot; directive, except that it does not apply to private caches.spring.resources.cache.cachecontrol.s-max-age= # Maximum time the response should be cached by shared caches, in seconds if no duration suffix is not specified.spring.resources.cache.cachecontrol.stale-if-error= # Maximum time the response may be used when errors are encountered, in seconds if no duration suffix is not specified.spring.resources.cache.cachecontrol.stale-while-revalidate= # Maximum time the response can be served after it becomes stale, in seconds if no duration suffix is not specified.spring.resources.cache.period= # Cache period for the resources served by the resource handler. If a duration suffix is not specified, seconds will be used.spring.resources.chain.cache=true # Whether to enable caching in the Resource chain.spring.resources.chain.compressed=false # Whether to enable resolution of already compressed resources (gzip, brotli).spring.resources.chain.enabled= # Whether to enable the Spring Resource Handling chain. By default, disabled unless at least one strategy has been enabled.spring.resources.chain.html-application-cache=false # Whether to enable HTML5 application cache manifest rewriting.spring.resources.chain.strategy.content.enabled=false # Whether to enable the content Version Strategy.spring.resources.chain.strategy.content.paths=/** # Comma-separated list of patterns to apply to the content Version Strategy.spring.resources.chain.strategy.fixed.enabled=false # Whether to enable the fixed Version Strategy.spring.resources.chain.strategy.fixed.paths=/** # Comma-separated list of patterns to apply to the fixed Version Strategy.spring.resources.chain.strategy.fixed.version= # Version string to use for the fixed Version Strategy.spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/ # Locations of static resources.# SPRING SESSION (SessionProperties)spring.session.store-type= # Session store type.spring.session.timeout= # Session timeout. If a duration suffix is not specified, seconds will be used.spring.session.servlet.filter-order=-2147483598 # Session repository filter order.spring.session.servlet.filter-dispatcher-types=async,error,request # Session repository filter dispatcher types.# SPRING SESSION HAZELCAST (HazelcastSessionProperties)spring.session.hazelcast.flush-mode=on-save # Sessions flush mode.spring.session.hazelcast.map-name=spring:session:sessions # Name of the map used to store sessions.# SPRING SESSION JDBC (JdbcSessionProperties)spring.session.jdbc.cleanup-cron=0 * * * * * # Cron expression for expired session cleanup job.spring.session.jdbc.initialize-schema=embedded # Database schema initialization mode.spring.session.jdbc.schema=classpath:org/springframework/session/jdbc/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema.spring.session.jdbc.table-name=SPRING_SESSION # Name of the database table used to store sessions.# SPRING SESSION MONGODB (MongoSessionProperties)spring.session.mongodb.collection-name=sessions # Collection name used to store sessions.# SPRING SESSION REDIS (RedisSessionProperties)spring.session.redis.cleanup-cron=0 * * * * * # Cron expression for expired session cleanup job.spring.session.redis.flush-mode=on-save # Sessions flush mode.spring.session.redis.namespace=spring:session # Namespace for keys used to store sessions.# THYMELEAF (ThymeleafAutoConfiguration)spring.thymeleaf.cache=true # Whether to enable template caching.spring.thymeleaf.check-template=true # Whether to check that the template exists before rendering it.spring.thymeleaf.check-template-location=true # Whether to check that the templates location exists.spring.thymeleaf.enabled=true # Whether to enable Thymeleaf view resolution for Web frameworks.spring.thymeleaf.enable-spring-el-compiler=false # Enable the SpringEL compiler in SpringEL expressions.spring.thymeleaf.encoding=UTF-8 # Template files encoding.spring.thymeleaf.excluded-view-names= # Comma-separated list of view names (patterns allowed) that should be excluded from resolution.spring.thymeleaf.mode=HTML # Template mode to be applied to templates. See also Thymeleaf&apos;s TemplateMode enum.spring.thymeleaf.prefix=classpath:/templates/ # Prefix that gets prepended to view names when building a URL.spring.thymeleaf.reactive.chunked-mode-view-names= # Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set.spring.thymeleaf.reactive.full-mode-view-names= # Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set.spring.thymeleaf.reactive.max-chunk-size=0B # Maximum size of data buffers used for writing to the response.spring.thymeleaf.reactive.media-types= # Media types supported by the view technology.spring.thymeleaf.render-hidden-markers-before-checkboxes=false # Whether hidden form inputs acting as markers for checkboxes should be rendered before the checkbox element itself.spring.thymeleaf.servlet.content-type=text/html # Content-Type value written to HTTP responses.spring.thymeleaf.servlet.produce-partial-output-while-processing=true # Whether Thymeleaf should start writing partial output as soon as possible or buffer until template processing is finished.spring.thymeleaf.suffix=.html # Suffix that gets appended to view names when building a URL.spring.thymeleaf.template-resolver-order= # Order of the template resolver in the chain.spring.thymeleaf.view-names= # Comma-separated list of view names (patterns allowed) that can be resolved.# SPRING WEBFLUX (WebFluxProperties)spring.webflux.date-format= # Date format to use. For instance, `dd/MM/yyyy`.spring.webflux.hiddenmethod.filter.enabled=true # Whether to enable Spring&apos;s HiddenHttpMethodFilter.spring.webflux.static-path-pattern=/** # Path pattern used for static resources.# SPRING WEB SERVICES (WebServicesProperties)spring.webservices.path=/services # Path that serves as the base URI for the services.spring.webservices.servlet.init= # Servlet init parameters to pass to Spring Web Services.spring.webservices.servlet.load-on-startup=-1 # Load on startup priority of the Spring Web Services servlet.spring.webservices.wsdl-locations= # Comma-separated list of locations of WSDLs and accompanying XSDs to be exposed as beans.# ----------------------------------------# SECURITY PROPERTIES# ----------------------------------------# SECURITY (SecurityProperties)spring.security.filter.order=-100 # Security filter chain order.spring.security.filter.dispatcher-types=async,error,request # Security filter chain dispatcher types.spring.security.user.name=user # Default user name.spring.security.user.password= # Password for the default user name.spring.security.user.roles= # Granted roles for the default user name.# SECURITY OAUTH2 CLIENT (OAuth2ClientProperties)spring.security.oauth2.client.provider.*= # OAuth provider details.spring.security.oauth2.client.registration.*= # OAuth client registrations.# SECURITY OAUTH2 RESOURCE SERVER (OAuth2ResourceServerProperties)spring.security.oauth2.resourceserver.jwt.jwk-set-uri= # JSON Web Key URI to use to verify the JWT token.spring.security.oauth2.resourceserver.jwt.issuer-uri= # URI that an OpenID Connect Provider asserts as its Issuer Identifier.# ----------------------------------------# DATA PROPERTIES# ----------------------------------------# FLYWAY (FlywayProperties)spring.flyway.baseline-description=&lt;&lt; Flyway Baseline &gt;&gt; # Description to tag an existing schema with when applying a baseline.spring.flyway.baseline-on-migrate=false # Whether to automatically call baseline when migrating a non-empty schema.spring.flyway.baseline-version=1 # Version to tag an existing schema with when executing baseline.spring.flyway.check-location=true # Whether to check that migration scripts location exists.spring.flyway.clean-disabled=false # Whether to disable cleaning of the database.spring.flyway.clean-on-validation-error=false # Whether to automatically call clean when a validation error occurs.spring.flyway.connect-retries=0 # Maximum number of retries when attempting to connect to the database.spring.flyway.enabled=true # Whether to enable flyway.spring.flyway.encoding=UTF-8 # Encoding of SQL migrations.spring.flyway.group=false # Whether to group all pending migrations together in the same transaction when applying them.spring.flyway.ignore-future-migrations=true # Whether to ignore future migrations when reading the schema history table.spring.flyway.ignore-ignored-migrations=false # Whether to ignore ignored migrations when reading the schema history table.spring.flyway.ignore-missing-migrations=false # Whether to ignore missing migrations when reading the schema history table.spring.flyway.ignore-pending-migrations=false # Whether to ignore pending migrations when reading the schema history table.spring.flyway.init-sqls= # SQL statements to execute to initialize a connection immediately after obtaining it.spring.flyway.installed-by= # Username recorded in the schema history table as having applied the migration.spring.flyway.locations=classpath:db/migration # Locations of migrations scripts. Can contain the special &quot;&#123;vendor&#125;&quot; placeholder to use vendor-specific locations.spring.flyway.mixed=false # Whether to allow mixing transactional and non-transactional statements within the same migration.spring.flyway.out-of-order=false # Whether to allow migrations to be run out of order.spring.flyway.password= # Login password of the database to migrate.spring.flyway.placeholder-prefix=$&#123; # Prefix of placeholders in migration scripts.spring.flyway.placeholder-replacement=true # Perform placeholder replacement in migration scripts.spring.flyway.placeholder-suffix=&#125; # Suffix of placeholders in migration scripts.spring.flyway.placeholders= # Placeholders and their replacements to apply to sql migration scripts.spring.flyway.repeatable-sql-migration-prefix=R # File name prefix for repeatable SQL migrations.spring.flyway.schemas= # Scheme names managed by Flyway (case-sensitive).spring.flyway.skip-default-callbacks=false # Whether to skip default callbacks. If true, only custom callbacks are used.spring.flyway.skip-default-resolvers=false # Whether to skip default resolvers. If true, only custom resolvers are used.spring.flyway.sql-migration-prefix=V # File name prefix for SQL migrations.spring.flyway.sql-migration-separator=__ # File name separator for SQL migrations.spring.flyway.sql-migration-suffixes=.sql # File name suffix for SQL migrations.spring.flyway.table=flyway_schema_history # Name of the schema schema history table that will be used by Flyway.spring.flyway.target= # Target version up to which migrations should be considered.spring.flyway.url= # JDBC url of the database to migrate. If not set, the primary configured data source is used.spring.flyway.user= # Login user of the database to migrate.spring.flyway.validate-on-migrate=true # Whether to automatically call validate when performing a migration.# LIQUIBASE (LiquibaseProperties)spring.liquibase.change-log=classpath:/db/changelog/db.changelog-master.yaml # Change log configuration path.spring.liquibase.contexts= # Comma-separated list of runtime contexts to use.spring.liquibase.database-change-log-lock-table=DATABASECHANGELOGLOCK # Name of table to use for tracking concurrent Liquibase usage.spring.liquibase.database-change-log-table=DATABASECHANGELOG # Name of table to use for tracking change history.spring.liquibase.default-schema= # Default database schema.spring.liquibase.drop-first=false # Whether to first drop the database schema.spring.liquibase.enabled=true # Whether to enable Liquibase support.spring.liquibase.labels= # Comma-separated list of runtime labels to use.spring.liquibase.liquibase-schema= # Schema to use for Liquibase objects.spring.liquibase.liquibase-tablespace= # Tablespace to use for Liquibase objects.spring.liquibase.parameters.*= # Change log parameters.spring.liquibase.password= # Login password of the database to migrate.spring.liquibase.rollback-file= # File to which rollback SQL is written when an update is performed.spring.liquibase.test-rollback-on-update=false # Whether rollback should be tested before update is performed.spring.liquibase.url= # JDBC URL of the database to migrate. If not set, the primary configured data source is used.spring.liquibase.user= # Login user of the database to migrate.# COUCHBASE (CouchbaseProperties)spring.couchbase.bootstrap-hosts= # Couchbase nodes (host or IP address) to bootstrap from.spring.couchbase.bucket.name=default # Name of the bucket to connect to.spring.couchbase.bucket.password= # Password of the bucket.spring.couchbase.env.endpoints.key-value=1 # Number of sockets per node against the key/value service.spring.couchbase.env.endpoints.queryservice.min-endpoints=1 # Minimum number of sockets per node.spring.couchbase.env.endpoints.queryservice.max-endpoints=1 # Maximum number of sockets per node.spring.couchbase.env.endpoints.viewservice.min-endpoints=1 # Minimum number of sockets per node.spring.couchbase.env.endpoints.viewservice.max-endpoints=1 # Maximum number of sockets per node.spring.couchbase.env.ssl.enabled= # Whether to enable SSL support. Enabled automatically if a &quot;keyStore&quot; is provided unless specified otherwise.spring.couchbase.env.ssl.key-store= # Path to the JVM key store that holds the certificates.spring.couchbase.env.ssl.key-store-password= # Password used to access the key store.spring.couchbase.env.timeouts.connect=5000ms # Bucket connections timeouts.spring.couchbase.env.timeouts.key-value=2500ms # Blocking operations performed on a specific key timeout.spring.couchbase.env.timeouts.query=7500ms # N1QL query operations timeout.spring.couchbase.env.timeouts.socket-connect=1000ms # Socket connect connections timeout.spring.couchbase.env.timeouts.view=7500ms # Regular and geospatial view operations timeout.# DAO (PersistenceExceptionTranslationAutoConfiguration)spring.dao.exceptiontranslation.enabled=true # Whether to enable the PersistenceExceptionTranslationPostProcessor.# CASSANDRA (CassandraProperties)spring.data.cassandra.cluster-name= # Name of the Cassandra cluster.spring.data.cassandra.compression=none # Compression supported by the Cassandra binary protocol.spring.data.cassandra.connect-timeout= # Socket option: connection time out.spring.data.cassandra.consistency-level= # Queries consistency level.spring.data.cassandra.contact-points=localhost # Cluster node addresses.spring.data.cassandra.fetch-size= # Queries default fetch size.spring.data.cassandra.jmx-enabled=false # Whether to enable JMX reporting.spring.data.cassandra.keyspace-name= # Keyspace name to use.spring.data.cassandra.port= # Port of the Cassandra server.spring.data.cassandra.password= # Login password of the server.spring.data.cassandra.pool.heartbeat-interval=30s # Heartbeat interval after which a message is sent on an idle connection to make sure it&apos;s still alive. If a duration suffix is not specified, seconds will be used.spring.data.cassandra.pool.idle-timeout=120s # Idle timeout before an idle connection is removed. If a duration suffix is not specified, seconds will be used.spring.data.cassandra.pool.max-queue-size=256 # Maximum number of requests that get queued if no connection is available.spring.data.cassandra.pool.pool-timeout=5000ms # Pool timeout when trying to acquire a connection from a host&apos;s pool.spring.data.cassandra.read-timeout= # Socket option: read time out.spring.data.cassandra.repositories.type=auto # Type of Cassandra repositories to enable.spring.data.cassandra.serial-consistency-level= # Queries serial consistency level.spring.data.cassandra.schema-action=none # Schema action to take at startup.spring.data.cassandra.ssl=false # Enable SSL support.spring.data.cassandra.username= # Login user of the server.# DATA COUCHBASE (CouchbaseDataProperties)spring.data.couchbase.auto-index=false # Automatically create views and indexes.spring.data.couchbase.consistency=read-your-own-writes # Consistency to apply by default on generated queries.spring.data.couchbase.repositories.type=auto # Type of Couchbase repositories to enable.# ELASTICSEARCH (ElasticsearchProperties)spring.data.elasticsearch.cluster-name=elasticsearch # Elasticsearch cluster name.spring.data.elasticsearch.cluster-nodes= # Comma-separated list of cluster node addresses.spring.data.elasticsearch.properties.*= # Additional properties used to configure the client.spring.data.elasticsearch.repositories.enabled=true # Whether to enable Elasticsearch repositories.# DATA JDBCspring.data.jdbc.repositories.enabled=true # Whether to enable JDBC repositories.# DATA LDAPspring.data.ldap.repositories.enabled=true # Whether to enable LDAP repositories.# MONGODB (MongoProperties)spring.data.mongodb.authentication-database= # Authentication database name.spring.data.mongodb.database= # Database name.spring.data.mongodb.field-naming-strategy= # Fully qualified name of the FieldNamingStrategy to use.spring.data.mongodb.grid-fs-database= # GridFS database name.spring.data.mongodb.host= # Mongo server host. Cannot be set with URI.spring.data.mongodb.password= # Login password of the mongo server. Cannot be set with URI.spring.data.mongodb.port= # Mongo server port. Cannot be set with URI.spring.data.mongodb.repositories.type=auto # Type of Mongo repositories to enable.spring.data.mongodb.uri=mongodb://localhost/test # Mongo database URI. Cannot be set with host, port and credentials.spring.data.mongodb.username= # Login user of the mongo server. Cannot be set with URI.# DATA REDISspring.data.redis.repositories.enabled=true # Whether to enable Redis repositories.# NEO4J (Neo4jProperties)spring.data.neo4j.auto-index=none # Auto index mode.spring.data.neo4j.embedded.enabled=true # Whether to enable embedded mode if the embedded driver is available.spring.data.neo4j.open-in-view=true # Register OpenSessionInViewInterceptor. Binds a Neo4j Session to the thread for the entire processing of the request.spring.data.neo4j.password= # Login password of the server.spring.data.neo4j.repositories.enabled=true # Whether to enable Neo4j repositories.spring.data.neo4j.uri= # URI used by the driver. Auto-detected by default.spring.data.neo4j.username= # Login user of the server.# DATA REST (RepositoryRestProperties)spring.data.rest.base-path= # Base path to be used by Spring Data REST to expose repository resources.spring.data.rest.default-media-type= # Content type to use as a default when none is specified.spring.data.rest.default-page-size= # Default size of pages.spring.data.rest.detection-strategy=default # Strategy to use to determine which repositories get exposed.spring.data.rest.enable-enum-translation= # Whether to enable enum value translation through the Spring Data REST default resource bundle.spring.data.rest.limit-param-name= # Name of the URL query string parameter that indicates how many results to return at once.spring.data.rest.max-page-size= # Maximum size of pages.spring.data.rest.page-param-name= # Name of the URL query string parameter that indicates what page to return.spring.data.rest.return-body-on-create= # Whether to return a response body after creating an entity.spring.data.rest.return-body-on-update= # Whether to return a response body after updating an entity.spring.data.rest.sort-param-name= # Name of the URL query string parameter that indicates what direction to sort results.# SOLR (SolrProperties)spring.data.solr.host=http://127.0.0.1:8983/solr # Solr host. Ignored if &quot;zk-host&quot; is set.spring.data.solr.repositories.enabled=true # Whether to enable Solr repositories.spring.data.solr.zk-host= # ZooKeeper host address in the form HOST:PORT.# DATA WEB (SpringDataWebProperties)spring.data.web.pageable.default-page-size=20 # Default page size.spring.data.web.pageable.max-page-size=2000 # Maximum page size to be accepted.spring.data.web.pageable.one-indexed-parameters=false # Whether to expose and assume 1-based page number indexes.spring.data.web.pageable.page-parameter=page # Page index parameter name.spring.data.web.pageable.prefix= # General prefix to be prepended to the page number and page size parameters.spring.data.web.pageable.qualifier-delimiter=_ # Delimiter to be used between the qualifier and the actual page number and size properties.spring.data.web.pageable.size-parameter=size # Page size parameter name.spring.data.web.sort.sort-parameter=sort # Sort parameter name.# DATASOURCE (DataSourceAutoConfiguration &amp; DataSourceProperties)spring.datasource.continue-on-error=false # Whether to stop if an error occurs while initializing the database.spring.datasource.data= # Data (DML) script resource references.spring.datasource.data-username= # Username of the database to execute DML scripts (if different).spring.datasource.data-password= # Password of the database to execute DML scripts (if different).spring.datasource.dbcp2.*= # Commons DBCP2 specific settingsspring.datasource.driver-class-name= # Fully qualified name of the JDBC driver. Auto-detected based on the URL by default.spring.datasource.generate-unique-name=false # Whether to generate a random datasource name.spring.datasource.hikari.*= # Hikari specific settingsspring.datasource.initialization-mode=embedded # Initialize the datasource with available DDL and DML scripts.spring.datasource.jmx-enabled=false # Whether to enable JMX support (if provided by the underlying pool).spring.datasource.jndi-name= # JNDI location of the datasource. Class, url, username &amp; password are ignored when set.spring.datasource.name= # Name of the datasource. Default to &quot;testdb&quot; when using an embedded database.spring.datasource.password= # Login password of the database.spring.datasource.platform=all # Platform to use in the DDL or DML scripts (such as schema-$&#123;platform&#125;.sql or data-$&#123;platform&#125;.sql).spring.datasource.schema= # Schema (DDL) script resource references.spring.datasource.schema-username= # Username of the database to execute DDL scripts (if different).spring.datasource.schema-password= # Password of the database to execute DDL scripts (if different).spring.datasource.separator=; # Statement separator in SQL initialization scripts.spring.datasource.sql-script-encoding= # SQL scripts encoding.spring.datasource.tomcat.*= # Tomcat datasource specific settingsspring.datasource.type= # Fully qualified name of the connection pool implementation to use. By default, it is auto-detected from the classpath.spring.datasource.url= # JDBC URL of the database.spring.datasource.username= # Login username of the database.spring.datasource.xa.data-source-class-name= # XA datasource fully qualified name.spring.datasource.xa.properties= # Properties to pass to the XA data source.# JEST (Elasticsearch HTTP client) (JestProperties)spring.elasticsearch.jest.connection-timeout=3s # Connection timeout.spring.elasticsearch.jest.multi-threaded=true # Whether to enable connection requests from multiple execution threads.spring.elasticsearch.jest.password= # Login password.spring.elasticsearch.jest.proxy.host= # Proxy host the HTTP client should use.spring.elasticsearch.jest.proxy.port= # Proxy port the HTTP client should use.spring.elasticsearch.jest.read-timeout=3s # Read timeout.spring.elasticsearch.jest.uris=http://localhost:9200 # Comma-separated list of the Elasticsearch instances to use.spring.elasticsearch.jest.username= # Login username.# Elasticsearch REST clients (RestClientProperties)spring.elasticsearch.rest.password= # Credentials password. spring.elasticsearch.rest.uris=http://localhost:9200 # Comma-separated list of the Elasticsearch instances to use. spring.elasticsearch.rest.username= # Credentials username.# H2 Web Console (H2ConsoleProperties)spring.h2.console.enabled=false # Whether to enable the console.spring.h2.console.path=/h2-console # Path at which the console is available.spring.h2.console.settings.trace=false # Whether to enable trace output.spring.h2.console.settings.web-allow-others=false # Whether to enable remote access.# InfluxDB (InfluxDbProperties)spring.influx.password= # Login password.spring.influx.url= # URL of the InfluxDB instance to which to connect.spring.influx.user= # Login user.# JOOQ (JooqProperties)spring.jooq.sql-dialect= # SQL dialect to use. Auto-detected by default.# JDBC (JdbcProperties)spring.jdbc.template.fetch-size=-1 # Number of rows that should be fetched from the database when more rows are needed.spring.jdbc.template.max-rows=-1 # Maximum number of rows.spring.jdbc.template.query-timeout= # Query timeout. Default is to use the JDBC driver&apos;s default configuration. If a duration suffix is not specified, seconds will be used.# JPA (JpaBaseConfiguration, HibernateJpaAutoConfiguration)spring.data.jpa.repositories.bootstrap-mode=default # Bootstrap mode for JPA repositories.spring.data.jpa.repositories.enabled=true # Whether to enable JPA repositories.spring.jpa.database= # Target database to operate on, auto-detected by default. Can be alternatively set using the &quot;databasePlatform&quot; property.spring.jpa.database-platform= # Name of the target database to operate on, auto-detected by default. Can be alternatively set using the &quot;Database&quot; enum.spring.jpa.generate-ddl=false # Whether to initialize the schema on startup.spring.jpa.hibernate.ddl-auto= # DDL mode. This is actually a shortcut for the &quot;hibernate.hbm2ddl.auto&quot; property. Defaults to &quot;create-drop&quot; when using an embedded database and no schema manager was detected. Otherwise, defaults to &quot;none&quot;.spring.jpa.hibernate.naming.implicit-strategy= # Fully qualified name of the implicit naming strategy.spring.jpa.hibernate.naming.physical-strategy= # Fully qualified name of the physical naming strategy.spring.jpa.hibernate.use-new-id-generator-mappings= # Whether to use Hibernate&apos;s newer IdentifierGenerator for AUTO, TABLE and SEQUENCE.spring.jpa.mapping-resources= # Mapping resources (equivalent to &quot;mapping-file&quot; entries in persistence.xml).spring.jpa.open-in-view=true # Register OpenEntityManagerInViewInterceptor. Binds a JPA EntityManager to the thread for the entire processing of the request.spring.jpa.properties.*= # Additional native properties to set on the JPA provider.spring.jpa.show-sql=false # Whether to enable logging of SQL statements.# JTA (JtaAutoConfiguration)spring.jta.enabled=true # Whether to enable JTA support.spring.jta.log-dir= # Transaction logs directory.spring.jta.transaction-manager-id= # Transaction manager unique identifier.# ATOMIKOS (AtomikosProperties)spring.jta.atomikos.connectionfactory.borrow-connection-timeout=30 # Timeout, in seconds, for borrowing connections from the pool.spring.jta.atomikos.connectionfactory.ignore-session-transacted-flag=true # Whether to ignore the transacted flag when creating session.spring.jta.atomikos.connectionfactory.local-transaction-mode=false # Whether local transactions are desired.spring.jta.atomikos.connectionfactory.maintenance-interval=60 # Time, in seconds, between runs of the pool&apos;s maintenance thread.spring.jta.atomikos.connectionfactory.max-idle-time=60 # Time, in seconds, after which connections are cleaned up from the pool.spring.jta.atomikos.connectionfactory.max-lifetime=0 # Time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit.spring.jta.atomikos.connectionfactory.max-pool-size=1 # Maximum size of the pool.spring.jta.atomikos.connectionfactory.min-pool-size=1 # Minimum size of the pool.spring.jta.atomikos.connectionfactory.reap-timeout=0 # Reap timeout, in seconds, for borrowed connections. 0 denotes no limit.spring.jta.atomikos.connectionfactory.unique-resource-name=jmsConnectionFactory # Unique name used to identify the resource during recovery.spring.jta.atomikos.connectionfactory.xa-connection-factory-class-name= # Vendor-specific implementation of XAConnectionFactory.spring.jta.atomikos.connectionfactory.xa-properties= # Vendor-specific XA properties.spring.jta.atomikos.datasource.borrow-connection-timeout=30 # Timeout, in seconds, for borrowing connections from the pool.spring.jta.atomikos.datasource.concurrent-connection-validation=true # Whether to use concurrent connection validation.spring.jta.atomikos.datasource.default-isolation-level= # Default isolation level of connections provided by the pool.spring.jta.atomikos.datasource.login-timeout=0 # Timeout, in seconds, for establishing a database connection.spring.jta.atomikos.datasource.maintenance-interval=60 # Time, in seconds, between runs of the pool&apos;s maintenance thread.spring.jta.atomikos.datasource.max-idle-time=60 # Time, in seconds, after which connections are cleaned up from the pool.spring.jta.atomikos.datasource.max-lifetime=0 # Time, in seconds, that a connection can be pooled for before being destroyed. 0 denotes no limit.spring.jta.atomikos.datasource.max-pool-size=1 # Maximum size of the pool.spring.jta.atomikos.datasource.min-pool-size=1 # Minimum size of the pool.spring.jta.atomikos.datasource.reap-timeout=0 # Reap timeout, in seconds, for borrowed connections. 0 denotes no limit.spring.jta.atomikos.datasource.test-query= # SQL query or statement used to validate a connection before returning it.spring.jta.atomikos.datasource.unique-resource-name=dataSource # Unique name used to identify the resource during recovery.spring.jta.atomikos.datasource.xa-data-source-class-name= # Vendor-specific implementation of XAConnectionFactory.spring.jta.atomikos.datasource.xa-properties= # Vendor-specific XA properties.spring.jta.atomikos.properties.allow-sub-transactions=true # Specify whether sub-transactions are allowed.spring.jta.atomikos.properties.checkpoint-interval=500 # Interval between checkpoints, expressed as the number of log writes between two checkpoints.spring.jta.atomikos.properties.default-jta-timeout=10000ms # Default timeout for JTA transactions.spring.jta.atomikos.properties.default-max-wait-time-on-shutdown=9223372036854775807 # How long should normal shutdown (no-force) wait for transactions to complete.spring.jta.atomikos.properties.enable-logging=true # Whether to enable disk logging.spring.jta.atomikos.properties.force-shutdown-on-vm-exit=false # Whether a VM shutdown should trigger forced shutdown of the transaction core.spring.jta.atomikos.properties.log-base-dir= # Directory in which the log files should be stored.spring.jta.atomikos.properties.log-base-name=tmlog # Transactions log file base name.spring.jta.atomikos.properties.max-actives=50 # Maximum number of active transactions.spring.jta.atomikos.properties.max-timeout=300000ms # Maximum timeout that can be allowed for transactions.spring.jta.atomikos.properties.recovery.delay=10000ms # Delay between two recovery scans.spring.jta.atomikos.properties.recovery.forget-orphaned-log-entries-delay=86400000ms # Delay after which recovery can cleanup pending (&apos;orphaned&apos;) log entries.spring.jta.atomikos.properties.recovery.max-retries=5 # Number of retry attempts to commit the transaction before throwing an exception.spring.jta.atomikos.properties.recovery.retry-interval=10000ms # Delay between retry attempts.spring.jta.atomikos.properties.serial-jta-transactions=true # Whether sub-transactions should be joined when possible.spring.jta.atomikos.properties.service= # Transaction manager implementation that should be started.spring.jta.atomikos.properties.threaded-two-phase-commit=false # Whether to use different (and concurrent) threads for two-phase commit on the participating resources.spring.jta.atomikos.properties.transaction-manager-unique-name= # The transaction manager&apos;s unique name.# BITRONIXspring.jta.bitronix.connectionfactory.acquire-increment=1 # Number of connections to create when growing the pool.spring.jta.bitronix.connectionfactory.acquisition-interval=1 # Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired.spring.jta.bitronix.connectionfactory.acquisition-timeout=30 # Timeout, in seconds, for acquiring connections from the pool.spring.jta.bitronix.connectionfactory.allow-local-transactions=false # Whether the transaction manager should allow mixing XA and non-XA transactions.spring.jta.bitronix.connectionfactory.apply-transaction-timeout=false # Whether the transaction timeout should be set on the XAResource when it is enlisted.spring.jta.bitronix.connectionfactory.automatic-enlisting-enabled=true # Whether resources should be enlisted and delisted automatically.spring.jta.bitronix.connectionfactory.cache-producers-consumers=true # Whether producers and consumers should be cached.spring.jta.bitronix.connectionfactory.class-name= # Underlying implementation class name of the XA resource.spring.jta.bitronix.connectionfactory.defer-connection-release=true # Whether the provider can run many transactions on the same connection and supports transaction interleaving.spring.jta.bitronix.connectionfactory.disabled=false # Whether this resource is disabled, meaning it&apos;s temporarily forbidden to acquire a connection from its pool.spring.jta.bitronix.connectionfactory.driver-properties= # Properties that should be set on the underlying implementation.spring.jta.bitronix.connectionfactory.ignore-recovery-failures=false # Whether recovery failures should be ignored.spring.jta.bitronix.connectionfactory.max-idle-time=60 # Time, in seconds, after which connections are cleaned up from the pool.spring.jta.bitronix.connectionfactory.max-pool-size=0 # Maximum size of the pool. 0 denotes no limit.spring.jta.bitronix.connectionfactory.min-pool-size=0 # Minimum size of the pool.spring.jta.bitronix.connectionfactory.password= # Password to use to connect to the JMS provider.spring.jta.bitronix.connectionfactory.share-transaction-connections=false # Whether connections in the ACCESSIBLE state can be shared within the context of a transaction.spring.jta.bitronix.connectionfactory.test-connections=false # Whether connections should be tested when acquired from the pool.spring.jta.bitronix.connectionfactory.two-pc-ordering-position=1 # Position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, always last is Integer.MAX_VALUE).spring.jta.bitronix.connectionfactory.unique-name=jmsConnectionFactory # Unique name used to identify the resource during recovery.spring.jta.bitronix.connectionfactory.use-tm-join=true # Whether TMJOIN should be used when starting XAResources.spring.jta.bitronix.connectionfactory.user= # User to use to connect to the JMS provider.spring.jta.bitronix.datasource.acquire-increment=1 # Number of connections to create when growing the pool.spring.jta.bitronix.datasource.acquisition-interval=1 # Time, in seconds, to wait before trying to acquire a connection again after an invalid connection was acquired.spring.jta.bitronix.datasource.acquisition-timeout=30 # Timeout, in seconds, for acquiring connections from the pool.spring.jta.bitronix.datasource.allow-local-transactions=false # Whether the transaction manager should allow mixing XA and non-XA transactions.spring.jta.bitronix.datasource.apply-transaction-timeout=false # Whether the transaction timeout should be set on the XAResource when it is enlisted.spring.jta.bitronix.datasource.automatic-enlisting-enabled=true # Whether resources should be enlisted and delisted automatically.spring.jta.bitronix.datasource.class-name= # Underlying implementation class name of the XA resource.spring.jta.bitronix.datasource.cursor-holdability= # Default cursor holdability for connections.spring.jta.bitronix.datasource.defer-connection-release=true # Whether the database can run many transactions on the same connection and supports transaction interleaving.spring.jta.bitronix.datasource.disabled=false # Whether this resource is disabled, meaning it&apos;s temporarily forbidden to acquire a connection from its pool.spring.jta.bitronix.datasource.driver-properties= # Properties that should be set on the underlying implementation.spring.jta.bitronix.datasource.enable-jdbc4-connection-test=false # Whether Connection.isValid() is called when acquiring a connection from the pool.spring.jta.bitronix.datasource.ignore-recovery-failures=false # Whether recovery failures should be ignored.spring.jta.bitronix.datasource.isolation-level= # Default isolation level for connections.spring.jta.bitronix.datasource.local-auto-commit= # Default auto-commit mode for local transactions.spring.jta.bitronix.datasource.login-timeout= # Timeout, in seconds, for establishing a database connection.spring.jta.bitronix.datasource.max-idle-time=60 # Time, in seconds, after which connections are cleaned up from the pool.spring.jta.bitronix.datasource.max-pool-size=0 # Maximum size of the pool. 0 denotes no limit.spring.jta.bitronix.datasource.min-pool-size=0 # Minimum size of the pool.spring.jta.bitronix.datasource.prepared-statement-cache-size=0 # Target size of the prepared statement cache. 0 disables the cache.spring.jta.bitronix.datasource.share-transaction-connections=false # Whether connections in the ACCESSIBLE state can be shared within the context of a transaction.spring.jta.bitronix.datasource.test-query= # SQL query or statement used to validate a connection before returning it.spring.jta.bitronix.datasource.two-pc-ordering-position=1 # Position that this resource should take during two-phase commit (always first is Integer.MIN_VALUE, and always last is Integer.MAX_VALUE).spring.jta.bitronix.datasource.unique-name=dataSource # Unique name used to identify the resource during recovery.spring.jta.bitronix.datasource.use-tm-join=true # Whether TMJOIN should be used when starting XAResources.spring.jta.bitronix.properties.allow-multiple-lrc=false # Whether to allow multiple LRC resources to be enlisted into the same transaction.spring.jta.bitronix.properties.asynchronous2-pc=false # Whether to enable asynchronously execution of two phase commit.spring.jta.bitronix.properties.background-recovery-interval-seconds=60 # Interval in seconds at which to run the recovery process in the background.spring.jta.bitronix.properties.current-node-only-recovery=true # Whether to recover only the current node.spring.jta.bitronix.properties.debug-zero-resource-transaction=false # Whether to log the creation and commit call stacks of transactions executed without a single enlisted resource.spring.jta.bitronix.properties.default-transaction-timeout=60 # Default transaction timeout, in seconds.spring.jta.bitronix.properties.disable-jmx=false # Whether to enable JMX support.spring.jta.bitronix.properties.exception-analyzer= # Set the fully qualified name of the exception analyzer implementation to use.spring.jta.bitronix.properties.filter-log-status=false # Whether to enable filtering of logs so that only mandatory logs are written.spring.jta.bitronix.properties.force-batching-enabled=true # Whether disk forces are batched.spring.jta.bitronix.properties.forced-write-enabled=true # Whether logs are forced to disk.spring.jta.bitronix.properties.graceful-shutdown-interval=60 # Maximum amount of seconds the TM waits for transactions to get done before aborting them at shutdown time.spring.jta.bitronix.properties.jndi-transaction-synchronization-registry-name= # JNDI name of the TransactionSynchronizationRegistry.spring.jta.bitronix.properties.jndi-user-transaction-name= # JNDI name of the UserTransaction.spring.jta.bitronix.properties.journal=disk # Name of the journal. Can be &apos;disk&apos;, &apos;null&apos;, or a class name.spring.jta.bitronix.properties.log-part1-filename=btm1.tlog # Name of the first fragment of the journal.spring.jta.bitronix.properties.log-part2-filename=btm2.tlog # Name of the second fragment of the journal.spring.jta.bitronix.properties.max-log-size-in-mb=2 # Maximum size in megabytes of the journal fragments.spring.jta.bitronix.properties.resource-configuration-filename= # ResourceLoader configuration file name.spring.jta.bitronix.properties.server-id= # ASCII ID that must uniquely identify this TM instance. Defaults to the machine&apos;s IP address.spring.jta.bitronix.properties.skip-corrupted-logs=false # Skip corrupted transactions log entries.spring.jta.bitronix.properties.warn-about-zero-resource-transaction=true # Whether to log a warning for transactions executed without a single enlisted resource.# EMBEDDED MONGODB (EmbeddedMongoProperties)spring.mongodb.embedded.features=sync_delay # Comma-separated list of features to enable.spring.mongodb.embedded.storage.database-dir= # Directory used for data storage.spring.mongodb.embedded.storage.oplog-size= # Maximum size of the oplog.spring.mongodb.embedded.storage.repl-set-name= # Name of the replica set.spring.mongodb.embedded.version=3.5.5 # Version of Mongo to use.# REDIS (RedisProperties)spring.redis.cluster.max-redirects= # Maximum number of redirects to follow when executing commands across the cluster.spring.redis.cluster.nodes= # Comma-separated list of &quot;host:port&quot; pairs to bootstrap from.spring.redis.database=0 # Database index used by the connection factory.spring.redis.url= # Connection URL. Overrides host, port, and password. User is ignored. Example: redis://user:password@example.com:6379spring.redis.host=localhost # Redis server host.spring.redis.jedis.pool.max-active=8 # Maximum number of connections that can be allocated by the pool at a given time. Use a negative value for no limit.spring.redis.jedis.pool.max-idle=8 # Maximum number of &quot;idle&quot; connections in the pool. Use a negative value to indicate an unlimited number of idle connections.spring.redis.jedis.pool.max-wait=-1ms # Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely.spring.redis.jedis.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive.spring.redis.lettuce.pool.max-active=8 # Maximum number of connections that can be allocated by the pool at a given time. Use a negative value for no limit.spring.redis.lettuce.pool.max-idle=8 # Maximum number of &quot;idle&quot; connections in the pool. Use a negative value to indicate an unlimited number of idle connections.spring.redis.lettuce.pool.max-wait=-1ms # Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. Use a negative value to block indefinitely.spring.redis.lettuce.pool.min-idle=0 # Target for the minimum number of idle connections to maintain in the pool. This setting only has an effect if it is positive.spring.redis.lettuce.shutdown-timeout=100ms # Shutdown timeout.spring.redis.password= # Login password of the redis server.spring.redis.port=6379 # Redis server port.spring.redis.sentinel.master= # Name of the Redis server.spring.redis.sentinel.nodes= # Comma-separated list of &quot;host:port&quot; pairs.spring.redis.ssl=false # Whether to enable SSL support.spring.redis.timeout= # Connection timeout.# TRANSACTION (TransactionProperties)spring.transaction.default-timeout= # Default transaction timeout. If a duration suffix is not specified, seconds will be used.spring.transaction.rollback-on-commit-failure= # Whether to roll back on commit failures.# ----------------------------------------# INTEGRATION PROPERTIES# ----------------------------------------# ACTIVEMQ (ActiveMQProperties)spring.activemq.broker-url= # URL of the ActiveMQ broker. Auto-generated by default.spring.activemq.close-timeout=15s # Time to wait before considering a close complete.spring.activemq.in-memory=true # Whether the default broker URL should be in memory. Ignored if an explicit broker has been specified.spring.activemq.non-blocking-redelivery=false # Whether to stop message delivery before re-delivering messages from a rolled back transaction. This implies that message order is not preserved when this is enabled.spring.activemq.password= # Login password of the broker.spring.activemq.send-timeout=0ms # Time to wait on message sends for a response. Set it to 0 to wait forever.spring.activemq.user= # Login user of the broker.spring.activemq.packages.trust-all= # Whether to trust all packages.spring.activemq.packages.trusted= # Comma-separated list of specific packages to trust (when not trusting all packages).spring.activemq.pool.block-if-full=true # Whether to block when a connection is requested and the pool is full. Set it to false to throw a &quot;JMSException&quot; instead.spring.activemq.pool.block-if-full-timeout=-1ms # Blocking period before throwing an exception if the pool is still full.spring.activemq.pool.enabled=false # Whether a JmsPoolConnectionFactory should be created, instead of a regular ConnectionFactory.spring.activemq.pool.idle-timeout=30s # Connection idle timeout.spring.activemq.pool.max-connections=1 # Maximum number of pooled connections.spring.activemq.pool.max-sessions-per-connection=500 # Maximum number of pooled sessions per connection in the pool.spring.activemq.pool.time-between-expiration-check=-1ms # Time to sleep between runs of the idle connection eviction thread. When negative, no idle connection eviction thread runs.spring.activemq.pool.use-anonymous-producers=true # Whether to use only one anonymous &quot;MessageProducer&quot; instance. Set it to false to create one &quot;MessageProducer&quot; every time one is required.# ARTEMIS (ArtemisProperties)spring.artemis.embedded.cluster-password= # Cluster password. Randomly generated on startup by default.spring.artemis.embedded.data-directory= # Journal file directory. Not necessary if persistence is turned off.spring.artemis.embedded.enabled=true # Whether to enable embedded mode if the Artemis server APIs are available.spring.artemis.embedded.persistent=false # Whether to enable persistent store.spring.artemis.embedded.queues= # Comma-separated list of queues to create on startup.spring.artemis.embedded.server-id= # Server ID. By default, an auto-incremented counter is used.spring.artemis.embedded.topics= # Comma-separated list of topics to create on startup.spring.artemis.host=localhost # Artemis broker host.spring.artemis.mode= # Artemis deployment mode, auto-detected by default.spring.artemis.password= # Login password of the broker.spring.artemis.pool.block-if-full=true # Whether to block when a connection is requested and the pool is full. Set it to false to throw a &quot;JMSException&quot; instead.spring.artemis.pool.block-if-full-timeout=-1ms # Blocking period before throwing an exception if the pool is still full.spring.artemis.pool.enabled=false # Whether a JmsPoolConnectionFactory should be created, instead of a regular ConnectionFactory.spring.artemis.pool.idle-timeout=30s # Connection idle timeout.spring.artemis.pool.max-connections=1 # Maximum number of pooled connections.spring.artemis.pool.max-sessions-per-connection=500 # Maximum number of pooled sessions per connection in the pool.spring.artemis.pool.time-between-expiration-check=-1ms # Time to sleep between runs of the idle connection eviction thread. When negative, no idle connection eviction thread runs.spring.artemis.pool.use-anonymous-producers=true # Whether to use only one anonymous &quot;MessageProducer&quot; instance. Set it to false to create one &quot;MessageProducer&quot; every time one is required.spring.artemis.port=61616 # Artemis broker port.spring.artemis.user= # Login user of the broker.# SPRING BATCH (BatchProperties)spring.batch.initialize-schema=embedded # Database schema initialization mode.spring.batch.job.enabled=true # Execute all Spring Batch jobs in the context on startup.spring.batch.job.names= # Comma-separated list of job names to execute on startup (for instance, `job1,job2`). By default, all Jobs found in the context are executed.spring.batch.schema=classpath:org/springframework/batch/core/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema.spring.batch.table-prefix= # Table prefix for all the batch meta-data tables.# SPRING INTEGRATION (IntegrationProperties)spring.integration.jdbc.initialize-schema=embedded # Database schema initialization mode.spring.integration.jdbc.schema=classpath:org/springframework/integration/jdbc/schema-@@platform@@.sql # Path to the SQL file to use to initialize the database schema.# JMS (JmsProperties)spring.jms.cache.consumers=false # Whether to cache message consumers.spring.jms.cache.enabled=true # Whether to cache sessions.spring.jms.cache.producers=true # Whether to cache message producers.spring.jms.cache.session-cache-size=1 # Size of the session cache (per JMS Session type).spring.jms.jndi-name= # Connection factory JNDI name. When set, takes precedence to others connection factory auto-configurations.spring.jms.listener.acknowledge-mode= # Acknowledge mode of the container. By default, the listener is transacted with automatic acknowledgment.spring.jms.listener.auto-startup=true # Start the container automatically on startup.spring.jms.listener.concurrency= # Minimum number of concurrent consumers.spring.jms.listener.max-concurrency= # Maximum number of concurrent consumers.spring.jms.pub-sub-domain=false # Whether the default destination type is topic.spring.jms.template.default-destination= # Default destination to use on send and receive operations that do not have a destination parameter.spring.jms.template.delivery-delay= # Delivery delay to use for send calls.spring.jms.template.delivery-mode= # Delivery mode. Enables QoS (Quality of Service) when set.spring.jms.template.priority= # Priority of a message when sending. Enables QoS (Quality of Service) when set.spring.jms.template.qos-enabled= # Whether to enable explicit QoS (Quality of Service) when sending a message.spring.jms.template.receive-timeout= # Timeout to use for receive calls.spring.jms.template.time-to-live= # Time-to-live of a message when sending. Enables QoS (Quality of Service) when set.# APACHE KAFKA (KafkaProperties)spring.kafka.admin.client-id= # ID to pass to the server when making requests. Used for server-side logging.spring.kafka.admin.fail-fast=false # Whether to fail fast if the broker is not available on startup.spring.kafka.admin.properties.*= # Additional admin-specific properties used to configure the client.spring.kafka.admin.ssl.key-password= # Password of the private key in the key store file.spring.kafka.admin.ssl.key-store-location= # Location of the key store file.spring.kafka.admin.ssl.key-store-password= # Store password for the key store file.spring.kafka.admin.ssl.key-store-type= # Type of the key store.spring.kafka.admin.ssl.protocol= # SSL protocol to use.spring.kafka.admin.ssl.trust-store-location= # Location of the trust store file.spring.kafka.admin.ssl.trust-store-password= # Store password for the trust store file.spring.kafka.admin.ssl.trust-store-type= # Type of the trust store.spring.kafka.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Applies to all components unless overridden.spring.kafka.client-id= # ID to pass to the server when making requests. Used for server-side logging.spring.kafka.consumer.auto-commit-interval= # Frequency with which the consumer offsets are auto-committed to Kafka if &apos;enable.auto.commit&apos; is set to true.spring.kafka.consumer.auto-offset-reset= # What to do when there is no initial offset in Kafka or if the current offset no longer exists on the server.spring.kafka.consumer.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for consumers.spring.kafka.consumer.client-id= # ID to pass to the server when making requests. Used for server-side logging.spring.kafka.consumer.enable-auto-commit= # Whether the consumer&apos;s offset is periodically committed in the background.spring.kafka.consumer.fetch-max-wait= # Maximum amount of time the server blocks before answering the fetch request if there isn&apos;t sufficient data to immediately satisfy the requirement given by &quot;fetch-min-size&quot;.spring.kafka.consumer.fetch-min-size= # Minimum amount of data the server should return for a fetch request.spring.kafka.consumer.group-id= # Unique string that identifies the consumer group to which this consumer belongs.spring.kafka.consumer.heartbeat-interval= # Expected time between heartbeats to the consumer coordinator.spring.kafka.consumer.key-deserializer= # Deserializer class for keys.spring.kafka.consumer.max-poll-records= # Maximum number of records returned in a single call to poll().spring.kafka.consumer.properties.*= # Additional consumer-specific properties used to configure the client.spring.kafka.consumer.ssl.key-password= # Password of the private key in the key store file.spring.kafka.consumer.ssl.key-store-location= # Location of the key store file.spring.kafka.consumer.ssl.key-store-password= # Store password for the key store file.spring.kafka.consumer.ssl.key-store-type= # Type of the key store.spring.kafka.consumer.ssl.protocol= # SSL protocol to use.spring.kafka.consumer.ssl.trust-store-location= # Location of the trust store file.spring.kafka.consumer.ssl.trust-store-password= # Store password for the trust store file.spring.kafka.consumer.ssl.trust-store-type= # Type of the trust store.spring.kafka.consumer.value-deserializer= # Deserializer class for values.spring.kafka.jaas.control-flag=required # Control flag for login configuration.spring.kafka.jaas.enabled=false # Whether to enable JAAS configuration.spring.kafka.jaas.login-module=com.sun.security.auth.module.Krb5LoginModule # Login module.spring.kafka.jaas.options= # Additional JAAS options.spring.kafka.listener.ack-count= # Number of records between offset commits when ackMode is &quot;COUNT&quot; or &quot;COUNT_TIME&quot;.spring.kafka.listener.ack-mode= # Listener AckMode. See the spring-kafka documentation.spring.kafka.listener.ack-time= # Time between offset commits when ackMode is &quot;TIME&quot; or &quot;COUNT_TIME&quot;.spring.kafka.listener.client-id= # Prefix for the listener&apos;s consumer client.id property.spring.kafka.listener.concurrency= # Number of threads to run in the listener containers.spring.kafka.listener.idle-event-interval= # Time between publishing idle consumer events (no data received).spring.kafka.listener.log-container-config= # Whether to log the container configuration during initialization (INFO level).spring.kafka.listener.monitor-interval= # Time between checks for non-responsive consumers. If a duration suffix is not specified, seconds will be used.spring.kafka.listener.no-poll-threshold= # Multiplier applied to &quot;pollTimeout&quot; to determine if a consumer is non-responsive.spring.kafka.listener.poll-timeout= # Timeout to use when polling the consumer.spring.kafka.listener.type=single # Listener type.spring.kafka.producer.acks= # Number of acknowledgments the producer requires the leader to have received before considering a request complete.spring.kafka.producer.batch-size= # Default batch size.spring.kafka.producer.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for producers.spring.kafka.producer.buffer-memory= # Total memory size the producer can use to buffer records waiting to be sent to the server.spring.kafka.producer.client-id= # ID to pass to the server when making requests. Used for server-side logging.spring.kafka.producer.compression-type= # Compression type for all data generated by the producer.spring.kafka.producer.key-serializer= # Serializer class for keys.spring.kafka.producer.properties.*= # Additional producer-specific properties used to configure the client.spring.kafka.producer.retries= # When greater than zero, enables retrying of failed sends.spring.kafka.producer.ssl.key-password= # Password of the private key in the key store file.spring.kafka.producer.ssl.key-store-location= # Location of the key store file.spring.kafka.producer.ssl.key-store-password= # Store password for the key store file.spring.kafka.producer.ssl.key-store-type= # Type of the key store.spring.kafka.producer.ssl.protocol= # SSL protocol to use.spring.kafka.producer.ssl.trust-store-location= # Location of the trust store file.spring.kafka.producer.ssl.trust-store-password= # Store password for the trust store file.spring.kafka.producer.ssl.trust-store-type= # Type of the trust store.spring.kafka.producer.transaction-id-prefix= # When non empty, enables transaction support for producer.spring.kafka.producer.value-serializer= # Serializer class for values.spring.kafka.properties.*= # Additional properties, common to producers and consumers, used to configure the client.spring.kafka.ssl.key-password= # Password of the private key in the key store file.spring.kafka.ssl.key-store-location= # Location of the key store file.spring.kafka.ssl.key-store-password= # Store password for the key store file.spring.kafka.ssl.key-store-type= # Type of the key store.spring.kafka.ssl.protocol= # SSL protocol to use.spring.kafka.ssl.trust-store-location= # Location of the trust store file.spring.kafka.ssl.trust-store-password= # Store password for the trust store file.spring.kafka.ssl.trust-store-type= # Type of the trust store.spring.kafka.streams.application-id= # Kafka streams application.id property; default spring.application.name.spring.kafka.streams.auto-startup=true # Whether or not to auto-start the streams factory bean.spring.kafka.streams.bootstrap-servers= # Comma-delimited list of host:port pairs to use for establishing the initial connections to the Kafka cluster. Overrides the global property, for streams.spring.kafka.streams.cache-max-size-buffering= # Maximum memory size to be used for buffering across all threads.spring.kafka.streams.client-id= # ID to pass to the server when making requests. Used for server-side logging.spring.kafka.streams.properties.*= # Additional Kafka properties used to configure the streams.spring.kafka.streams.replication-factor= # The replication factor for change log topics and repartition topics created by the stream processing application.spring.kafka.streams.ssl.key-password= # Password of the private key in the key store file.spring.kafka.streams.ssl.key-store-location= # Location of the key store file.spring.kafka.streams.ssl.key-store-password= # Store password for the key store file.spring.kafka.streams.ssl.key-store-type= # Type of the key store.spring.kafka.streams.ssl.protocol= # SSL protocol to use.spring.kafka.streams.ssl.trust-store-location= # Location of the trust store file.spring.kafka.streams.ssl.trust-store-password= # Store password for the trust store file.spring.kafka.streams.ssl.trust-store-type= # Type of the trust store.spring.kafka.streams.state-dir= # Directory location for the state store.spring.kafka.template.default-topic= # Default topic to which messages are sent.# RABBIT (RabbitProperties)spring.rabbitmq.addresses= # Comma-separated list of addresses to which the client should connect.spring.rabbitmq.cache.channel.checkout-timeout= # Duration to wait to obtain a channel if the cache size has been reached.spring.rabbitmq.cache.channel.size= # Number of channels to retain in the cache.spring.rabbitmq.cache.connection.mode=channel # Connection factory cache mode.spring.rabbitmq.cache.connection.size= # Number of connections to cache.spring.rabbitmq.connection-timeout= # Connection timeout. Set it to zero to wait forever.spring.rabbitmq.dynamic=true # Whether to create an AmqpAdmin bean.spring.rabbitmq.host=localhost # RabbitMQ host.spring.rabbitmq.listener.direct.acknowledge-mode= # Acknowledge mode of container.spring.rabbitmq.listener.direct.auto-startup=true # Whether to start the container automatically on startup.spring.rabbitmq.listener.direct.consumers-per-queue= # Number of consumers per queue.spring.rabbitmq.listener.direct.default-requeue-rejected= # Whether rejected deliveries are re-queued by default.spring.rabbitmq.listener.direct.idle-event-interval= # How often idle container events should be published.spring.rabbitmq.listener.direct.missing-queues-fatal=false # Whether to fail if the queues declared by the container are not available on the broker.spring.rabbitmq.listener.direct.prefetch= # Maximum number of unacknowledged messages that can be outstanding at each consumer.spring.rabbitmq.listener.direct.retry.enabled=false # Whether publishing retries are enabled.spring.rabbitmq.listener.direct.retry.initial-interval=1000ms # Duration between the first and second attempt to deliver a message.spring.rabbitmq.listener.direct.retry.max-attempts=3 # Maximum number of attempts to deliver a message.spring.rabbitmq.listener.direct.retry.max-interval=10000ms # Maximum duration between attempts.spring.rabbitmq.listener.direct.retry.multiplier=1 # Multiplier to apply to the previous retry interval.spring.rabbitmq.listener.direct.retry.stateless=true # Whether retries are stateless or stateful.spring.rabbitmq.listener.simple.acknowledge-mode= # Acknowledge mode of container.spring.rabbitmq.listener.simple.auto-startup=true # Whether to start the container automatically on startup.spring.rabbitmq.listener.simple.concurrency= # Minimum number of listener invoker threads.spring.rabbitmq.listener.simple.default-requeue-rejected= # Whether rejected deliveries are re-queued by default.spring.rabbitmq.listener.simple.idle-event-interval= # How often idle container events should be published.spring.rabbitmq.listener.simple.max-concurrency= # Maximum number of listener invoker threads.spring.rabbitmq.listener.simple.missing-queues-fatal=true # Whether to fail if the queues declared by the container are not available on the broker and/or whether to stop the container if one or more queues are deleted at runtime.spring.rabbitmq.listener.simple.prefetch= # Maximum number of unacknowledged messages that can be outstanding at each consumer.spring.rabbitmq.listener.simple.retry.enabled=false # Whether publishing retries are enabled.spring.rabbitmq.listener.simple.retry.initial-interval=1000ms # Duration between the first and second attempt to deliver a message.spring.rabbitmq.listener.simple.retry.max-attempts=3 # Maximum number of attempts to deliver a message.spring.rabbitmq.listener.simple.retry.max-interval=10000ms # Maximum duration between attempts.spring.rabbitmq.listener.simple.retry.multiplier=1 # Multiplier to apply to the previous retry interval.spring.rabbitmq.listener.simple.retry.stateless=true # Whether retries are stateless or stateful.spring.rabbitmq.listener.simple.transaction-size= # Number of messages to be processed between acks when the acknowledge mode is AUTO. If larger than prefetch, prefetch will be increased to this value.spring.rabbitmq.listener.type=simple # Listener container type.spring.rabbitmq.password=guest # Login to authenticate against the broker.spring.rabbitmq.port=5672 # RabbitMQ port.spring.rabbitmq.publisher-confirms=false # Whether to enable publisher confirms.spring.rabbitmq.publisher-returns=false # Whether to enable publisher returns.spring.rabbitmq.requested-heartbeat= # Requested heartbeat timeout; zero for none. If a duration suffix is not specified, seconds will be used.spring.rabbitmq.ssl.algorithm= # SSL algorithm to use. By default, configured by the Rabbit client library.spring.rabbitmq.ssl.enabled=false # Whether to enable SSL support.spring.rabbitmq.ssl.key-store= # Path to the key store that holds the SSL certificate.spring.rabbitmq.ssl.key-store-password= # Password used to access the key store.spring.rabbitmq.ssl.key-store-type=PKCS12 # Key store type.spring.rabbitmq.ssl.trust-store= # Trust store that holds SSL certificates.spring.rabbitmq.ssl.trust-store-password= # Password used to access the trust store.spring.rabbitmq.ssl.trust-store-type=JKS # Trust store type.spring.rabbitmq.ssl.validate-server-certificate=true # Whether to enable server side certificate validation.spring.rabbitmq.ssl.verify-hostname=true # Whether to enable hostname verification.spring.rabbitmq.template.default-receive-queue= # Name of the default queue to receive messages from when none is specified explicitly.spring.rabbitmq.template.exchange= # Name of the default exchange to use for send operations.spring.rabbitmq.template.mandatory= # Whether to enable mandatory messages.spring.rabbitmq.template.receive-timeout= # Timeout for `receive()` operations.spring.rabbitmq.template.reply-timeout= # Timeout for `sendAndReceive()` operations.spring.rabbitmq.template.retry.enabled=false # Whether publishing retries are enabled.spring.rabbitmq.template.retry.initial-interval=1000ms # Duration between the first and second attempt to deliver a message.spring.rabbitmq.template.retry.max-attempts=3 # Maximum number of attempts to deliver a message.spring.rabbitmq.template.retry.max-interval=10000ms # Maximum duration between attempts.spring.rabbitmq.template.retry.multiplier=1 # Multiplier to apply to the previous retry interval.spring.rabbitmq.template.routing-key= # Value of a default routing key to use for send operations.spring.rabbitmq.username=guest # Login user to authenticate to the broker.spring.rabbitmq.virtual-host= # Virtual host to use when connecting to the broker.# ----------------------------------------# ACTUATOR PROPERTIES# ----------------------------------------# MANAGEMENT HTTP SERVER (ManagementServerProperties)management.server.add-application-context-header=false # Add the &quot;X-Application-Context&quot; HTTP header in each response.management.server.address= # Network address to which the management endpoints should bind. Requires a custom management.server.port.management.server.port= # Management endpoint HTTP port (uses the same port as the application by default). Configure a different port to use management-specific SSL.management.server.servlet.context-path= # Management endpoint context-path (for instance, `/management`). Requires a custom management.server.port.management.server.ssl.ciphers= # Supported SSL ciphers.management.server.ssl.client-auth= # Client authentication mode.management.server.ssl.enabled=true # Whether to enable SSL support.management.server.ssl.enabled-protocols= # Enabled SSL protocols.management.server.ssl.key-alias= # Alias that identifies the key in the key store.management.server.ssl.key-password= # Password used to access the key in the key store.management.server.ssl.key-store= # Path to the key store that holds the SSL certificate (typically a jks file).management.server.ssl.key-store-password= # Password used to access the key store.management.server.ssl.key-store-provider= # Provider for the key store.management.server.ssl.key-store-type= # Type of the key store.management.server.ssl.protocol=TLS # SSL protocol to use.management.server.ssl.trust-store= # Trust store that holds SSL certificates.management.server.ssl.trust-store-password= # Password used to access the trust store.management.server.ssl.trust-store-provider= # Provider for the trust store.management.server.ssl.trust-store-type= # Type of the trust store.# CLOUDFOUNDRYmanagement.cloudfoundry.enabled=true # Whether to enable extended Cloud Foundry actuator endpoints.management.cloudfoundry.skip-ssl-validation=false # Whether to skip SSL verification for Cloud Foundry actuator endpoint security calls.# ENDPOINTS GENERAL CONFIGURATIONmanagement.endpoints.enabled-by-default= # Whether to enable or disable all endpoints by default.# ENDPOINTS JMX CONFIGURATION (JmxEndpointProperties)management.endpoints.jmx.domain=org.springframework.boot # Endpoints JMX domain name. Fallback to &apos;spring.jmx.default-domain&apos; if set.management.endpoints.jmx.exposure.include=* # Endpoint IDs that should be included or &apos;*&apos; for all.management.endpoints.jmx.exposure.exclude= # Endpoint IDs that should be excluded or &apos;*&apos; for all.management.endpoints.jmx.static-names= # Additional static properties to append to all ObjectNames of MBeans representing Endpoints.# ENDPOINTS WEB CONFIGURATION (WebEndpointProperties)management.endpoints.web.exposure.include=health,info # Endpoint IDs that should be included or &apos;*&apos; for all.management.endpoints.web.exposure.exclude= # Endpoint IDs that should be excluded or &apos;*&apos; for all.management.endpoints.web.base-path=/actuator # Base path for Web endpoints. Relative to server.servlet.context-path or management.server.servlet.context-path if management.server.port is configured.management.endpoints.web.path-mapping= # Mapping between endpoint IDs and the path that should expose them.# ENDPOINTS CORS CONFIGURATION (CorsEndpointProperties)management.endpoints.web.cors.allow-credentials= # Whether credentials are supported. When not set, credentials are not supported.management.endpoints.web.cors.allowed-headers= # Comma-separated list of headers to allow in a request. &apos;*&apos; allows all headers.management.endpoints.web.cors.allowed-methods= # Comma-separated list of methods to allow. &apos;*&apos; allows all methods. When not set, defaults to GET.management.endpoints.web.cors.allowed-origins= # Comma-separated list of origins to allow. &apos;*&apos; allows all origins. When not set, CORS support is disabled.management.endpoints.web.cors.exposed-headers= # Comma-separated list of headers to include in a response.management.endpoints.web.cors.max-age=1800s # How long the response from a pre-flight request can be cached by clients. If a duration suffix is not specified, seconds will be used.# AUDIT EVENTS ENDPOINT (AuditEventsEndpoint)management.endpoint.auditevents.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.auditevents.enabled=true # Whether to enable the auditevents endpoint.# BEANS ENDPOINT (BeansEndpoint)management.endpoint.beans.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.beans.enabled=true # Whether to enable the beans endpoint.# CACHES ENDPOINT (CachesEndpoint)management.endpoint.caches.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.caches.enabled=true # Whether to enable the caches endpoint.# CONDITIONS REPORT ENDPOINT (ConditionsReportEndpoint)management.endpoint.conditions.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.conditions.enabled=true # Whether to enable the conditions endpoint.# CONFIGURATION PROPERTIES REPORT ENDPOINT (ConfigurationPropertiesReportEndpoint, ConfigurationPropertiesReportEndpointProperties)management.endpoint.configprops.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.configprops.enabled=true # Whether to enable the configprops endpoint.management.endpoint.configprops.keys-to-sanitize=password,secret,key,token,.*credentials.*,vcap_services,sun.java.command # Keys that should be sanitized. Keys can be simple strings that the property ends with or regular expressions.# ENVIRONMENT ENDPOINT (EnvironmentEndpoint, EnvironmentEndpointProperties)management.endpoint.env.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.env.enabled=true # Whether to enable the env endpoint.management.endpoint.env.keys-to-sanitize=password,secret,key,token,.*credentials.*,vcap_services,sun.java.command # Keys that should be sanitized. Keys can be simple strings that the property ends with or regular expressions.# FLYWAY ENDPOINT (FlywayEndpoint)management.endpoint.flyway.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.flyway.enabled=true # Whether to enable the flyway endpoint.# HEALTH ENDPOINT (HealthEndpoint, HealthEndpointProperties)management.endpoint.health.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.health.enabled=true # Whether to enable the health endpoint.management.endpoint.health.roles= # Roles used to determine whether or not a user is authorized to be shown details. When empty, all authenticated users are authorized.management.endpoint.health.show-details=never # When to show full health details.# HEAP DUMP ENDPOINT (HeapDumpWebEndpoint)management.endpoint.heapdump.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.heapdump.enabled=true # Whether to enable the heapdump endpoint.# HTTP TRACE ENDPOINT (HttpTraceEndpoint)management.endpoint.httptrace.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.httptrace.enabled=true # Whether to enable the httptrace endpoint.# INFO ENDPOINT (InfoEndpoint)info= # Arbitrary properties to add to the info endpoint.management.endpoint.info.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.info.enabled=true # Whether to enable the info endpoint.# INTEGRATION GRAPH ENDPOINT (IntegrationGraphEndpoint)management.endpoint.integrationgraph.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.integrationgraph.enabled=true # Whether to enable the integrationgraph endpoint.# JOLOKIA ENDPOINT (JolokiaProperties)management.endpoint.jolokia.config.*= # Jolokia settings. Refer to the documentation of Jolokia for more details.management.endpoint.jolokia.enabled=true # Whether to enable the jolokia endpoint.# LIQUIBASE ENDPOINT (LiquibaseEndpoint)management.endpoint.liquibase.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.liquibase.enabled=true # Whether to enable the liquibase endpoint.# LOG FILE ENDPOINT (LogFileWebEndpoint, LogFileWebEndpointProperties)management.endpoint.logfile.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.logfile.enabled=true # Whether to enable the logfile endpoint.management.endpoint.logfile.external-file= # External Logfile to be accessed. Can be used if the logfile is written by output redirect and not by the logging system itself.# LOGGERS ENDPOINT (LoggersEndpoint)management.endpoint.loggers.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.loggers.enabled=true # Whether to enable the loggers endpoint.# REQUEST MAPPING ENDPOINT (MappingsEndpoint)management.endpoint.mappings.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.mappings.enabled=true # Whether to enable the mappings endpoint.# METRICS ENDPOINT (MetricsEndpoint)management.endpoint.metrics.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.metrics.enabled=true # Whether to enable the metrics endpoint.# PROMETHEUS ENDPOINT (PrometheusScrapeEndpoint)management.endpoint.prometheus.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.prometheus.enabled=true # Whether to enable the prometheus endpoint.# SCHEDULED TASKS ENDPOINT (ScheduledTasksEndpoint)management.endpoint.scheduledtasks.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.scheduledtasks.enabled=true # Whether to enable the scheduledtasks endpoint.# SESSIONS ENDPOINT (SessionsEndpoint)management.endpoint.sessions.enabled=true # Whether to enable the sessions endpoint.# SHUTDOWN ENDPOINT (ShutdownEndpoint)management.endpoint.shutdown.enabled=false # Whether to enable the shutdown endpoint.# THREAD DUMP ENDPOINT (ThreadDumpEndpoint)management.endpoint.threaddump.cache.time-to-live=0ms # Maximum time that a response can be cached.management.endpoint.threaddump.enabled=true # Whether to enable the threaddump endpoint.# HEALTH INDICATORSmanagement.health.db.enabled=true # Whether to enable database health check.management.health.cassandra.enabled=true # Whether to enable Cassandra health check.management.health.couchbase.enabled=true # Whether to enable Couchbase health check.management.health.defaults.enabled=true # Whether to enable default health indicators.management.health.diskspace.enabled=true # Whether to enable disk space health check.management.health.diskspace.path= # Path used to compute the available disk space.management.health.diskspace.threshold=10MB # Minimum disk space that should be available.management.health.elasticsearch.enabled=true # Whether to enable Elasticsearch health check.management.health.elasticsearch.indices= # Comma-separated index names.management.health.elasticsearch.response-timeout=100ms # Time to wait for a response from the cluster.management.health.influxdb.enabled=true # Whether to enable InfluxDB health check.management.health.jms.enabled=true # Whether to enable JMS health check.management.health.ldap.enabled=true # Whether to enable LDAP health check.management.health.mail.enabled=true # Whether to enable Mail health check.management.health.mongo.enabled=true # Whether to enable MongoDB health check.management.health.neo4j.enabled=true # Whether to enable Neo4j health check.management.health.rabbit.enabled=true # Whether to enable RabbitMQ health check.management.health.redis.enabled=true # Whether to enable Redis health check.management.health.solr.enabled=true # Whether to enable Solr health check.management.health.status.http-mapping= # Mapping of health statuses to HTTP status codes. By default, registered health statuses map to sensible defaults (for example, UP maps to 200).management.health.status.order=DOWN,OUT_OF_SERVICE,UP,UNKNOWN # Comma-separated list of health statuses in order of severity.# HTTP TRACING (HttpTraceProperties)management.trace.http.enabled=true # Whether to enable HTTP request-response tracing.management.trace.http.include=request-headers,response-headers,cookies,errors # Items to be included in the trace.# INFO CONTRIBUTORS (InfoContributorProperties)management.info.build.enabled=true # Whether to enable build info.management.info.defaults.enabled=true # Whether to enable default info contributors.management.info.env.enabled=true # Whether to enable environment info.management.info.git.enabled=true # Whether to enable git info.management.info.git.mode=simple # Mode to use to expose git information.# METRICSmanagement.metrics.distribution.maximum-expected-value.*= # Maximum value that meter IDs starting-with the specified name are expected to observe.management.metrics.distribution.minimum-expected-value.*= # Minimum value that meter IDs starting-with the specified name are expected to observe.management.metrics.distribution.percentiles.*= # Specific computed non-aggregable percentiles to ship to the backend for meter IDs starting-with the specified name.management.metrics.distribution.percentiles-histogram.*= # Whether meter IDs starting with the specified name should publish percentile histograms.management.metrics.distribution.sla.*= # Specific SLA boundaries for meter IDs starting-with the specified name. The longest match wins.management.metrics.enable.*= # Whether meter IDs starting-with the specified name should be enabled. The longest match wins, the key `all` can also be used to configure all meters.management.metrics.export.appoptics.api-token= # AppOptics API token.management.metrics.export.appoptics.batch-size=500 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.appoptics.connect-timeout=5s # Connection timeout for requests to this backend.management.metrics.export.appoptics.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.appoptics.host-tag=instance # Tag that will be mapped to &quot;@host&quot; when shipping metrics to AppOptics.management.metrics.export.appoptics.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.appoptics.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.appoptics.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.appoptics.uri=https://api.appoptics.com/v1/measurements # URI to ship metrics to.management.metrics.export.atlas.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.atlas.config-refresh-frequency=10s # Frequency for refreshing config settings from the LWC service.management.metrics.export.atlas.config-time-to-live=150s # Time to live for subscriptions from the LWC service.management.metrics.export.atlas.config-uri=http://localhost:7101/lwc/api/v1/expressions/local-dev # URI for the Atlas LWC endpoint to retrieve current subscriptions.management.metrics.export.atlas.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.atlas.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.atlas.eval-uri=http://localhost:7101/lwc/api/v1/evaluate # URI for the Atlas LWC endpoint to evaluate the data for a subscription.management.metrics.export.atlas.lwc-enabled=false # Whether to enable streaming to Atlas LWC.management.metrics.export.atlas.meter-time-to-live=15m # Time to live for meters that do not have any activity. After this period the meter will be considered expired and will not get reported.management.metrics.export.atlas.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.atlas.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.atlas.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.atlas.uri=http://localhost:7101/api/v1/publish # URI of the Atlas server.management.metrics.export.datadog.api-key= # Datadog API key.management.metrics.export.datadog.application-key= # Datadog application key. Not strictly required, but improves the Datadog experience by sending meter descriptions, types, and base units to Datadog.management.metrics.export.datadog.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.datadog.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.datadog.descriptions=true # Whether to publish descriptions metadata to Datadog. Turn this off to minimize the amount of metadata sent.management.metrics.export.datadog.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.datadog.host-tag=instance # Tag that will be mapped to &quot;host&quot; when shipping metrics to Datadog.management.metrics.export.datadog.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.datadog.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.datadog.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.datadog.uri=https://app.datadoghq.com # URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Datadog, you can define the location of the proxy with this.management.metrics.export.dynatrace.api-token= # Dynatrace authentication token.management.metrics.export.dynatrace.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.dynatrace.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.dynatrace.device-id= # ID of the custom device that is exporting metrics to Dynatrace.management.metrics.export.dynatrace.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.dynatrace.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.dynatrace.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.dynatrace.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.dynatrace.technology-type=java # Technology type for exported metrics. Used to group metrics under a logical technology name in the Dynatrace UI.management.metrics.export.dynatrace.uri= # URI to ship metrics to. Should be used for SaaS, self managed instances or to en-route through an internal proxy.management.metrics.export.elastic.auto-create-index=true # Whether to create the index automatically if it does not exist.management.metrics.export.elastic.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.elastic.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.elastic.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.elastic.host=http://localhost:9200 # Host to export metrics to.management.metrics.export.elastic.index=metrics # Index to export metrics to.management.metrics.export.elastic.index-date-format=yyyy-MM # Index date format used for rolling indices. Appended to the index name, preceded by a &apos;-&apos;.management.metrics.export.elastic.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.elastic.password= # Login password of the Elastic server.management.metrics.export.elastic.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.elastic.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.elastic.timestamp-field-name=@timestamp # Name of the timestamp field.management.metrics.export.elastic.user-name= # Login user of the Elastic server.management.metrics.export.ganglia.addressing-mode=multicast # UDP addressing mode, either unicast or multicast.management.metrics.export.ganglia.duration-units=milliseconds # Base time unit used to report durations.management.metrics.export.ganglia.enabled=true # Whether exporting of metrics to Ganglia is enabled.management.metrics.export.ganglia.host=localhost # Host of the Ganglia server to receive exported metrics.management.metrics.export.ganglia.port=8649 # Port of the Ganglia server to receive exported metrics.management.metrics.export.ganglia.protocol-version=3.1 # Ganglia protocol version. Must be either 3.1 or 3.0.management.metrics.export.ganglia.rate-units=seconds # Base time unit used to report rates.management.metrics.export.ganglia.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.ganglia.time-to-live=1 # Time to live for metrics on Ganglia. Set the multi-cast Time-To-Live to be one greater than the number of hops (routers) between the hosts.management.metrics.export.graphite.duration-units=milliseconds # Base time unit used to report durations.management.metrics.export.graphite.enabled=true # Whether exporting of metrics to Graphite is enabled.management.metrics.export.graphite.host=localhost # Host of the Graphite server to receive exported metrics.management.metrics.export.graphite.port=2004 # Port of the Graphite server to receive exported metrics.management.metrics.export.graphite.protocol=pickled # Protocol to use while shipping data to Graphite.management.metrics.export.graphite.rate-units=seconds # Base time unit used to report rates.management.metrics.export.graphite.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.graphite.tags-as-prefix= # For the default naming convention, turn the specified tag keys into part of the metric prefix.management.metrics.export.humio.api-token= # Humio API token.management.metrics.export.humio.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.humio.connect-timeout=5s # Connection timeout for requests to this backend.management.metrics.export.humio.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.humio.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.humio.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.humio.repository=sandbox # Name of the repository to publish metrics to.management.metrics.export.humio.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.humio.tags.*= # Humio tags describing the data source in which metrics will be stored. Humio tags are a distinct concept from Micrometer&apos;s tags. Micrometer&apos;s tags are used to divide metrics along dimensional boundaries.management.metrics.export.humio.uri=https://cloud.humio.com # URI to ship metrics to. If you need to publish metrics to an internal proxy en-route to Humio, you can define the location of the proxy with this.management.metrics.export.influx.auto-create-db=true # Whether to create the Influx database if it does not exist before attempting to publish metrics to it.management.metrics.export.influx.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.influx.compressed=true # Whether to enable GZIP compression of metrics batches published to Influx.management.metrics.export.influx.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.influx.consistency=one # Write consistency for each point.management.metrics.export.influx.db=mydb # Tag that will be mapped to &quot;host&quot; when shipping metrics to Influx.management.metrics.export.influx.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.influx.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.influx.password= # Login password of the Influx server.management.metrics.export.influx.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.influx.retention-duration= # Time period for which Influx should retain data in the current database.management.metrics.export.influx.retention-shard-duration= # Time range covered by a shard group.management.metrics.export.influx.retention-policy= # Retention policy to use (Influx writes to the DEFAULT retention policy if one is not specified).management.metrics.export.influx.retention-replication-factor= # How many copies of the data are stored in the cluster.management.metrics.export.influx.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.influx.uri=http://localhost:8086 # URI of the Influx server.management.metrics.export.influx.user-name= # Login user of the Influx server.management.metrics.export.jmx.domain=metrics # Metrics JMX domain name.management.metrics.export.jmx.enabled=true # Whether exporting of metrics to JMX is enabled.management.metrics.export.jmx.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.kairos.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.kairos.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.kairos.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.kairos.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.kairos.password= # Login password of the KairosDB server.management.metrics.export.kairos.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.kairos.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.kairos.uri= localhost:8080/api/v1/datapoints # URI of the KairosDB server.management.metrics.export.kairos.user-name= # Login user of the KairosDB server.management.metrics.export.newrelic.account-id= # New Relic account ID.management.metrics.export.newrelic.api-key= # New Relic API key.management.metrics.export.newrelic.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.newrelic.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.newrelic.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.newrelic.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.newrelic.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.newrelic.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.newrelic.uri=https://insights-collector.newrelic.com # URI to ship metrics to.management.metrics.export.prometheus.descriptions=true # Whether to enable publishing descriptions as part of the scrape payload to Prometheus. Turn this off to minimize the amount of data sent on each scrape.management.metrics.export.prometheus.enabled=true # Whether exporting of metrics to Prometheus is enabled.management.metrics.export.prometheus.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.prometheus.pushgateway.base-url=localhost:9091 # Base URL for the Pushgateway.management.metrics.export.prometheus.pushgateway.enabled=false # Enable publishing via a Prometheus Pushgateway.management.metrics.export.prometheus.pushgateway.grouping-key= # Grouping key for the pushed metrics.management.metrics.export.prometheus.pushgateway.job= # Job identifier for this application instance.management.metrics.export.prometheus.pushgateway.push-rate=1m # Frequency with which to push metrics.management.metrics.export.prometheus.pushgateway.shutdown-operation= # Operation that should be performed on shutdown.management.metrics.export.signalfx.access-token= # SignalFX access token.management.metrics.export.signalfx.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.signalfx.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.signalfx.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.signalfx.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.signalfx.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.signalfx.source= # Uniquely identifies the app instance that is publishing metrics to SignalFx. Defaults to the local host name.management.metrics.export.signalfx.step=10s # Step size (i.e. reporting frequency) to use.management.metrics.export.signalfx.uri=https://ingest.signalfx.com # URI to ship metrics to.management.metrics.export.simple.enabled=true # Whether, in the absence of any other exporter, exporting of metrics to an in-memory backend is enabled.management.metrics.export.simple.mode=cumulative # Counting mode.management.metrics.export.simple.step=1m # Step size (i.e. reporting frequency) to use.management.metrics.export.statsd.enabled=true # Whether exporting of metrics to StatsD is enabled.management.metrics.export.statsd.flavor=datadog # StatsD line protocol to use.management.metrics.export.statsd.host=localhost # Host of the StatsD server to receive exported metrics.management.metrics.export.statsd.max-packet-length=1400 # Total length of a single payload should be kept within your network&apos;s MTU.management.metrics.export.statsd.polling-frequency=10s # How often gauges will be polled. When a gauge is polled, its value is recalculated and if the value has changed (or publishUnchangedMeters is true), it is sent to the StatsD server.management.metrics.export.statsd.port=8125 # Port of the StatsD server to receive exported metrics.management.metrics.export.statsd.publish-unchanged-meters=true # Whether to send unchanged meters to the StatsD server.management.metrics.export.wavefront.api-token= # API token used when publishing metrics directly to the Wavefront API host.management.metrics.export.wavefront.batch-size=10000 # Number of measurements per request to use for this backend. If more measurements are found, then multiple requests will be made.management.metrics.export.wavefront.connect-timeout=1s # Connection timeout for requests to this backend.management.metrics.export.wavefront.enabled=true # Whether exporting of metrics to this backend is enabled.management.metrics.export.wavefront.global-prefix= # Global prefix to separate metrics originating from this app&apos;s white box instrumentation from those originating from other Wavefront integrations when viewed in the Wavefront UI.management.metrics.export.wavefront.num-threads=2 # Number of threads to use with the metrics publishing scheduler.management.metrics.export.wavefront.read-timeout=10s # Read timeout for requests to this backend.management.metrics.export.wavefront.source= # Unique identifier for the app instance that is the source of metrics being published to Wavefront. Defaults to the local host name.management.metrics.export.wavefront.step=10s # Step size (i.e. reporting frequency) to use.management.metrics.export.wavefront.uri=https://longboard.wavefront.com # URI to ship metrics to.management.metrics.use-global-registry=true # Whether auto-configured MeterRegistry implementations should be bound to the global static registry on Metrics.management.metrics.tags.*= # Common tags that are applied to every meter.management.metrics.web.client.max-uri-tags=100 # Maximum number of unique URI tag values allowed. After the max number of tag values is reached, metrics with additional tag values are denied by filter.management.metrics.web.client.requests-metric-name=http.client.requests # Name of the metric for sent requests.management.metrics.web.server.auto-time-requests=true # Whether requests handled by Spring MVC, WebFlux or Jersey should be automatically timed.management.metrics.web.server.max-uri-tags=100 # Maximum number of unique URI tag values allowed. After the max number of tag values is reached, metrics with additional tag values are denied by filter.management.metrics.web.server.requests-metric-name=http.server.requests # Name of the metric for received requests.# ----------------------------------------# DEVTOOLS PROPERTIES# ----------------------------------------# DEVTOOLS (DevToolsProperties)spring.devtools.add-properties=true # Whether to enable development property defaults.spring.devtools.livereload.enabled=true # Whether to enable a livereload.com-compatible server.spring.devtools.livereload.port=35729 # Server port.spring.devtools.restart.additional-exclude= # Additional patterns that should be excluded from triggering a full restart.spring.devtools.restart.additional-paths= # Additional paths to watch for changes.spring.devtools.restart.enabled=true # Whether to enable automatic restart.spring.devtools.restart.exclude=META-INF/maven/**,META-INF/resources/**,resources/**,static/**,public/**,templates/**,**/*Test.class,**/*Tests.class,git.properties,META-INF/build-info.properties # Patterns that should be excluded from triggering a full restart.spring.devtools.restart.log-condition-evaluation-delta=true # Whether to log the condition evaluation delta upon restart.spring.devtools.restart.poll-interval=1s # Amount of time to wait between polling for classpath changes.spring.devtools.restart.quiet-period=400ms # Amount of quiet time required without any classpath changes before a restart is triggered.spring.devtools.restart.trigger-file= # Name of a specific file that, when changed, triggers the restart check. If not specified, any classpath file change triggers the restart.# REMOTE DEVTOOLS (RemoteDevToolsProperties)spring.devtools.remote.context-path=/.~~spring-boot!~ # Context path used to handle the remote connection.spring.devtools.remote.proxy.host= # The host of the proxy to use to connect to the remote application.spring.devtools.remote.proxy.port= # The port of the proxy to use to connect to the remote application.spring.devtools.remote.restart.enabled=true # Whether to enable remote restart.spring.devtools.remote.secret= # A shared secret required to establish a connection (required to enable remote support).spring.devtools.remote.secret-header-name=X-AUTH-TOKEN # HTTP header used to transfer the shared secret.# ----------------------------------------# TESTING PROPERTIES# ----------------------------------------spring.test.database.replace=any # Type of existing DataSource to replace.spring.test.mockmvc.print=default # MVC Print option.]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>Properties</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RestTemplate接收泛型参数]]></title>
    <url>%2F2019%2F09%2F28%2FRestTemplate%E6%8E%A5%E6%94%B6%E6%B3%9B%E5%9E%8B%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[前提使用restTemplate的postForObject等方法不能正确接收除了String的其他泛型的参数 如：Map&lt;String, UserDTO&gt;；List&lt;UserDTO&gt;中的UserDTO会自动用LinkedHashMap接收，从而变成 Map&lt;String, LinkedHashMap&gt; 和 List&lt;LinkedHashMap&gt;，导致接收端不能正确接收参数 解决方法使用ParameterizedTypeReference&lt;T&gt;指定泛型，就可以正确接收数据了 例子1123ParameterizedTypeReference&lt;Map&lt;String, UserDTO&gt;&gt; typeRef = new ParameterizedTypeReference&lt;Map&lt;String, UserDTO&gt;&gt;() &#123;&#125;; ResponseEntity&lt;Map&lt;String, UserDTO&gt;&gt; responseEntity = restTemplate.exchange(requestUrl, HttpMethod.POST, new HttpEntity&lt;&gt;(params), typeRef); Map&lt;String, UserDTO&gt; list = responseEntity.getBody(); 例子2123ParameterizedTypeReference&lt;List&lt;UserDTO&gt;&gt; typeRef = new ParameterizedTypeReference&lt;List&lt;UserDTO&gt;&gt;() &#123;&#125;; ResponseEntity&lt;List&lt;UserDTO&gt;&gt; responseEntity = restTemplate.exchange(requestUrl, HttpMethod.POST, new HttpEntity&lt;&gt;(params), typeRef); List&lt;UserDTO&gt; list = responseEntity.getBody();]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>RestTemplate</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Springboot的@Conditional注解]]></title>
    <url>%2F2019%2F09%2F20%2FSpringboot%E7%9A%84%40Conditional%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[@Conditional 官方文档定义：“Indicates that a component is only eligible for registration when all specified conditions match”，意思是只有满足一些列条件之后才创建bean。 所以Springboot利用@Conditional注解来确定是否要加载该实例 定义12345678910111213package org.springframework.context.annotation;import java.lang.annotation.Documented;import java.lang.annotation.ElementType;import java.lang.annotation.Retention;import java.lang.annotation.RetentionPolicy;import java.lang.annotation.Target;@Target(&#123;ElementType.TYPE, ElementType.METHOD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Conditional &#123; Class&lt;? extends Condition&gt;[] value();&#125; 使用的位置: 类级别可以放在注标识有@Component（包含@Configuration）的类上 作为一个meta-annotation，组成自定义注解 方法级别可以放在标识由@Bean的方法上 @Conditional拓展注解 @ConditionalOnBean: 当且仅当Springboot容器存在指定的bean classes and/or bean names，才创建注解所修饰的实例。 @ConditionalOnMissingBean: 当且仅当Springboot容器不存在指定的bean classes and/or bean names，才创建注解所修饰的实例 @ConditionalOnClass：和@ConditionalOnBean类似，当classpath中存在指定的class或className @ConditionalOnMissingClass：和@ConditionalOnMissingBean类似，当classpath中存在指定的class或className @ConditionalOnProperty：当且仅当application.properties/application.yml存在指定的配置项时，才创建注解所修饰的实例 @ConditionalOnJava：指定JDK的版本 @ConditionalOnExpression：表达式用${..}=false等来表示 @ConditionalOnJndi：JNDI存在该项时创建 @ConditionalOnResource：在classpath下存在指定的resource时创建 @ConditionalOnSingleCandidate 当容器中指定的class仅存在一个bean的时候，才创建注解所修饰的实例 @ConditionalOnWebApplication：在web环境下创建]]></content>
      <categories>
        <category>Springboot</category>
      </categories>
      <tags>
        <tag>Springboot</tag>
        <tag>Annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String的常见问题]]></title>
    <url>%2F2019%2F09%2F06%2FString%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[双引号和构造器两种形式创建字符串的区别例子11234String a = "abcd";String b = "abcd";System.out.println(a == b); // TrueSystem.out.println(a.equals(b)); // True a == b 结果为 true，是因为 a 和 b 都指向 方法区(method area) 同一个字符串文字，内存引用是同一个。 当多次创建相同的字符串文字时，只存储每个不同字符串值的一个副本。这个叫做字符串留驻/留用，Java 中所有编译期字符串常量都会被自动留驻。 例子21234String c = new String("abcd");String d = new String("abcd");System.out.println(c == d); // FalseSystem.out.println(c.equals(d)); // True c==d 结果为 false，因为 c 和 d 的引用指向堆中不同的对象，不同的对象肯定有不同的内存引用 上述两种情况，用图形来理解的话是： 一个是在方法区，一个是在堆中， 这是JVM模型的两个不同的区域。 JVM模型的图： 所有new的对象都在Heap中。 运行期字符串留驻1234String c = new String("abcd").intern();String d = new String("abcd").intern();System.out.println(c == d); // TrueSystem.out.println(c.equals(d)); // True c == d 结果为 true， intern (英文有拘留，软禁的意思)的作用了，通过调用 intern()方法，就好比把创建的字符串拘留在方法区一样了 在面试时甚至还会问你下面代码创建了几个对象 1String d = new String("abcd") 如果方法区里有&quot;abcd&quot;， 则只创建一个new String对象 如果方法区里没有&quot;abcd&quot;，则创建两个字符串对象 一个是方法区的&quot;abcd&quot; 一个是堆的new String(&quot;abcd&quot;) 所以，正常情况下没必要使用构造器创建对象，因为这很可能会产生一个额外的没用的对象。 但是也有例外： 12String s = "abcd";s = s.concat("ef"); 当在字符串 s 后面拼接字符串&quot;ef&quot;时，会在堆中创建一个新的对象，并将 s 的引用指向新创建的对象，由于 String 创建的是不可变对象，所以 String 类中的所有方法都不会改变它自身，而是返回一个新的字符串(快打开你的 IDE，看看是否每个操作String 的方法最后都是返回有 return new String 字样)，到这里你应该理解了一个道理: 如果需要一个字符串被修改，我们最好使用 StringBuffer 或者 StringBuilder，否则，由于每次操作字符串都会创建一个新的对象，而旧的对象不会有引用指向它，这样我们会浪费很多垃圾回收的时间 为什么 String 类被 final 修饰字符串池的需求字符串池(String intern pool) 是方法区中的一个特殊存储区域。当创建一个字符串时，如果该字符串已经存在于池中，那么返回现有字符串的引用，而不是创建一个新对象。所以说，如果一个字符串是可变的，那么改变一个引用的值，将导致原本指向该值的引用获取到错误的值。 缓存 hashcode字符串的hashcode在Java中经常使用。例如，在HashMap或HashSet中。不可变保证hashcode始终是相同的，这样就可以在不担心更改的情况下兑现它。这意味着，不需要每次使用hashcode时都计算它。这样更有效率。所以你会在 String 类中看到下面的成员变量的定义: 12/** Cache the hash code for the string */private int hash; // Default to 0 安全性String被广泛用作许多java类的参数，例如网络连接、打开文件等。如果字符串不是不可变的，连接或文件将被更改，这可能导致严重的安全威胁。该方法认为它连接到一台机器上，但实际上并没有。可变字符串也可能导致反射中的安全问题，因为参数是字符串。 不可变对象天生是线程安全的由于不可变对象不能被更改，所以它们可以在多个线程之间自由共享。这消除了同步的需求。 总之，出于效率和安全性的考虑，String 被设计为不可变的。这也是为什么在一般情况下，不可变类是首选的原因。 附加说明关于不可变对象和不可变引用总是搞不清楚 1final User user = new User(); 上面的代码指的是 user 引用不能被更改指向内存的其他地址，但是由于 User 是可变对象，我们可以调用 user 的 setter 方法修改其属性]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring自定义注解]]></title>
    <url>%2F2019%2F09%2F05%2FSpring%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前提 在业务开发过程中我们会使用很多的注解，但是框架自有的注解并不是总能满足我们复杂的业务需求，所以我们一般会自定义注解来满足需求。根据注解使用的位置，分成字段注解、方法、类注解来介绍如何自定义注解。 字段注解 字段注解一般是用于校验字段是否满足要求，其中hibernate-validate依赖就提供了很多常用的校验注解 ，如@NotNull、@Range等，可以解决常见的业务场景。然而如果想要校验跟业务强相关的需求，那么已有的注解就不满足了。例如：传入的参数需要判断是否在指定的String集合中（业务枚举集合也可）。 自定义注解@Check123456789101112131415161718192021//只允许用在类的字段上@Target(&#123; ElementType.FIELD&#125;) //注解保留在程序运行期间，此时可以通过反射获得定义在某个类上的所有注解@Retention(RetentionPolicy.RUNTIME) //指定与注解关联的验证器@Constraint(validatedBy = ParamConstraintValidated.class)public @interface Check &#123; /** * 合法的参数值 **/ String[] paramValues(); /** * 提示信息 **/ String message() default "参数不为指定值"; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; @Target 定义注解的使用位置，用来说明该注解可以被声明在那些元素上。 ElementType.TYPE：说明该注解只能被声明在一个类上。 ElementType.FIELD：说明该注解只能被声明在一个类的字段上。 ElementType.METHOD：说明该注解只能被声明在一个类的方法上。 ElementType.PARAMETER：说明该注解只能被声明在一个方法参数上。 ElementType.CONSTRUCTOR：说明该注解只能声明在一个类的构造方法上。 ElementType.LOCAL_VARIABLE：说明该注解只能声明在一个局部变量上。 ElementType.ANNOTATION_TYPE：说明该注解只能声明在一个注解类型上。 ElementType.PACKAGE：说明该注解只能声明在一个包名上 @Constraint 通过使用validatedBy来指定与注解关联的验证器 @Retention用来说明该注解类的生命周期。 RetentionPolicy.SOURCE: 注解只保留在源文件中 RetentionPolicy.CLASS : 注解保留在class文件中，在加载到JVM虚拟机时丢弃 RetentionPolicy.RUNTIME: 注解保留在程序运行期间，此时可以通过反射获得定义在某个类上的所有注解。 定义处理@Check的验证器类验证器类需要实现ConstraintValidator泛型接口 第一个泛型参数类型：校验的注解。 第二个泛型参数Object：校验字段类型。 需要实现initialize和isValid方法，isValid方法为校验逻辑，initialize方法初始化工作 1234567891011121314151617181920public class ParamConstraintValidated implements ConstraintValidator&lt;Check, Object&gt; &#123; /** * 合法的参数值，从注解中获取 * */ private List&lt;String&gt; paramValues; @Override public void initialize(Check constraintAnnotation) &#123; //初始化时获取注解上的值 paramValues = Arrays.asList(constraintAnnotation.paramValues()); &#125; public boolean isValid(Object o, ConstraintValidatorContext constraintValidatorContext) &#123; if (paramValues.contains(o)) &#123; return true; &#125; //不在指定的参数列表中 return false; &#125;&#125; 使用方式定义一个实体类，对sex字段加校验，其值必须为woman或者man 1234567891011121314@Getter@Setterpublic class User &#123; /** * 姓名 * */ private String name; /** * 性别 man or women * */ @Check(paramValues = &#123;"man", "woman"&#125;) private String sex;&#125; 测试在需要检验的对象上加上@Validated注解 1234567@RestController("/api/test")public class TestController &#123; @PostMapping public Object test(@Validated @RequestBody User user) &#123; return "hello world"; &#125;&#125; 方法、类注解 在开发过程中遇到过这样的需求，如只有有权限的用户的才能访问这个类中的方法或某个具体的方法、查找数据的时候先不从数据库查找，先从guava cache中查找，在从redis查找，最后查找mysql（多级缓存）。 这时候我们可以自定义注解去完成这个要求： 第一个场景：定义类似Spring Security下的@PreAuthorize一样的功能的权限校验的注解 123&gt; - @PreAuthorize("hasRole('ADMIN')") &gt; - @PreAuthorize("hasAuthority('ADMIN')")&gt; 第二个场景：定义类似Spring-data-redis下的@Cacheable的注解 权限注解自定义注解@PermissionCheck该注解的作用范围为类或者方法上 12345678@Target(&#123; ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface PermissionCheck &#123; /** * 资源key * */ String resourceKey();&#125; 定义处理@PermissionCheck的拦截器类12345678910111213141516171819202122232425262728293031323334353637383940public class PermissionCheckInterceptor extends HandlerInterceptorAdapter &#123; /** * 处理器处理之前调用 */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; HandlerMethod handlerMethod = (HandlerMethod)handler; PermissionCheck permission = findPermissionCheck(handlerMethod); //如果没有添加权限注解则直接跳过允许访问 if (permission == null) &#123; return true; &#125; //获取注解中的值 String resourceKey = permission.resourceKey(); //TODO 权限校验一般需要获取用户信息，通过查询数据库进行权限校验 //TODO 这里只进行简单演示，如果resourceKey为testKey则校验通过，否则不通过 return "testKey".equals(resourceKey); &#125; /** * 根据handlerMethod返回注解信息 * * @param handlerMethod 方法对象 * @return PermissionCheck注解 */ private PermissionCheck findPermissionCheck(HandlerMethod handlerMethod) &#123; //在方法上寻找注解 PermissionCheck permission = handlerMethod.getMethodAnnotation(PermissionCheck.class); if (permission == null) &#123; //在类上寻找注解 permission = handlerMethod.getBeanType().getAnnotation(PermissionCheck.class); &#125; return permission; &#125;&#125; 测试12345@GetMapping("/api/test")@PermissionCheck(resourceKey = "test")public Object testPermissionCheck() &#123; return "hello world";&#125; 缓存注解自定义注解@CustomCache12345678@Target(&#123; ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface CustomCache &#123; /** * 缓存的key值 * */ String key();&#125; 定义处理 @CustomCache的切面类12345678910111213141516171819202122232425262728293031323334@Aspect@Componentpublic class CustomCacheAspect &#123; /** * 在方法执行之前对注解进行处理 * * @param pjd * @param customCache 注解 * @return 返回中的值 * */ @Around("@annotation(com.test.annotation.CustomCache)") public Object dealProcess(ProceedingJoinPoint pjd, CustomCache customCache) &#123; Object result = null; if (customCache.key() == null) &#123; //TODO throw error &#125; //TODO 业务场景会比这个复杂的多，会涉及参数的解析如key可能是#&#123;id&#125;这些，数据查询 //TODO 这里做简单演示，如果key为testKey则返回hello world if ("testKey".equals(customCache.key())) &#123; return "hello word"; &#125; //执行目标方法 try &#123; result = pjd.proceed(); &#125; catch (Throwable throwable) &#123; throwable.printStackTrace(); &#125; return result; &#125;&#125; 因为缓存注解需要在方法执行之前有返回值，所以没有通过拦截器处理这个注解，而是通过使用切面在执行方法之前对注解进行处理。如果注解没有返回值，将会返回方法中的值。 测试12345@GetMapping("/api/cache")@CustomCache(key = "test")public Object testCustomCache() &#123; return "don't hit cache";&#125;]]></content>
      <categories>
        <category>注解</category>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>注解</tag>
        <tag>Spring</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lambda表达式Collectors.toMap当value为null时报错]]></title>
    <url>%2F2019%2F08%2F26%2Flambda%E8%A1%A8%E8%BE%BE%E5%BC%8FCollectors-toMap%E5%BD%93value%E4%B8%BAnull%E6%97%B6%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[原因 Map.merge方法合并时，merge不允许value为null导致的 源码如下12345678910111213default V merge(K key, V value, BiFunction&lt;? super V, ? super V, ? extends V&gt; remappingFunction) &#123; Objects.requireNonNull(remappingFunction); Objects.requireNonNull(value); V oldValue = get(key); V newValue = (oldValue == null) ? value : remappingFunction.apply(oldValue, value); if (newValue == null) &#123; remove(key); &#125; else &#123; put(key, newValue); &#125; return newValue; &#125; 解决的办法123456fundsSerials.stream() .collect( Collectors.toMap( FundsSerial::getSerialNo, fundsSerial -&gt; Optional.of(fundsSerial).map(FundsSerial::getOrderNo).orElse(""), (k1, k2) -&gt; k2)); 使用：Optional.of(fundsSerial) 或者 Optional.ofNullable(fundsSerial) 转成 optional 对象， 然后使用orElse()，换成默认值。]]></content>
      <categories>
        <category>JDK</category>
        <category>JDK8</category>
      </categories>
      <tags>
        <tag>lambda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[将jar包设置成windows的service]]></title>
    <url>%2F2019%2F08%2F25%2F%E5%B0%86jar%E5%8C%85%E6%B3%A8%E5%86%8C%E6%88%90windows%E7%9A%84%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[前提 在开发过程中，有时会需要自定义一些服务启动，但是每次都要手动在命令行窗口中运行命令，而且会有一个窗口在任务栏里，太麻烦了。 例如，我需要部署一个Jrebel的注册器，idea才可以激活Jrebel成功，所以想注册成一个服务，开机后自动启动服务，不需要再操作任何东西。 准备Jar包自行准备需要注册成服务的jar包。如果是Java程序的话，maven打包成jar即可。 这里我需要一个Jrebel的注册器的Jar包。 JrebelLicenseService下载地址 下载winswwinsw是一款可将可执行程序安装成Windows Service的开源工具。winsw下载地址 只需要下载可执行文件(.exe)和配置文件(.xml)，配置文件是为了参考需要配置什么，到时直接在上面改即可。 sample-minimal.xml: 最小配置 sample-allOptions.xml： 可选择的配置 配置配置服务所需要的文件 首先创建一个文件夹Jrebel。 将下载好的JrebelBrainsLicenseServerforJava-1.0-SNAPSHOT-jar-with-dependencies.jar改名为Jrebel.jar，并复制进去。 将下载好的WinSW.NET4.exe改名为Jrebel.exe，并复制进去。 因为我不需要特别复杂的配置，所以使用sample-minimal.xml即可，改名为Jrebel.xml，并复制进去。 如图所示： 修改配置文件修改Jrebel.xml的内容： 12345678910111213141516171819202122232425262728&lt;configuration&gt; &lt;!-- 服务ID: 启动、关闭、删除服务时，都是通过ID来操作的 --&gt; &lt;id&gt;JrebelService&lt;/id&gt; &lt;!-- 服务名称, 用于显示 --&gt; &lt;name&gt;JrebelService&lt;/name&gt; &lt;!-- 服务描述 --&gt; &lt;description&gt;This service is Jrebel License Service&lt;/description&gt; &lt;!-- 若配置了java环境变量，直接写成“java”就行; --&gt; &lt;!-- 也可以写成这样来指定使用的jdk版本: C:\Program Files\Java\jdk8\jre\bin\java --&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;!-- 启动参数 --&gt; &lt;!-- jar包不在同一个文件夹下可以使用这种写法: G:\Jrebel\Jrebel.jar--&gt; &lt;arguments&gt;-jar Jrebel.jar -p 8081&lt;/arguments&gt; &lt;!-- 设置开机启动 --&gt; &lt;startmode&gt;Automatic&lt;/startmode&gt; &lt;!-- 日志配置 --&gt; &lt;!-- logpath指定在哪个文件下记录log，这里是在同级目录下创建logs文件夹存放log --&gt; &lt;logpath&gt;logs&lt;/logpath&gt; &lt;logmode&gt;rotate&lt;/logmode&gt; &lt;/configuration&gt; 注册服务 已管理员身份打开命令行。 进入到(.exe)目录下，即D:\Jrebel。如果将该路径配置在Path路径下，则这步省略。 输入 Jrebel install 即可注册服务。 服务注册成功后，使用net start JrebelService运行该服务。 这样就完成了。 可能碰到的问题“无法将XXXX项识别为 cmdlet、函数、脚本文件或可运行程序的名称”的问题 在这次服务中出现的话，应该是报：“无法将Jrebel.exe项识别为 cmdlet、函数、脚本文件或可运行程序的名称”的问题 解决方法： 第一种方式：需要在有Jrebel.exe的目录下执行该命令。 第二种方式：将Jrebel.exe所在目录配置在Path路径下。 FATAL - WMI Operation failure: AccessDenied 这个是因为没有使用管理员身份运行Jrebel命令，用管理员身份打开命令行即可。 升级虽然是成功配置好了服务，但是关闭服务，注销服务，都需要重新在命令行中执行命令，也是很麻烦的。所以可以编写bat文件来执行这些操作。 这里主要是四个命令： Jrebel install net start JrebelService net stop JrebelService Jrebel uninstall 所以需要创建四个bat文件，分别是 Jrebel_install.bat 对应命令 Jrebel install Jrebel_start.bat 对应命令 net start JrebelService Jrebel_stop.bat 对应命令 net stop JrebelService Jrebel_uninstall.bat 对应命令 net stop JrebelService 和 Jrebel uninstall 但是这样还不行，因为这些服务命令需要使用管理员身份运行才有效。 所以需要在每个bat文件里面开头加上这段命令用于申请管理员权限： 1234567891011121314@echo off&gt;nul 2&gt;&amp;1 "%SYSTEMROOT%\system32\cacls.exe" "%SYSTEMROOT%\system32\config\system"if '%errorlevel%' NEQ '0' (goto UACPrompt) else ( goto gotAdmin ):UACPromptecho Set UAC = CreateObject^("Shell.Application"^) &gt; "%temp%\getadmin.vbs"echo UAC.ShellExecute "%~s0", "", "", "runas", 1 &gt;&gt; "%temp%\getadmin.vbs""%temp%\getadmin.vbs"exit /B:gotAdminif exist "%temp%\getadmin.vbs" ( del "%temp%\getadmin.vbs" )pushd "%CD%"cd /D "%~dp0" 其中一个示例： 这样就可以双击执行bat文件，快速注册服务，启动服务，停止服务和注销服务。]]></content>
      <categories>
        <category>开发环境</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>jar</tag>
        <tag>Jrebel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows安装nvm管理node和npm]]></title>
    <url>%2F2019%2F08%2F24%2Fwindows%E5%AE%89%E8%A3%85nvm%E7%AE%A1%E7%90%86node%E5%92%8Cnpm%2F</url>
    <content type="text"><![CDATA[什么是nvm nvm是nodeJS的版本管理器，可以运行在多种操作系统上。nvm for windows 是使用go语言编写的软件。 下载nvm-windows下载地址 如图所示： nvm-noinstall.zip： 免安装版本，但是使用之前需要配置环境变量 nvm-setup.zip：安装包，下载之后点击安装，无需配置就可以使用。 Source code(zip)：zip压缩的源码 Sourc code(tar.gz)：tar.gz的源码，一般用于Unix系统 检测检查是否安装成功，打开命令行窗口中输入nvm 如果出现nvm版本号和一系列帮助指令，则说明nvm安装成功。 否则，可能会提示nvm: command not found 使用nvm for windows是一个命令行工具，在控制台输入nvm,就可以看到它的命令用法。 基本命令 nvm arch [32|64] ： 显示node是运行在32位还是64位模式。参数可指定32或64来覆盖默认体系结构。 nvm install &lt;version&gt; [arch]： 该可以是node.js版本或最新稳定版本latest。（可选[arch]）指定安装32位或64位版本（默认为系统arch）。设置[arch]为all以安装32和64位版本。在命令后面添加--insecure ，可以绕过远端下载服务器的SSL验证。 nvm list [available]： 列出已经安装的node.js版本。可选的available，显示可下载版本的部分列表。这个命令可以简写为nvm ls [available]。 nvm on： 启用node.js版本管理。 nvm off： 禁用node.js版本管理(不卸载任何东西) nvm proxy [url]： 设置用于下载的代理。留[url]空白，以查看当前的代理。设置[url]为none删除代理。 nvm node_mirror [url]：设置node镜像，默认为https://nodejs.org/dist/.。我建议设置为淘宝的镜像https://npm.taobao.org/mirrors/node/ nvm npm_mirror [url]：设置npm镜像，默认为https://github.com/npm/npm/archive/。我建议设置为淘宝的镜像https://npm.taobao.org/mirrors/npm/ nvm uninstall &lt;version&gt;： 卸载指定版本的nodejs。 nvm use [version] [arch]： 切换到使用指定的nodejs版本。可以指定32/64位[arch]。nvm use &lt;arch&gt;将继续使用所选版本，但根据提供的值切换到32/64位模式的&lt;arch&gt; nvm root [path]： 设置 nvm 存储node.js不同版本的目录 ,如果未设置，将使用当前目录。 nvm version： 显示当前运行的nvm版本，可以简写为nvm v 安装某个版本的node这里我安装的是v12.9.0 nvm install 12.9.0 nvm use 12.9.0 配置环境变量根据安装nvm时，选择的node的地址，复制到系统Path路径下 方便可以在全局使用，以及在git bash中使用 注意： 安装完后，git bash可能会提示 node: command not found 或者 npm: command not found 重启一下就好了。]]></content>
      <categories>
        <category>开发环境</category>
      </categories>
      <tags>
        <tag>windows</tag>
        <tag>nvm</tag>
        <tag>node</tag>
        <tag>npm</tag>
      </tags>
  </entry>
</search>
